{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W207 Fall 2017 Final ProjectÂ¶\n",
    "## Data Set Up - Data Cleaning and Feature Engineering\n",
    "Laura Williams, Kim Vignola, Cyprian Gascoigne  \n",
    "SF Crime Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads raw data (saved in a zip file) from Kaggle, processes and organizes the data for training a variety of machine learning models, and outputs the data as zipped csv files that other notebooks can unzip and use to train different models.\n",
    "\n",
    "The intention is that data cleaning and/or feature engineering will be added to this file as we progress through the project and look for additional way to process the data to improve our predictions.\n",
    "\n",
    "For ease of processing this data, exploratory data analysis will be in a separate notebook.\n",
    "\n",
    "Single zipped output file (called data.zip) includes:  \n",
    "\n",
    "1) train_data.csv and train_labels.csv - includes 80% of the total training data, for training models that are not yet going to be submitted to Kaggle\n",
    "\n",
    "2) dev_data.csv and dev_labels.csv - includes 20% of the total training data, for testing models before they are submitted to Kaggle\n",
    "\n",
    "3) train_data_all.csv and train_labels_all.csv - includes all the training data. After testing models with the train and dev data split above, train the model from this full set of data for submission to Kaggle.\n",
    "\n",
    "4) test_data_all.csv - create predictions on this data for submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip raw data into a subdirectory \n",
    "unzip_files = zipfile.ZipFile(\"raw_data.zip\", \"r\")\n",
    "unzip_files.extractall(\"raw_data\")\n",
    "unzip_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read CSV files into pandas dataframes\n",
    "train = pd.read_csv(\"raw_data/train.csv\")\n",
    "test = pd.read_csv(\"raw_data/test.csv\")\n",
    "weather = pd.read_csv(\"raw_data/SF_county.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import datetime and holiday modules (note this takes a a few minutes to run)\n",
    "from datetime import datetime, timedelta, date\n",
    "import holidays\n",
    "\n",
    "# extract month, year and hour from both datasets\n",
    "train[\"month\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "train[\"year\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "train[\"day\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "train[\"hour\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "\n",
    "test[\"month\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "test[\"year\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "test[\"day\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "test[\"hour\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "\n",
    "# map holidays\n",
    "US_Holidays = holidays.UnitedStates()\n",
    "train[\"holidays\"] = train[\"Dates\"].map(lambda x: x in US_Holidays)\n",
    "test[\"holidays\"] = test[\"Dates\"].map(lambda x: x in US_Holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datetime import timedelta, date I Will mess around with this later\n",
    "#print(\"2015-1-1\" in US_Holidays)\n",
    "#print(date(2015,1,1)in US_Holidays)\n",
    "#print(date(train[\"year\"], train[\"month\"], 1) + timedelta(days = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary for dayparts\n",
    "time_periods = {6:\"early_morning\", 7:\"early_morning\", 8:\"early_morning\", \n",
    "               9:\"late_morning\", 10:\"late_morning\", 11:\"late_morning\",\n",
    "              12:\"early_afternoon\", 13:\"early_afternoon\", 14:\"early_afternoon\",\n",
    "              15:\"late_afternoon\", 16:\"late_afternoon\", 17:\"late_afternoon\",\n",
    "              18:\"early_evening\",  19:\"early_evening\",  20:\"early_evening\",\n",
    "              21:\"late_evening\", 22:\"late_evening\", 23:\"late_evening\",\n",
    "              0:\"late_night\", 1:\"late_night\", 2:\"late_night\",\n",
    "              3:\"late_night\", 4:\"late_night\", 5:\"late_night\"}\n",
    "\n",
    "# map time periods\n",
    "train[\"dayparts\"] = train[\"hour\"].map(time_periods)\n",
    "test[\"dayparts\"] = test[\"hour\"].map(time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          DATE  PRCP  SNOW  TMAX  TMIN\n",
      "0       1/1/03  0.00   0.0    59    46\n",
      "1       1/2/03  0.00   0.0    57    48\n",
      "2       1/3/03  0.00   0.0    63    47\n",
      "3       1/4/03  0.00   0.0    66    50\n",
      "4       1/5/03  0.00   0.0    69    49\n",
      "5       1/6/03  0.00   0.0    69    52\n",
      "6       1/7/03  0.00   0.0    66    47\n",
      "7       1/8/03  0.00   0.0    63    45\n",
      "8       1/9/03  0.99   0.0    56    50\n",
      "9      1/10/03  0.13   0.0    61    53\n",
      "10     1/11/03  0.00   0.0    60    53\n",
      "11     1/12/03  0.06   0.0    61    54\n",
      "12     1/13/03  0.00   0.0    64    54\n",
      "13     1/14/03  0.00   0.0    62    51\n",
      "14     1/15/03  0.00   0.0    67    50\n",
      "15     1/16/03  0.00   0.0    67    49\n",
      "16     1/17/03  0.00   0.0    68    48\n",
      "17     1/18/03  0.00   0.0    61    45\n",
      "18     1/19/03  0.00   0.0    52    44\n",
      "19     1/20/03  0.02   0.0    51    44\n",
      "20     1/21/03  0.34   0.0    58    49\n",
      "21     1/22/03  0.17   0.0    61    51\n",
      "22     1/23/03  0.04   0.0    61    55\n",
      "23     1/24/03  0.00   0.0    63    54\n",
      "24     1/25/03  0.00   0.0    68    53\n",
      "25     1/26/03  0.00   0.0    62    55\n",
      "26     1/27/03  0.00   0.0    68    51\n",
      "27     1/28/03  0.00   0.0    68    51\n",
      "28     1/29/03  0.00   0.0    64    49\n",
      "29     1/30/03  0.00   0.0    64    51\n",
      "...        ...   ...   ...   ...   ...\n",
      "4718   12/2/15  0.00   0.0    60    50\n",
      "4719   12/3/15  0.24   0.0    60    49\n",
      "4720   12/4/15  0.00   0.0    60    50\n",
      "4721   12/5/15  0.00   0.0    57    48\n",
      "4722   12/6/15  0.05   0.0    60    51\n",
      "4723   12/7/15  0.00   0.0    61    56\n",
      "4724   12/8/15  0.00   0.0    62    54\n",
      "4725   12/9/15  0.01   0.0    64    55\n",
      "4726  12/10/15  0.52   0.0    62    50\n",
      "4727  12/11/15  0.28   0.0    56    46\n",
      "4728  12/12/15  0.00   0.0    58    45\n",
      "4729  12/13/15  0.74   0.0    57    47\n",
      "4730  12/14/15  0.00   0.0    56    46\n",
      "4731  12/15/15  0.00   0.0    57    45\n",
      "4732  12/16/15  0.00   0.0    55    44\n",
      "4733  12/17/15  0.00   0.0    57    45\n",
      "4734  12/18/15  0.32   0.0    58    48\n",
      "4735  12/19/15  0.13   0.0    55    48\n",
      "4736  12/20/15  1.08   0.0    56    45\n",
      "4737  12/21/15  1.30   0.0    58    50\n",
      "4738  12/22/15  0.23   0.0    59    52\n",
      "4739  12/23/15  0.02   0.0    54    48\n",
      "4740  12/24/15  0.17   0.0    53    45\n",
      "4741  12/25/15  0.00   0.0    51    44\n",
      "4742  12/26/15  0.00   0.0    53    44\n",
      "4743  12/27/15  0.00   0.0    47    39\n",
      "4744  12/28/15  0.06   0.0    49    40\n",
      "4745  12/29/15  0.00   0.0    55    41\n",
      "4746  12/30/15  0.03   0.0    48    41\n",
      "4747  12/31/15  0.00   0.0    51    42\n",
      "\n",
      "[4748 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# clean up weather data\n",
    "del weather['NAME']\n",
    "weather[\"SNOW\"] = weather[\"SNOW\"].fillna(0)\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop time from train and test date fields to be able to map Dates against weather data; remove hyphens too.\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "\n",
    "# Convert Weather DATE to same format as train and test data\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strptime(x,\"%m/%d/%y\"))\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150513 object\n",
      "20150510 object\n",
      "20030101 object\n"
     ]
    }
   ],
   "source": [
    "# confirm that dates are now in the same format\n",
    "print(train[\"Dates\"][0], train[\"Dates\"].dtypes)\n",
    "print(test[\"Dates\"][0], test[\"Dates\"].dtypes)\n",
    "print(weather[\"DATE\"][0], weather[\"DATE\"].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# convert date objects to numeric?  This didn't seem to do the job...still getting a string error.\n",
    "train[\"Dates\"] = pd.to_numeric(train[\"Dates\"])\n",
    "test[\"Dates\"] = pd.to_numeric(test[\"Dates\"])\n",
    "weather[\"DATE\"] = pd.to_numeric(weather[\"DATE\"])\n",
    "print(type(train[\"Dates\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Dates                Category  \\\n",
      "0       20150513                WARRANTS   \n",
      "1       20150513          OTHER OFFENSES   \n",
      "2       20150513          OTHER OFFENSES   \n",
      "3       20150513           LARCENY/THEFT   \n",
      "4       20150513           LARCENY/THEFT   \n",
      "5       20150513           LARCENY/THEFT   \n",
      "6       20150513           VEHICLE THEFT   \n",
      "7       20150513           VEHICLE THEFT   \n",
      "8       20150513           LARCENY/THEFT   \n",
      "9       20150513           LARCENY/THEFT   \n",
      "10      20150513           LARCENY/THEFT   \n",
      "11      20150513          OTHER OFFENSES   \n",
      "12      20150513               VANDALISM   \n",
      "13      20150513           LARCENY/THEFT   \n",
      "14      20150513            NON-CRIMINAL   \n",
      "15      20150513            NON-CRIMINAL   \n",
      "16      20150513                 ROBBERY   \n",
      "17      20150513                 ASSAULT   \n",
      "18      20150513          OTHER OFFENSES   \n",
      "19      20150513            NON-CRIMINAL   \n",
      "20      20150513           LARCENY/THEFT   \n",
      "21      20150513                 ROBBERY   \n",
      "22      20150513                WARRANTS   \n",
      "23      20150513            NON-CRIMINAL   \n",
      "24      20150513           LARCENY/THEFT   \n",
      "25      20150513            NON-CRIMINAL   \n",
      "26      20150513           LARCENY/THEFT   \n",
      "27      20150513           LARCENY/THEFT   \n",
      "28      20150513           LARCENY/THEFT   \n",
      "29      20150513          OTHER OFFENSES   \n",
      "...          ...                     ...   \n",
      "878019  20030106          OTHER OFFENSES   \n",
      "878020  20030106          OTHER OFFENSES   \n",
      "878021  20030106               VANDALISM   \n",
      "878022  20030106           VEHICLE THEFT   \n",
      "878023  20030106           LARCENY/THEFT   \n",
      "878024  20030106          OTHER OFFENSES   \n",
      "878025  20030106          OTHER OFFENSES   \n",
      "878026  20030106                WARRANTS   \n",
      "878027  20030106                WARRANTS   \n",
      "878028  20030106                 ASSAULT   \n",
      "878029  20030106          OTHER OFFENSES   \n",
      "878030  20030106   SEX OFFENSES FORCIBLE   \n",
      "878031  20030106                 ASSAULT   \n",
      "878032  20030106          OTHER OFFENSES   \n",
      "878033  20030106               VANDALISM   \n",
      "878034  20030106                TRESPASS   \n",
      "878035  20030106                 ASSAULT   \n",
      "878036  20030106           LARCENY/THEFT   \n",
      "878037  20030106               VANDALISM   \n",
      "878038  20030106                WARRANTS   \n",
      "878039  20030106          OTHER OFFENSES   \n",
      "878040  20030106                 ASSAULT   \n",
      "878041  20030106          OTHER OFFENSES   \n",
      "878042  20030106                 ASSAULT   \n",
      "878043  20030106          OTHER OFFENSES   \n",
      "878044  20030106                 ROBBERY   \n",
      "878045  20030106           LARCENY/THEFT   \n",
      "878046  20030106           LARCENY/THEFT   \n",
      "878047  20030106               VANDALISM   \n",
      "878048  20030106  FORGERY/COUNTERFEITING   \n",
      "\n",
      "                                         Descript  DayOfWeek  PdDistrict  \\\n",
      "0                                  WARRANT ARREST  Wednesday    NORTHERN   \n",
      "1                        TRAFFIC VIOLATION ARREST  Wednesday    NORTHERN   \n",
      "2                        TRAFFIC VIOLATION ARREST  Wednesday    NORTHERN   \n",
      "3                    GRAND THEFT FROM LOCKED AUTO  Wednesday    NORTHERN   \n",
      "4                    GRAND THEFT FROM LOCKED AUTO  Wednesday        PARK   \n",
      "5                  GRAND THEFT FROM UNLOCKED AUTO  Wednesday   INGLESIDE   \n",
      "6                               STOLEN AUTOMOBILE  Wednesday   INGLESIDE   \n",
      "7                               STOLEN AUTOMOBILE  Wednesday     BAYVIEW   \n",
      "8                    GRAND THEFT FROM LOCKED AUTO  Wednesday    RICHMOND   \n",
      "9                    GRAND THEFT FROM LOCKED AUTO  Wednesday     CENTRAL   \n",
      "10                   PETTY THEFT FROM LOCKED AUTO  Wednesday     CENTRAL   \n",
      "11                    MISCELLANEOUS INVESTIGATION  Wednesday     TARAVAL   \n",
      "12      MALICIOUS MISCHIEF, VANDALISM OF VEHICLES  Wednesday  TENDERLOIN   \n",
      "13                   GRAND THEFT FROM LOCKED AUTO  Wednesday    NORTHERN   \n",
      "14                                 FOUND PROPERTY  Wednesday     BAYVIEW   \n",
      "15                                 FOUND PROPERTY  Wednesday     BAYVIEW   \n",
      "16                    ROBBERY, ARMED WITH A KNIFE  Wednesday  TENDERLOIN   \n",
      "17           AGGRAVATED ASSAULT WITH BODILY FORCE  Wednesday   INGLESIDE   \n",
      "18                              TRAFFIC VIOLATION  Wednesday     BAYVIEW   \n",
      "19                                 FOUND PROPERTY  Wednesday  TENDERLOIN   \n",
      "20                   GRAND THEFT FROM LOCKED AUTO  Wednesday   INGLESIDE   \n",
      "21                          ROBBERY, BODILY FORCE  Wednesday   INGLESIDE   \n",
      "22                                 WARRANT ARREST  Wednesday  TENDERLOIN   \n",
      "23       STAY AWAY OR COURT ORDER, NON-DV RELATED  Wednesday  TENDERLOIN   \n",
      "24                   GRAND THEFT FROM LOCKED AUTO  Wednesday    NORTHERN   \n",
      "25                                  LOST PROPERTY  Wednesday  TENDERLOIN   \n",
      "26                   GRAND THEFT FROM LOCKED AUTO  Wednesday    NORTHERN   \n",
      "27                   GRAND THEFT FROM LOCKED AUTO  Wednesday   INGLESIDE   \n",
      "28            ATTEMPTED THEFT FROM LOCKED VEHICLE  Wednesday     TARAVAL   \n",
      "29                    MISCELLANEOUS INVESTIGATION  Wednesday     TARAVAL   \n",
      "...                                           ...        ...         ...   \n",
      "878019      DRIVERS LICENSE, SUSPENDED OR REVOKED     Monday    SOUTHERN   \n",
      "878020                          TRAFFIC VIOLATION     Monday    NORTHERN   \n",
      "878021                         MALICIOUS MISCHIEF     Monday    NORTHERN   \n",
      "878022      RECOVERED VEHICLE - STOLEN OUTSIDE SF     Monday     MISSION   \n",
      "878023                     GRAND THEFT PICKPOCKET     Monday  TENDERLOIN   \n",
      "878024         VIOLATION OF MUNICIPAL POLICE CODE     Monday        PARK   \n",
      "878025                          TRAFFIC VIOLATION     Monday     BAYVIEW   \n",
      "878026                             WARRANT ARREST     Monday     BAYVIEW   \n",
      "878027            ENROUTE TO OUTSIDE JURISDICTION     Monday    SOUTHERN   \n",
      "878028       AGGRAVATED ASSAULT WITH BODILY FORCE     Monday    SOUTHERN   \n",
      "878029                        PROBATION VIOLATION     Monday  TENDERLOIN   \n",
      "878030                FORCIBLE RAPE, BODILY FORCE     Monday  TENDERLOIN   \n",
      "878031                                    BATTERY     Monday     BAYVIEW   \n",
      "878032      DRIVERS LICENSE, SUSPENDED OR REVOKED     Monday    NORTHERN   \n",
      "878033              MALICIOUS MISCHIEF, VANDALISM     Monday    RICHMOND   \n",
      "878034                                TRESPASSING     Monday    RICHMOND   \n",
      "878035                                    BATTERY     Monday    NORTHERN   \n",
      "878036                    PETTY THEFT SHOPLIFTING     Monday    NORTHERN   \n",
      "878037              MALICIOUS MISCHIEF, VANDALISM     Monday    NORTHERN   \n",
      "878038            ENROUTE TO OUTSIDE JURISDICTION     Monday  TENDERLOIN   \n",
      "878039      DRIVERS LICENSE, SUSPENDED OR REVOKED     Monday    NORTHERN   \n",
      "878040                INFLICT INJURY ON COHABITEE     Monday     MISSION   \n",
      "878041      DRIVERS LICENSE, SUSPENDED OR REVOKED     Monday    RICHMOND   \n",
      "878042              ATTEMPTED HOMICIDE WITH A GUN     Monday     BAYVIEW   \n",
      "878043                           PAROLE VIOLATION     Monday     BAYVIEW   \n",
      "878044           ROBBERY ON THE STREET WITH A GUN     Monday     TARAVAL   \n",
      "878045               GRAND THEFT FROM LOCKED AUTO     Monday   INGLESIDE   \n",
      "878046               GRAND THEFT FROM LOCKED AUTO     Monday    SOUTHERN   \n",
      "878047  MALICIOUS MISCHIEF, VANDALISM OF VEHICLES     Monday    SOUTHERN   \n",
      "878048                   CHECKS, FORGERY (FELONY)     Monday     BAYVIEW   \n",
      "\n",
      "            Resolution                        Address           X          Y  \\\n",
      "0       ARREST, BOOKED             OAK ST / LAGUNA ST -122.425892  37.774599   \n",
      "1       ARREST, BOOKED             OAK ST / LAGUNA ST -122.425892  37.774599   \n",
      "2       ARREST, BOOKED      VANNESS AV / GREENWICH ST -122.424363  37.800414   \n",
      "3                 NONE       1500 Block of LOMBARD ST -122.426995  37.800873   \n",
      "4                 NONE      100 Block of BRODERICK ST -122.438738  37.771541   \n",
      "5                 NONE            0 Block of TEDDY AV -122.403252  37.713431   \n",
      "6                 NONE            AVALON AV / PERU AV -122.423327  37.725138   \n",
      "7                 NONE       KIRKWOOD AV / DONAHUE ST -122.371274  37.727564   \n",
      "8                 NONE           600 Block of 47TH AV -122.508194  37.776601   \n",
      "9                 NONE  JEFFERSON ST / LEAVENWORTH ST -122.419088  37.807802   \n",
      "10                NONE  JEFFERSON ST / LEAVENWORTH ST -122.419088  37.807802   \n",
      "11                NONE          0 Block of ESCOLTA WY -122.487983  37.737667   \n",
      "12                NONE             TURK ST / JONES ST -122.412414  37.783004   \n",
      "13                NONE         FILLMORE ST / GEARY BL -122.432915  37.784353   \n",
      "14                NONE       200 Block of WILLIAMS AV -122.397744  37.729935   \n",
      "15                NONE          0 Block of MENDELL ST -122.383692  37.743189   \n",
      "16                NONE             EDDY ST / JONES ST -122.412597  37.783932   \n",
      "17                NONE         GODEUS ST / MISSION ST -122.421682  37.742822   \n",
      "18      ARREST, BOOKED         MENDELL ST / HUDSON AV -122.386401  37.738983   \n",
      "19                NONE          100 Block of JONES ST -122.412250  37.782556   \n",
      "20                NONE         200 Block of EVELYN WY -122.449389  37.742669   \n",
      "21                NONE      1600 Block of VALENCIA ST -122.420272  37.747332   \n",
      "22                NONE          100 Block of JONES ST -122.412250  37.782556   \n",
      "23                NONE          100 Block of JONES ST -122.412250  37.782556   \n",
      "24                NONE       FILLMORE ST / LOMBARD ST -122.436049  37.799841   \n",
      "25                NONE       300 Block of OFARRELL ST -122.410509  37.786043   \n",
      "26                NONE          2000 Block of BUSH ST -122.431018  37.787388   \n",
      "27                NONE        500 Block of COLLEGE AV -122.423656  37.732556   \n",
      "28                NONE          19TH AV / SANTIAGO ST -122.475773  37.744919   \n",
      "29                NONE          2000 Block of 41ST AV -122.499787  37.748518   \n",
      "...                ...                            ...         ...        ...   \n",
      "878019   ARREST, CITED             6TH ST / MARKET ST -122.410294  37.782231   \n",
      "878020   ARREST, CITED          VAN NESS AV / TURK ST -122.420642  37.781961   \n",
      "878021  NOT PROSECUTED           SANCHEZ ST / 14TH ST -122.431191  37.767595   \n",
      "878022            NONE           17TH ST / MISSION ST -122.419516  37.763429   \n",
      "878023            NONE          600 Block of ELLIS ST -122.416894  37.784286   \n",
      "878024   ARREST, CITED     600 Block of DIVISADERO ST -122.437781  37.775483   \n",
      "878025  ARREST, BOOKED         NEWHALL ST / GALVEZ AV -122.387710  37.740674   \n",
      "878026  ARREST, BOOKED         NEWHALL ST / GALVEZ AV -122.387710  37.740674   \n",
      "878027  ARREST, BOOKED         900 Block of MARKET ST -122.409708  37.782828   \n",
      "878028            NONE             6TH ST / MARKET ST -122.410294  37.782231   \n",
      "878029  ARREST, BOOKED   1400 Block of GOLDEN GATE AV -122.434423  37.779193   \n",
      "878030  ARREST, BOOKED   1400 Block of GOLDEN GATE AV -122.434423  37.779193   \n",
      "878031            NONE            3RD ST / NEWCOMB AV -122.390417  37.735593   \n",
      "878032   ARREST, CITED         GEARY BL / FRANKLIN ST -122.423031  37.785482   \n",
      "878033   ARREST, CITED          1000 Block of 22ND AV -122.391668  37.757793   \n",
      "878034   ARREST, CITED          1000 Block of 22ND AV -122.391668  37.757793   \n",
      "878035            NONE       1300 Block of WEBSTER ST -122.431046  37.783030   \n",
      "878036            NONE       1300 Block of WEBSTER ST -122.431046  37.783030   \n",
      "878037            NONE       1300 Block of WEBSTER ST -122.431046  37.783030   \n",
      "878038  ARREST, BOOKED           TAYLOR ST / GEARY ST -122.411519  37.786941   \n",
      "878039   ARREST, CITED        POLK ST / CALIFORNIA ST -122.420692  37.790577   \n",
      "878040            NONE        2800 Block of FOLSOM ST -122.414073  37.751685   \n",
      "878041   ARREST, CITED           CLEMENT ST / 14TH AV -122.472985  37.782552   \n",
      "878042  ARREST, BOOKED       1500 Block of SHAFTER AV -122.389769  37.730564   \n",
      "878043  ARREST, BOOKED       1500 Block of SHAFTER AV -122.389769  37.730564   \n",
      "878044            NONE     FARALLONES ST / CAPITOL AV -122.459033  37.714056   \n",
      "878045            NONE           600 Block of EDNA ST -122.447364  37.731948   \n",
      "878046            NONE             5TH ST / FOLSOM ST -122.403390  37.780266   \n",
      "878047            NONE           TOWNSEND ST / 2ND ST -122.390531  37.780607   \n",
      "878048            NONE       1800 Block of NEWCOMB AV -122.394926  37.738212   \n",
      "\n",
      "        month  year  day  hour  holidays       dayparts      DATE  PRCP  SNOW  \\\n",
      "0           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "1           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "2           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "3           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "4           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "5           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "6           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "7           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "8           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "9           5  2015   13    23     False   late_evening  20150513   0.0   0.0   \n",
      "10          5  2015   13    22     False   late_evening  20150513   0.0   0.0   \n",
      "11          5  2015   13    22     False   late_evening  20150513   0.0   0.0   \n",
      "12          5  2015   13    22     False   late_evening  20150513   0.0   0.0   \n",
      "13          5  2015   13    22     False   late_evening  20150513   0.0   0.0   \n",
      "14          5  2015   13    22     False   late_evening  20150513   0.0   0.0   \n",
      "15          5  2015   13    22     False   late_evening  20150513   0.0   0.0   \n",
      "16          5  2015   13    22     False   late_evening  20150513   0.0   0.0   \n",
      "17          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "18          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "19          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "20          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "21          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "22          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "23          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "24          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "25          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "26          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "27          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "28          5  2015   13    21     False   late_evening  20150513   0.0   0.0   \n",
      "29          5  2015   13    20     False  early_evening  20150513   0.0   0.0   \n",
      "...       ...   ...  ...   ...       ...            ...       ...   ...   ...   \n",
      "878019      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878020      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878021      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878022      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878023      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878024      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878025      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878026      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878027      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878028      1  2003    6     2     False     late_night  20030106   0.0   0.0   \n",
      "878029      1  2003    6     1     False     late_night  20030106   0.0   0.0   \n",
      "878030      1  2003    6     1     False     late_night  20030106   0.0   0.0   \n",
      "878031      1  2003    6     1     False     late_night  20030106   0.0   0.0   \n",
      "878032      1  2003    6     1     False     late_night  20030106   0.0   0.0   \n",
      "878033      1  2003    6     1     False     late_night  20030106   0.0   0.0   \n",
      "878034      1  2003    6     1     False     late_night  20030106   0.0   0.0   \n",
      "878035      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878036      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878037      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878038      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878039      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878040      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878041      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878042      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878043      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878044      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878045      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878046      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878047      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "878048      1  2003    6     0     False     late_night  20030106   0.0   0.0   \n",
      "\n",
      "        TMAX  TMIN  \n",
      "0         61    50  \n",
      "1         61    50  \n",
      "2         61    50  \n",
      "3         61    50  \n",
      "4         61    50  \n",
      "5         61    50  \n",
      "6         61    50  \n",
      "7         61    50  \n",
      "8         61    50  \n",
      "9         61    50  \n",
      "10        61    50  \n",
      "11        61    50  \n",
      "12        61    50  \n",
      "13        61    50  \n",
      "14        61    50  \n",
      "15        61    50  \n",
      "16        61    50  \n",
      "17        61    50  \n",
      "18        61    50  \n",
      "19        61    50  \n",
      "20        61    50  \n",
      "21        61    50  \n",
      "22        61    50  \n",
      "23        61    50  \n",
      "24        61    50  \n",
      "25        61    50  \n",
      "26        61    50  \n",
      "27        61    50  \n",
      "28        61    50  \n",
      "29        61    50  \n",
      "...      ...   ...  \n",
      "878019    69    52  \n",
      "878020    69    52  \n",
      "878021    69    52  \n",
      "878022    69    52  \n",
      "878023    69    52  \n",
      "878024    69    52  \n",
      "878025    69    52  \n",
      "878026    69    52  \n",
      "878027    69    52  \n",
      "878028    69    52  \n",
      "878029    69    52  \n",
      "878030    69    52  \n",
      "878031    69    52  \n",
      "878032    69    52  \n",
      "878033    69    52  \n",
      "878034    69    52  \n",
      "878035    69    52  \n",
      "878036    69    52  \n",
      "878037    69    52  \n",
      "878038    69    52  \n",
      "878039    69    52  \n",
      "878040    69    52  \n",
      "878041    69    52  \n",
      "878042    69    52  \n",
      "878043    69    52  \n",
      "878044    69    52  \n",
      "878045    69    52  \n",
      "878046    69    52  \n",
      "878047    69    52  \n",
      "878048    69    52  \n",
      "\n",
      "[878049 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "# When trying to map the dates getting error: 'str' object does not support item assignment\n",
    "\n",
    "# loop over weather data and update weather variables according to dates\n",
    "#for d in weather[\"DATE\"]:\n",
    "    #print(d)\n",
    "   # if not train[train[\"Dates\"]==d].empty:\n",
    "   #     train[train[\"Dates\"]==d][\"rain\"] = weather[weather[\"DATE\"]==d].PRCP\n",
    "   #     train[train[\"Dates\"]==d][\"max_temp\"] = weather[weather[\"DATE\"]==d].TMAX\n",
    "   #     train[train[\"Dates\"]==d][\"min_temp\"] = weather[weather[\"DATE\"]==d].TMIN\n",
    "    \n",
    "    #if not test[test[\"Dates\"]==d].empty:\n",
    "  #      test[test[\"Dates\"]==d][\"rain\"] = weather[weather[\"DATE\"]==d].PRCP\n",
    "      #  test[test[\"Dates\"]==d][\"max_temp\"] = weather[weather[\"DATE\"]==d].TMAX\n",
    "     #   test[test[\"Dates\"]==d][\"min_temp\"] = weather[weather[\"DATE\"]==d].TMIN\n",
    "\n",
    "        \n",
    "weather_train = pd.merge(train, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "weather_test = pd.merge(test, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "print(weather_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    878049.000000\n",
       "mean         37.771020\n",
       "std           0.456893\n",
       "min          37.707879\n",
       "25%          37.752427\n",
       "50%          37.775421\n",
       "75%          37.784369\n",
       "max          90.000000\n",
       "Name: Y, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data indicates outliers with latitude = 90 (aka the North Pole). Test data has these same outliers.\n",
    "train.Y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove cells where latitude > 40\n",
    "train = train[train.Y < 38]\n",
    "test = test[test.Y < 38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877982, 17)\n",
      "(884186, 15)\n",
      "Cases removed from train data = 67\n",
      "Cases removed from test data = 76\n"
     ]
    }
   ],
   "source": [
    "# print new shape\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print(\"Cases removed from train data =\", np.sum(878049 - train.shape[0]))\n",
    "print(\"Cases removed from test data =\", np.sum(884262 - test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y', 'month', 'year', 'hour', 'holidays',\n",
      "       'dayparts', 'rain', 'max_temp', 'min_temp'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'month',\n",
      "       'year', 'hour', 'holidays', 'dayparts', 'rain', 'max_temp', 'min_temp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode string features into numeric features\n",
    "LE = preprocessing.LabelEncoder()\n",
    "\n",
    "train_data_all = np.column_stack((LE.fit_transform(train['Dates']),\n",
    "                                 LE.fit_transform(train['DayOfWeek']),\n",
    "                                 LE.fit_transform(train['PdDistrict']),\n",
    "                                 LE.fit_transform(train['Address']),\n",
    "                                 train['X'],\n",
    "                                 train['Y']))\n",
    "\n",
    "train_labels_all = np.array(train['Category'])\n",
    "\n",
    "test_data_all = np.column_stack((LE.fit_transform(test['Dates']),\n",
    "                                LE.fit_transform(test['DayOfWeek']),\n",
    "                                LE.fit_transform(test['PdDistrict']),\n",
    "                                LE.fit_transform(test['Address']),\n",
    "                                test['X'],\n",
    "                                test['Y']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data and set aside 20% as development data\n",
    "n = train_data_all.shape[0]\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(train_data_all.shape[0]))\n",
    "\n",
    "train_data_all = train_data_all[shuffle]\n",
    "train_labels_all = train_labels_all[shuffle]\n",
    "\n",
    "n_train = int(0.8*n)\n",
    "\n",
    "train_data = train_data_all[:n_train,:]\n",
    "train_labels = train_labels_all[:n_train]\n",
    "dev_data = train_data_all[n_train:,:]\n",
    "dev_labels = train_labels_all[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (702439, 6)\n",
      "train_labels shape is (702439,)\n",
      "dev_data shape is (175610, 6)\n",
      "dev_labels shape is (175610,)\n",
      "train_data_all shape is (878049, 6)\n",
      "train_data_all shape is (878049, 6)\n",
      "train_labels_all shape is (878049,)\n",
      "test_data_all shape is (884262, 6)\n"
     ]
    }
   ],
   "source": [
    "# print shapes and some data to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)\n",
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: csv: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Save arrays as CSV files in a subdirectory\n",
    "\n",
    "# NOTE: mkdir will make a \"csv\" directory in your local repo if there is not already one there.\n",
    "# It will return an error if the directory already exists in your local repo\n",
    "# but that will not impact how this code runs\n",
    "\n",
    "! mkdir csv\n",
    "np.savetxt(\"csv/train_data.csv\", train_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/train_labels.csv\", train_labels, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_data.csv\", dev_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_labels.csv\", dev_labels, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/train_data_all.csv\", train_data_all, delimiter=\",\")\n",
    "np.savetxt(\"csv/train_labels_all.csv\", train_labels_all, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/test_data_all.csv\", test_data_all, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zip up the CSV files\n",
    "\n",
    "# **IMPORTANT**  This code will rewrite the existing data.zip file in your local repo\n",
    "# You will need to push it to the group repo for everyone to have the updated zip file\n",
    "\n",
    "zip_files = zipfile.ZipFile(\"data.zip\", \"w\")\n",
    "zip_files.write(\"csv/train_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/train_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/dev_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/dev_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/train_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/train_labels_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/test_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
