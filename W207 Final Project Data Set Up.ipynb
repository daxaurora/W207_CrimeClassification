{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W207 Fall 2017 Final ProjectÂ¶\n",
    "## Data Set Up - Data Cleaning and Feature Engineering\n",
    "Laura Williams, Kim Vignola, Cyprian Gascoigne  \n",
    "SF Crime Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads raw data (saved in a zip file) from Kaggle, processes and organizes the data for training a variety of machine learning models, and outputs the data as zipped csv files that other notebooks can unzip and use to train different models.\n",
    "\n",
    "The intention is that data cleaning and/or feature engineering will be added to this file as we progress through the project and look for additional way to process the data to improve our predictions.\n",
    "\n",
    "For ease of processing this data, exploratory data analysis will be in a separate notebook.\n",
    "\n",
    "Single zipped output file (called data.zip) includes:  \n",
    "\n",
    "1) train_data.csv and train_labels.csv - includes 80% of the total training data, for training models that are not yet going to be submitted to Kaggle\n",
    "\n",
    "2) dev_data.csv and dev_labels.csv - includes 20% of the total training data, for testing models before they are submitted to Kaggle\n",
    "\n",
    "3) train_data_all.csv and train_labels_all.csv - includes all the training data. After testing models with the train and dev data split above, train the model from this full set of data for submission to Kaggle.\n",
    "\n",
    "4) test_data_all.csv - create predictions on this data for submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip raw data into a subdirectory \n",
    "unzip_files = zipfile.ZipFile(\"raw_data.zip\", \"r\")\n",
    "unzip_files.extractall(\"raw_data\")\n",
    "unzip_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read CSV files into pandas dataframes\n",
    "train = pd.read_csv(\"raw_data/train.csv\")\n",
    "test = pd.read_csv(\"raw_data/test.csv\")\n",
    "weather = pd.read_csv(\"raw_data/SF_county.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import datetime and holiday modules (note this takes a a few minutes to run)\n",
    "from datetime import datetime, timedelta, date\n",
    "import holidays\n",
    "\n",
    "# extract month, year and hour from both datasets\n",
    "train[\"month\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "train[\"year\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "train[\"hour\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "#train[\"day\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "\n",
    "test[\"month\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "test[\"year\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "test[\"hour\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "#test[\"day\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "\n",
    "# map holidays\n",
    "US_Holidays = holidays.UnitedStates()\n",
    "train[\"holidays\"] = train[\"Dates\"].map(lambda x: x in US_Holidays)\n",
    "test[\"holidays\"] = test[\"Dates\"].map(lambda x: x in US_Holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from datetime import timedelta, date I Will mess around with this later\n",
    "#print(\"2015-1-1\" in US_Holidays)\n",
    "#print(date(2015,1,1)in US_Holidays)\n",
    "#print(date(train[\"year\"], train[\"month\"], 1) + timedelta(days = 1))\n",
    "\n",
    "# should also likely include seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary for dayparts\n",
    "time_periods = {6:\"early_morning\", 7:\"early_morning\", 8:\"early_morning\", \n",
    "               9:\"late_morning\", 10:\"late_morning\", 11:\"late_morning\",\n",
    "              12:\"early_afternoon\", 13:\"early_afternoon\", 14:\"early_afternoon\",\n",
    "              15:\"late_afternoon\", 16:\"late_afternoon\", 17:\"late_afternoon\",\n",
    "              18:\"early_evening\",  19:\"early_evening\",  20:\"early_evening\",\n",
    "              21:\"late_evening\", 22:\"late_evening\", 23:\"late_evening\",\n",
    "              0:\"late_night\", 1:\"late_night\", 2:\"late_night\",\n",
    "              3:\"late_night\", 4:\"late_night\", 5:\"late_night\"}\n",
    "\n",
    "# map time periods\n",
    "train[\"dayparts\"] = train[\"hour\"].map(time_periods)\n",
    "test[\"dayparts\"] = test[\"hour\"].map(time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up weather data\n",
    "del weather['NAME']\n",
    "weather[\"SNOW\"] = weather[\"SNOW\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop time from train and test date fields to be able to map Dates against weather data; remove hyphens too.\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "\n",
    "# Convert Weather DATE to same format as train and test data\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strptime(x,\"%m/%d/%y\"))\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150513 object\n",
      "20150510 object\n",
      "20030101 object\n"
     ]
    }
   ],
   "source": [
    "# confirm that dates are now in the same format\n",
    "print(train[\"Dates\"][0], train[\"Dates\"].dtypes)\n",
    "print(test[\"Dates\"][0], test[\"Dates\"].dtypes)\n",
    "print(weather[\"DATE\"][0], weather[\"DATE\"].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# convert date objects to numeric\n",
    "train[\"Dates\"] = pd.to_numeric(train[\"Dates\"])\n",
    "test[\"Dates\"] = pd.to_numeric(test[\"Dates\"])\n",
    "weather[\"DATE\"] = pd.to_numeric(weather[\"DATE\"])\n",
    "print(type(train[\"Dates\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# left merge weather data based on dates\n",
    "weather_train = pd.merge(train, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "del weather_train['DATE']\n",
    "weather_test = pd.merge(test, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "del weather_test['DATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    878049.000000\n",
       "mean         37.771020\n",
       "std           0.456893\n",
       "min          37.707879\n",
       "25%          37.752427\n",
       "50%          37.775421\n",
       "75%          37.784369\n",
       "max          90.000000\n",
       "Name: Y, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data indicates outliers with latitude = 90 (aka the North Pole). Test data has these same outliers.\n",
    "weather_train.Y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove cells where latitude > 38\n",
    "weather_train = weather_train[weather_train.Y < 38]\n",
    "weather_test = weather_test[weather_test.Y < 38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877982, 18)\n",
      "(884186, 16)\n",
      "Cases removed from train data = 67\n",
      "Cases removed from test data = 76\n"
     ]
    }
   ],
   "source": [
    "# print new shape\n",
    "print(weather_train.shape)\n",
    "print(weather_test.shape)\n",
    "\n",
    "print(\"Cases removed from train data =\", np.sum(878049 - weather_train.shape[0]))\n",
    "print(\"Cases removed from test data =\", np.sum(884262 - weather_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y', 'month', 'year', 'hour', 'holidays',\n",
      "       'dayparts'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'month',\n",
      "       'year', 'hour', 'holidays', 'dayparts'],\n",
      "      dtype='object')\n",
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y', 'month', 'year', 'hour', 'holidays',\n",
      "       'dayparts', 'PRCP', 'SNOW', 'TMAX', 'TMIN'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'month',\n",
      "       'year', 'hour', 'holidays', 'dayparts', 'PRCP', 'SNOW', 'TMAX', 'TMIN'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(test.columns)\n",
    "print(weather_train.columns)\n",
    "print(weather_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# Encode string features into numeric features\n",
    "LE = preprocessing.LabelEncoder()\n",
    "\n",
    "train_data_all = pd.get_dummies(weather_train, columns = [\"PdDistrict\", \"DayOfWeek\", \"month\", \"year\", \"dayparts\"])\n",
    "del train_data_all[\"Descript\"]\n",
    "del train_data_all[\"Resolution\"]\n",
    "#del train_data_all[\"day\"]\n",
    "train_labels_all = np.array(train_data_all['Category'])\n",
    "del train_data_all[\"Category\"]\n",
    "\n",
    "train_data_all[\"Dates\"] = LE.fit_transform(train_data_all[\"Dates\"])\n",
    "train_data_all[\"Address\"] = LE.fit_transform(train_data_all[\"Address\"])\n",
    "train_data_all.reindex()\n",
    "\n",
    "test_data_all = pd.get_dummies(weather_test, columns = [\"PdDistrict\", \"DayOfWeek\", \"month\", \"year\", \"dayparts\"])\n",
    "\n",
    "test_data_all[\"Dates\"] = LE.fit_transform(test_data_all[\"Dates\"])\n",
    "test_data_all[\"Address\"] = LE.fit_transform(test_data_all[\"Address\"])\n",
    "del test_data_all[\"Id\"]\n",
    "#del test_data_all[\"day\"]\n",
    "                                 \n",
    "print(test_data_all.columns == train_data_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data and set aside 20% as development data\n",
    "train_data_all = train_data_all.values\n",
    "test_data_all = test_data_all.values\n",
    "n = train_data_all.shape[0]\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(train_data_all.shape[0]))\n",
    "\n",
    "train_data_all = train_data_all[shuffle]\n",
    "train_labels_all = train_labels_all[shuffle]\n",
    "\n",
    "n_train = int(0.8*n)\n",
    "\n",
    "train_data = train_data_all[:n_train,:]\n",
    "train_labels = train_labels_all[:n_train]\n",
    "dev_data = train_data_all[n_train:,:]\n",
    "dev_labels = train_labels_all[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (702385, 59)\n",
      "train_labels shape is (702385,)\n",
      "dev_data shape is (175597, 59)\n",
      "dev_labels shape is (175597,)\n",
      "train_data_all shape is (877982, 59)\n",
      "train_data_all shape is (877982, 59)\n",
      "train_labels_all shape is (877982,)\n",
      "test_data_all shape is (884186, 59)\n"
     ]
    }
   ],
   "source": [
    "# print shapes and some data to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)\n",
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save arrays as CSV files in a subdirectory\n",
    "\n",
    "# NOTE: mkdir will make a \"csv\" directory in your local repo if there is not already one there.\n",
    "# It will return an error if the directory already exists in your local repo\n",
    "# but that will not impact how this code runs\n",
    "\n",
    "! mkdir csv\n",
    "np.savetxt(\"csv/train_data.csv\", train_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/train_labels.csv\", train_labels, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_data.csv\", dev_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_labels.csv\", dev_labels, fmt=\"%s\", delimiter=\",\")\n",
    "#np.savetxt(\"csv/train_data_all.csv\", train_data_all, delimiter=\",\")\n",
    "#np.savetxt(\"csv/train_labels_all.csv\", train_labels_all, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/test_data_all.csv\", test_data_all, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zip up the CSV files\n",
    "\n",
    "# **IMPORTANT**  This code will rewrite the existing data.zip file in your local repo\n",
    "# You will need to push it to the group repo for everyone to have the updated zip file\n",
    "\n",
    "zip_files = zipfile.ZipFile(\"data.zip\", \"w\")\n",
    "zip_files.write(\"csv/train_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/train_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/dev_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/dev_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "#zip_files.write(\"csv/train_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "#zip_files.write(\"csv/train_labels_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/test_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy =  0.223785144393\n"
     ]
    }
   ],
   "source": [
    "# Get baseline KNN accuracy with new features\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors = 1, n_jobs = -1)\n",
    "neigh.fit(train_data, train_labels)\n",
    "knn_pred = neigh.predict(dev_data)\n",
    "print(\"KNN Accuracy = \", np.mean(knn_pred == dev_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
