{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W207 Fall 2017 Final ProjectÂ¶\n",
    "## Data Set Up - Data Cleaning and Feature Engineering\n",
    "Laura Williams, Kim Vignola, Cyprian Gascoigne  \n",
    "SF Crime Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads raw data (saved in a zip file) from Kaggle, processes and organizes the data for training a variety of machine learning models, and outputs the data as zipped csv files that other notebooks can unzip and use to train different models.\n",
    "\n",
    "The intention is that data cleaning and/or feature engineering will be added to this file as we progress through the project and look for additional way to process the data to improve our predictions.\n",
    "\n",
    "For ease of processing this data, exploratory data analysis will be done separately.\n",
    "\n",
    "Resulting zipped files will include: \n",
    "\n",
    "1) train_data.csv and train_labels.csv - includes 80% of the total training data, for training models that are not yet going to be submitted to Kaggle\n",
    "\n",
    "2) dev_data.csv and dev_labels.csv - includes 20% of the total training data, for testing models before they are submitted to Kaggle\n",
    "\n",
    "3) train_data_all.csv and train_labels_all.csv - includes all the training data. After testing models with the train and dev data split above, train the model from this full set of data for submission to Kaggle.\n",
    "\n",
    "4) test_data_all.csv - create predictions on this data for submission to Kaggle.\n",
    "\n",
    "Weather data for San Franscisco County was added to this analysis.\n",
    "Source: https://www.ncdc.noaa.gov/cdo-web/search\n",
    "Report: Daily Summaries, Date Range: 1/1/2003 - 12/31/2015, Search for: Counties/San Francisco, Station: SAN FRANCISCO DOWNTOWN, CA US, Metrics: Precipitation, Maximum Temperature, Minimum Temperature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: holidays package is not native with anaconda and may need to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta, date\n",
    "import holidays\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip raw data into a subdirectory \n",
    "unzip_files = zipfile.ZipFile(\"raw_data.zip\", \"r\")\n",
    "unzip_files.extractall(\"raw_data\")\n",
    "unzip_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read CSV files into pandas dataframes\n",
    "train = pd.read_csv(\"raw_data/train.csv\")\n",
    "test = pd.read_csv(\"raw_data/test.csv\")\n",
    "weather = pd.read_csv(\"raw_data/SF_county.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract month, year and hour from both datasets\n",
    "train[\"month\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "train[\"year\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "train[\"hour\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "train[\"day\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "\n",
    "test[\"month\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "test[\"year\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "test[\"hour\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "test[\"day\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "\n",
    "# map holidays\n",
    "US_Holidays = holidays.UnitedStates()\n",
    "train[\"holidays\"] = train[\"Dates\"].map(lambda x: x in US_Holidays)\n",
    "test[\"holidays\"] = test[\"Dates\"].map(lambda x: x in US_Holidays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I Will mess around with this later\n",
    "#print(\"2015-1-1\" in US_Holidays)\n",
    "#print(date(2015,1,1)in US_Holidays)\n",
    "#print(date(train[\"year\"], train[\"month\"], 1) + timedelta(days = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull out first day of the month as it ranks first for crime volume and specific crimes may be associated with the day\n",
    "train[\"first_day\"] = [1 if x==1 else 0 for x in train[\"day\"]]\n",
    "test[\"first_day\"] = [1 if x==1 else 0 for x in test[\"day\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a bucket variable for month_year\n",
    "\n",
    "train[\"month_year\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "train[\"month_year\"] = train[\"month_year\"].map(lambda x: datetime.strftime(x,\"%Y-%m\"))\n",
    "\n",
    "test[\"month_year\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "test[\"month_year\"] = test[\"month_year\"].map(lambda x: datetime.strftime(x,\"%Y-%m\"))\n",
    "\n",
    "# would month_day have any value?\n",
    "#train[\"month_day\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "#train[\"month_day\"] = train[\"month_day\"].map(lambda x: datetime.strftime(x,\"%m-%d\"))\n",
    "#test[\"month_day\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "#test[\"month_day\"] = test[\"month_day\"].map(lambda x: datetime.strftime(x,\"%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse out day of year for bucketing seasons\n",
    "\n",
    "train[\"doy\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").timetuple().tm_yday)\n",
    "test[\"doy\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").timetuple().tm_yday)\n",
    "\n",
    "train[\"spring\"] = [1 if x in range(81,173) else 0 for x in train[\"doy\"]]\n",
    "train[\"summer\"] = [1 if x in range(173,265) else 0 for x in train[\"doy\"]]\n",
    "train[\"fall\"] = [1 if x in range(265,356) else 0 for x in train[\"doy\"]]\n",
    "train[\"winter\"] = [1 if x in range(1,81) or x in range(356,366) else 0 for x in train[\"doy\"]]\n",
    "\n",
    "test[\"spring\"] = [1 if x in range(81,173) else 0 for x in test[\"doy\"]]\n",
    "test[\"summer\"] = [1 if x in range(173,265) else 0 for x in test[\"doy\"]]\n",
    "test[\"fall\"] = [1 if x in range(265,356) else 0 for x in test[\"doy\"]]\n",
    "test[\"winter\"] = [1 if x in range(1,81) or x in range(356,366) else 0 for x in test[\"doy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary for bucketing hours\n",
    "time_periods = {6:\"early_morning\", 7:\"early_morning\", 8:\"early_morning\", \n",
    "               9:\"late_morning\", 10:\"late_morning\", 11:\"late_morning\",\n",
    "              12:\"early_afternoon\", 13:\"early_afternoon\", 14:\"early_afternoon\",\n",
    "              15:\"late_afternoon\", 16:\"late_afternoon\", 17:\"late_afternoon\",\n",
    "              18:\"early_evening\",  19:\"early_evening\",  20:\"early_evening\",\n",
    "              21:\"late_evening\", 22:\"late_evening\", 23:\"late_evening\",\n",
    "              0:\"late_night\", 1:\"late_night\", 2:\"late_night\",\n",
    "              3:\"late_night\", 4:\"late_night\", 5:\"late_night\"}\n",
    "\n",
    "# map time periods to dayparts\n",
    "train[\"dayparts\"] = train[\"hour\"].map(time_periods)\n",
    "test[\"dayparts\"] = test[\"hour\"].map(time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up weather data\n",
    "del weather['NAME']\n",
    "weather[\"SNOW\"] = weather[\"SNOW\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop time from train and test date fields to be able to map Dates against weather data; remove hyphens too.\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "\n",
    "# Convert Weather DATE to same format as train and test data\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strptime(x,\"%m/%d/%y\"))\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# convert date objects to numeric\n",
    "train[\"Dates\"] = pd.to_numeric(train[\"Dates\"])\n",
    "test[\"Dates\"] = pd.to_numeric(test[\"Dates\"])\n",
    "weather[\"DATE\"] = pd.to_numeric(weather[\"DATE\"])\n",
    "print(type(train[\"Dates\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# left merge weather data based on dates\n",
    "weather_train = pd.merge(train, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "del weather_train['DATE']\n",
    "del weather_train[\"SNOW\"]\n",
    "weather_test = pd.merge(test, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "del weather_test['DATE']\n",
    "del weather_test[\"SNOW\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, fix outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data indicates outliers with latitude = 90. Test data has these same outliers.\n",
    "# Set latitiude to the median of the district where the crime occured.\n",
    "districts = set(weather_train[\"PdDistrict\"])\n",
    "medians = {el:0 for el in districts}\n",
    "for district in districts:\n",
    "    medians[district] = weather_train[\"Y\"][weather_train[\"PdDistrict\"] == district].median()\n",
    "weather_train.loc[weather_train.Y > 38, \"Y\"] = weather_train[weather_train.Y > 38][\"PdDistrict\"].map(lambda x: \n",
    "                                                                                                     medians[x])\n",
    "weather_test.loc[weather_test.Y > 38, \"Y\"] = weather_test[weather_test.Y > 38][\"PdDistrict\"].map(lambda x : \n",
    "                                                                                                 medians[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 25)\n",
      "(884262, 23)\n",
      "Cases removed from train data = 0\n",
      "Cases removed from test data = 0\n",
      "Cases fixed in the train data = 67\n",
      "Cases fixed in the test data = 76\n"
     ]
    }
   ],
   "source": [
    "#print new shape\n",
    "print(weather_train.shape)\n",
    "print(weather_test.shape)\n",
    "\n",
    "print(\"Cases removed from train data =\", np.sum(878049 - weather_train.shape[0]))\n",
    "print(\"Cases removed from test data =\", np.sum(884262 - weather_test.shape[0]))\n",
    "print(\"Cases fixed in the train data =\", len(train[train.Y>38]))\n",
    "print(\"Cases fixed in the test data =\", len(test[test.Y > 38]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y', 'month', 'year', 'hour', 'day',\n",
      "       'holidays', 'first_day', 'month_year', 'doy', 'spring', 'summer',\n",
      "       'fall', 'winter', 'dayparts'],\n",
      "      dtype='object')\n",
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y', 'month', 'year', 'hour', 'day',\n",
      "       'holidays', 'first_day', 'month_year', 'doy', 'spring', 'summer',\n",
      "       'fall', 'winter', 'dayparts', 'PRCP', 'TMAX', 'TMIN'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'month',\n",
      "       'year', 'hour', 'day', 'holidays', 'first_day', 'month_year', 'doy',\n",
      "       'spring', 'summer', 'fall', 'winter', 'dayparts'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'month',\n",
      "       'year', 'hour', 'day', 'holidays', 'first_day', 'month_year', 'doy',\n",
      "       'spring', 'summer', 'fall', 'winter', 'dayparts', 'PRCP', 'TMAX',\n",
      "       'TMIN'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(weather_train.columns)\n",
    "print(test.columns)\n",
    "print(weather_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "weather_train[\"holidays\"] = weather_train[\"holidays\"].astype(int)\n",
    "weather_test[\"holidays\"] = weather_test[\"holidays\"].astype(int)\n",
    "print(type(weather_train[\"holidays\"][0]))\n",
    "print(type(weather_test[\"holidays\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Encode string features into numeric features\n",
    "LE = preprocessing.LabelEncoder()\n",
    "\n",
    "train_data_all = pd.get_dummies(weather_train, columns = ['Dates', 'Category', 'DayOfWeek', 'PdDistrict',\n",
    "       'Address', 'X', 'Y', 'month', 'year', 'hour', 'holidays', 'first_day', 'month_year', 'spring', 'summer',\n",
    "       'fall', 'winter', 'dayparts', 'PRCP', 'TMAX', 'TMIN'])\n",
    "del train_data_all[\"Dates\"]\n",
    "del train_data_all[\"Descript\"]\n",
    "del train_data_all[\"Resolution\"]\n",
    "del train_data_all[\"day\"]\n",
    "del train_data_all[\"doy\"]\n",
    "train_labels_all = np.array(train_data_all['Category'])\n",
    "del train_data_all[\"Category\"]\n",
    "\n",
    "train_data_all[\"Address\"] = LE.fit_transform(train_data_all[\"Address\"])\n",
    "train_data_all.reindex()\n",
    "\n",
    "test_data_all = pd.get_dummies(weather_test, columns = ['Dates', 'DayOfWeek', 'PdDistrict',\n",
    "       'Address', 'X', 'Y', 'month', 'year', 'hour', 'holidays', 'first_day', 'month_year', 'spring', 'summer',\n",
    "       'fall', 'winter', 'dayparts', 'PRCP', 'TMAX', 'TMIN'])\n",
    "\n",
    "test_data_all[\"Address\"] = LE.fit_transform(test_data_all[\"Address\"])\n",
    "del test_data_all[\"Id\"]\n",
    "del test_data_all[\"day\"]\n",
    "del test_data_all[\"doy\"]\n",
    "del test_data_all[\"Dates\"]\n",
    "                                 \n",
    "print(test_data_all.columns == train_data_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dates  Address           X          Y  hour  day  holidays  first_day  \\\n",
      "0   2248    19790 -122.425892  37.774599    23   13         0          0   \n",
      "1   2248    19790 -122.425892  37.774599    23   13         0          0   \n",
      "2   2248    22697 -122.424363  37.800414    23   13         0          0   \n",
      "\n",
      "  month_year month_day         ...           year_2013  year_2014  year_2015  \\\n",
      "0    2015-05     05-13         ...                   0          0          1   \n",
      "1    2015-05     05-13         ...                   0          0          1   \n",
      "2    2015-05     05-13         ...                   0          0          1   \n",
      "\n",
      "   dayparts_early_afternoon  dayparts_early_evening  dayparts_early_morning  \\\n",
      "0                         0                       0                       0   \n",
      "1                         0                       0                       0   \n",
      "2                         0                       0                       0   \n",
      "\n",
      "   dayparts_late_afternoon  dayparts_late_evening  dayparts_late_morning  \\\n",
      "0                        0                      1                      0   \n",
      "1                        0                      1                      0   \n",
      "2                        0                      1                      0   \n",
      "\n",
      "   dayparts_late_night  \n",
      "0                    0  \n",
      "1                    0  \n",
      "2                    0  \n",
      "\n",
      "[3 rows x 64 columns]\n",
      "Index(['Dates', 'Address', 'X', 'Y', 'hour', 'day', 'holidays', 'first_day',\n",
      "       'month_year', 'month_day', 'doy', 'seasons', 'PRCP', 'TMAX', 'TMIN',\n",
      "       'PdDistrict_BAYVIEW', 'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE',\n",
      "       'PdDistrict_MISSION', 'PdDistrict_NORTHERN', 'PdDistrict_PARK',\n",
      "       'PdDistrict_RICHMOND', 'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL',\n",
      "       'PdDistrict_TENDERLOIN', 'DayOfWeek_Friday', 'DayOfWeek_Monday',\n",
      "       'DayOfWeek_Saturday', 'DayOfWeek_Sunday', 'DayOfWeek_Thursday',\n",
      "       'DayOfWeek_Tuesday', 'DayOfWeek_Wednesday', 'month_1', 'month_2',\n",
      "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
      "       'month_9', 'month_10', 'month_11', 'month_12', 'year_2003', 'year_2004',\n",
      "       'year_2005', 'year_2006', 'year_2007', 'year_2008', 'year_2009',\n",
      "       'year_2010', 'year_2011', 'year_2012', 'year_2013', 'year_2014',\n",
      "       'year_2015', 'dayparts_early_afternoon', 'dayparts_early_evening',\n",
      "       'dayparts_early_morning', 'dayparts_late_afternoon',\n",
      "       'dayparts_late_evening', 'dayparts_late_morning',\n",
      "       'dayparts_late_night'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_data_all.head(3))\n",
    "print(train_data_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-d8cd89354ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_data_all\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_data_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_data_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_data_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_data_all\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_data_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6340\u001b[0m                                       skipna=skipna)\n\u001b[1;32m   6341\u001b[0m         return self._reduce(f, name, axis=axis, skipna=skipna,\n\u001b[0;32m-> 6342\u001b[0;31m                             numeric_only=numeric_only)\n\u001b[0m\u001b[1;32m   6343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6344\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   5059\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5060\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5061\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5062\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   5048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5049\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5050\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5052\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_agg_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mdtype_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "train_data_all = (train_data_all - train_data_all.mean()) / (train_data_all.std())\n",
    "test_data_all = (test_data_all - train_data_all.mean())/(train_data_all.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Dates  Address           X          Y  hour      holidays          PRCP  \\\n",
      "0  2265.0   6407.0 -122.399588  37.735051  23.0  1.134103e-15  7.976514e-14   \n",
      "1  2265.0   9744.0 -122.391523  37.732432  23.0  1.134103e-15  7.976514e-14   \n",
      "2  2265.0   6336.0 -122.426002  37.792212  23.0  1.134103e-15  7.976514e-14   \n",
      "3  2265.0  10633.0 -122.437394  37.721412  23.0  1.134103e-15  7.976514e-14   \n",
      "4  2265.0  10633.0 -122.437394  37.721412  23.0  1.134103e-15  7.976514e-14   \n",
      "\n",
      "   TMAX  TMIN  PdDistrict_BAYVIEW         ...              year_2013  \\\n",
      "0  57.0  49.0        1.000000e+00         ...           3.143028e-13   \n",
      "1  57.0  49.0        1.000000e+00         ...           3.143028e-13   \n",
      "2  57.0  49.0       -3.280039e-14         ...           3.143028e-13   \n",
      "3  57.0  49.0       -3.280039e-14         ...           3.143028e-13   \n",
      "4  57.0  49.0       -3.280039e-14         ...           3.143028e-13   \n",
      "\n",
      "      year_2014  year_2015  dayparts_early_afternoon  dayparts_early_evening  \\\n",
      "0 -4.016814e-12        1.0             -4.851726e-14            4.313563e-14   \n",
      "1 -4.016814e-12        1.0             -4.851726e-14            4.313563e-14   \n",
      "2 -4.016814e-12        1.0             -4.851726e-14            4.313563e-14   \n",
      "3 -4.016814e-12        1.0             -4.851726e-14            4.313563e-14   \n",
      "4 -4.016814e-12        1.0             -4.851726e-14            4.313563e-14   \n",
      "\n",
      "   dayparts_early_morning  dayparts_late_afternoon  dayparts_late_evening  \\\n",
      "0            4.774700e-14             2.782321e-14                    1.0   \n",
      "1            4.774700e-14             2.782321e-14                    1.0   \n",
      "2            4.774700e-14             2.782321e-14                    1.0   \n",
      "3            4.774700e-14             2.782321e-14                    1.0   \n",
      "4            4.774700e-14             2.782321e-14                    1.0   \n",
      "\n",
      "   dayparts_late_morning  dayparts_late_night  \n",
      "0           5.322651e-14         1.176042e-14  \n",
      "1           5.322651e-14         1.176042e-14  \n",
      "2           5.322651e-14         1.176042e-14  \n",
      "3           5.322651e-14         1.176042e-14  \n",
      "4           5.322651e-14         1.176042e-14  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_data_all[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data and set aside 20% as development data\n",
    "train_data_all = train_data_all.values\n",
    "test_data_all = test_data_all.values\n",
    "n = train_data_all.shape[0]\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(train_data_all.shape[0]))\n",
    "\n",
    "train_data_all = train_data_all[shuffle]\n",
    "train_labels_all = train_labels_all[shuffle]\n",
    "\n",
    "n_train = int(0.8*n)\n",
    "\n",
    "train_data = train_data_all[:n_train,:]\n",
    "train_labels = train_labels_all[:n_train]\n",
    "dev_data = train_data_all[n_train:,:]\n",
    "dev_labels = train_labels_all[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (702439, 58)\n",
      "train_labels shape is (702439,)\n",
      "dev_data shape is (175610, 58)\n",
      "dev_labels shape is (175610,)\n",
      "train_data_all shape is (878049, 58)\n",
      "train_labels_all shape is (878049,)\n",
      "test_data_all shape is (884262, 58)\n"
     ]
    }
   ],
   "source": [
    "# print shapes and some data to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)\n",
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: csv: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Save data as CSV files in a subdirectory\n",
    "\n",
    "# NOTE: mkdir will make a \"csv\" directory in your local repo if there is not already one there.\n",
    "# It will return an error if the directory already exists in your local repo\n",
    "# but that will not impact how this code runs\n",
    "\n",
    "! mkdir csv\n",
    "np.savetxt(\"csv/train_data.csv\", train_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/train_labels.csv\", train_labels, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_data.csv\", dev_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_labels.csv\", dev_labels, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/train_data_all.csv\", train_data_all, delimiter=\",\")\n",
    "np.savetxt(\"csv/train_labels_all.csv\", train_labels_all, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/test_data_all.csv\", test_data_all, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zip up the CSV files\n",
    "\n",
    "# **IMPORTANT**  This code will rewrite existing zip files in your local repo\n",
    "# You will need to push it to the group repo for everyone to have the updated zip file\n",
    "\n",
    "# Full set of training data and labels --> data.zip\n",
    "zip_train_all = zipfile.ZipFile(\"data.zip\", \"w\")\n",
    "zip_train_all.write(\"csv/train_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_train_all.write(\"csv/train_labels_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_train_all.close()\n",
    "\n",
    "# Subset of training data and labels --> data_subset.zip\n",
    "zip_train_subset = zipfile.ZipFile(\"data_subset.zip\", \"w\")\n",
    "zip_train_subset.write(\"csv/train_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_train_subset.write(\"csv/train_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_train_subset.close()\n",
    "\n",
    "\n",
    "# Data used for testing models (test data from Kaggle and our 20% development data) --> testing.zip\n",
    "zip_testing = zipfile.ZipFile(\"testing.zip\", \"w\")\n",
    "zip_testing.write(\"csv/test_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_testing.write(\"csv/dev_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_testing.write(\"csv/dev_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_testing.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
