{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W207 Fall 2017 Final ProjectÂ¶\n",
    "## Data Set Up - Data Cleaning and Feature Engineering\n",
    "Laura Williams, Kim Vignola, Cyprian Gascoigne  \n",
    "SF Crime Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads raw data (saved in a zip file) from Kaggle, processes and organizes the data for training a variety of machine learning models, and outputs the data as zipped csv files that other notebooks can unzip and use to train different models.\n",
    "\n",
    "The intention is that data cleaning and/or feature engineering will be added to this file as we progress through the project and look for additional way to process the data to improve our predictions.\n",
    "\n",
    "For ease of processing this data, exploratory data analysis will be in a separate notebook.\n",
    "\n",
    "Single zipped output file (called data.zip) includes:  \n",
    "\n",
    "1) train_data.csv and train_labels.csv - includes 80% of the total training data, for training models that are not yet going to be submitted to Kaggle\n",
    "\n",
    "2) dev_data.csv and dev_labels.csv - includes 20% of the total training data, for testing models before they are submitted to Kaggle\n",
    "\n",
    "3) train_data_all.csv and train_labels_all.csv - includes all the training data. After testing models with the train and dev data split above, train the model from this full set of data for submission to Kaggle.\n",
    "\n",
    "4) test_data_all.csv - create predictions on this data for submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip raw data into a subdirectory \n",
    "unzip_files = zipfile.ZipFile(\"raw_data.zip\", \"r\")\n",
    "unzip_files.extractall(\"raw_data\")\n",
    "unzip_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read CSV files into pandas dataframes\n",
    "train = pd.read_csv(\"raw_data/train.csv\")\n",
    "test = pd.read_csv(\"raw_data/test.csv\")\n",
    "weather = pd.read_csv(\"raw_data/SF_county.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import datetime and holiday modules (note this takes a a few minutes to run)\n",
    "from datetime import datetime, timedelta, date\n",
    "import holidays\n",
    "\n",
    "# extract month, year and hour from both datasets\n",
    "train[\"month\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "train[\"year\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "train[\"hour\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "#train[\"day\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "\n",
    "test[\"month\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "test[\"year\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "test[\"hour\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "#test[\"day\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "\n",
    "# map holidays\n",
    "US_Holidays = holidays.UnitedStates()\n",
    "train[\"holidays\"] = train[\"Dates\"].map(lambda x: x in US_Holidays)\n",
    "test[\"holidays\"] = test[\"Dates\"].map(lambda x: x in US_Holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from datetime import timedelta, date I Will mess around with this later\n",
    "#print(\"2015-1-1\" in US_Holidays)\n",
    "#print(date(2015,1,1)in US_Holidays)\n",
    "#print(date(train[\"year\"], train[\"month\"], 1) + timedelta(days = 1))\n",
    "\n",
    "# should also likely include seasons.  This is just a placeholder...\n",
    "\n",
    "#seasons = {'Summer':(datetime(year,6,21), datetime(year,9,22)),\n",
    "#           'Autumn':(datetime(year,9,23), datetime(year,12,20)),\n",
    "#           'Spring':(datetime(year,3,21), datetime(year,6,20)),\n",
    "#           'Winter':(datetime(year,3,21), datetime(year,6,20))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for dayparts\n",
    "time_periods = {6:\"early_morning\", 7:\"early_morning\", 8:\"early_morning\", \n",
    "               9:\"late_morning\", 10:\"late_morning\", 11:\"late_morning\",\n",
    "              12:\"early_afternoon\", 13:\"early_afternoon\", 14:\"early_afternoon\",\n",
    "              15:\"late_afternoon\", 16:\"late_afternoon\", 17:\"late_afternoon\",\n",
    "              18:\"early_evening\",  19:\"early_evening\",  20:\"early_evening\",\n",
    "              21:\"late_evening\", 22:\"late_evening\", 23:\"late_evening\",\n",
    "              0:\"late_night\", 1:\"late_night\", 2:\"late_night\",\n",
    "              3:\"late_night\", 4:\"late_night\", 5:\"late_night\"}\n",
    "\n",
    "# map time periods\n",
    "train[\"dayparts\"] = train[\"hour\"].map(time_periods)\n",
    "test[\"dayparts\"] = test[\"hour\"].map(time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up weather data\n",
    "del weather['NAME']\n",
    "weather[\"SNOW\"] = weather[\"SNOW\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop time from train and test date fields to be able to map Dates against weather data; remove hyphens too.\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "\n",
    "# Convert Weather DATE to same format as train and test data\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strptime(x,\"%m/%d/%y\"))\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150513 object\n",
      "20150510 object\n",
      "20030101 object\n"
     ]
    }
   ],
   "source": [
    "# confirm that dates are now in the same format\n",
    "print(train[\"Dates\"][0], train[\"Dates\"].dtypes)\n",
    "print(test[\"Dates\"][0], test[\"Dates\"].dtypes)\n",
    "print(weather[\"DATE\"][0], weather[\"DATE\"].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# convert date objects to numeric\n",
    "train[\"Dates\"] = pd.to_numeric(train[\"Dates\"])\n",
    "test[\"Dates\"] = pd.to_numeric(test[\"Dates\"])\n",
    "weather[\"DATE\"] = pd.to_numeric(weather[\"DATE\"])\n",
    "print(type(train[\"Dates\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# left merge weather data based on dates\n",
    "weather_train = pd.merge(train, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "del weather_train['DATE']\n",
    "weather_test = pd.merge(test, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "del weather_test['DATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    878049.000000\n",
       "mean         37.771020\n",
       "std           0.456893\n",
       "min          37.707879\n",
       "25%          37.752427\n",
       "50%          37.775421\n",
       "75%          37.784369\n",
       "max          90.000000\n",
       "Name: Y, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data indicates outliers with latitude = 90 (aka the North Pole). Test data has these same outliers.\n",
    "# weather_train.Y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instead of omitting columns with erroneous lat/long data putting this as a placehold to look into replacing based on district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove cells where latitude > 38\n",
    "# weather_train = weather_train[weather_train.Y < 38]\n",
    "# weather_test = weather_test[weather_test.Y < 38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 18)\n",
      "(884262, 16)\n",
      "Cases removed from train data = 0\n",
      "Cases removed from test data = 0\n"
     ]
    }
   ],
   "source": [
    "# print new shape\n",
    "# print(weather_train.shape)\n",
    "#print(weather_test.shape)\n",
    "\n",
    "# print(\"Cases removed from train data =\", np.sum(878049 - weather_train.shape[0]))\n",
    "# print(\"Cases removed from test data =\", np.sum(884262 - weather_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y', 'month', 'year', 'hour', 'holidays',\n",
      "       'dayparts'],\n",
      "      dtype='object')\n",
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y', 'month', 'year', 'hour', 'holidays',\n",
      "       'dayparts', 'PRCP', 'SNOW', 'TMAX', 'TMIN'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'month',\n",
      "       'year', 'hour', 'holidays', 'dayparts'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'month',\n",
      "       'year', 'hour', 'holidays', 'dayparts', 'PRCP', 'SNOW', 'TMAX', 'TMIN'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(weather_train.columns)\n",
    "print(test.columns)\n",
    "print(weather_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "#weather_train[\"holidays\"] = pd.to_numeric(weather_train[\"holidays\"])\n",
    "#weather_test[\"holidays\"] = pd.to_numeric(weather_test[\"holidays\"])\n",
    "\n",
    "weather_train[\"holidays\"] = weather_train[\"holidays\"].astype(int)\n",
    "weather_test[\"holidays\"] = weather_test[\"holidays\"].astype(int)\n",
    "print(type(weather_train[\"holidays\"][0]))\n",
    "print(type(weather_test[\"holidays\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# Encode string features into numeric features\n",
    "LE = preprocessing.LabelEncoder()\n",
    "\n",
    "train_data_all = pd.get_dummies(weather_train, columns = [\"PdDistrict\", \"DayOfWeek\", \"month\", \"year\", \"dayparts\", \"hour\"])\n",
    "del train_data_all[\"Descript\"]\n",
    "del train_data_all[\"Resolution\"]\n",
    "#del train_data_all[\"day\"]\n",
    "train_labels_all = np.array(train_data_all['Category'])\n",
    "del train_data_all[\"Category\"]\n",
    "\n",
    "train_data_all[\"Dates\"] = LE.fit_transform(train_data_all[\"Dates\"])\n",
    "train_data_all[\"Address\"] = LE.fit_transform(train_data_all[\"Address\"])\n",
    "train_data_all.reindex()\n",
    "\n",
    "test_data_all = pd.get_dummies(weather_test, columns = [\"PdDistrict\", \"DayOfWeek\", \"month\", \"year\", \"dayparts\", \"hour\"])\n",
    "\n",
    "test_data_all[\"Dates\"] = LE.fit_transform(test_data_all[\"Dates\"])\n",
    "test_data_all[\"Address\"] = LE.fit_transform(test_data_all[\"Address\"])\n",
    "del test_data_all[\"Id\"]\n",
    "#del test_data_all[\"day\"]\n",
    "                                 \n",
    "print(test_data_all.columns == train_data_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dates  Address           X          Y  holidays  PRCP  SNOW  TMAX  TMIN  \\\n",
      "0   2248    19790 -122.425892  37.774599     False   0.0   0.0    61    50   \n",
      "1   2248    19790 -122.425892  37.774599     False   0.0   0.0    61    50   \n",
      "2   2248    22697 -122.424363  37.800414     False   0.0   0.0    61    50   \n",
      "\n",
      "   PdDistrict_BAYVIEW   ...     hour_14  hour_15  hour_16  hour_17  hour_18  \\\n",
      "0                   0   ...           0        0        0        0        0   \n",
      "1                   0   ...           0        0        0        0        0   \n",
      "2                   0   ...           0        0        0        0        0   \n",
      "\n",
      "   hour_19  hour_20  hour_21  hour_22  hour_23  \n",
      "0        0        0        0        0        1  \n",
      "1        0        0        0        0        1  \n",
      "2        0        0        0        0        1  \n",
      "\n",
      "[3 rows x 82 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_all.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a8325fd280ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcol_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_data_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcol_sd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_data_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcol_mean\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol_sd\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6360\u001b[0m                                       skipna=skipna, ddof=ddof)\n\u001b[1;32m   6361\u001b[0m         return self._reduce(f, name, axis=axis, numeric_only=numeric_only,\n\u001b[0;32m-> 6362\u001b[0;31m                             skipna=skipna, ddof=ddof)\n\u001b[0m\u001b[1;32m   6363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   5058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumeric_only\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5059\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5060\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5061\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5062\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3270\u001b[0m         \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mflot64\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3271\u001b[0m         \"\"\"\n\u001b[0;32m-> 3272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3274\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mas_matrix\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m   3251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3253\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mas_matrix\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m   3448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3450\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_interleave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3475\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3476\u001b[0m             \u001b[0mrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3477\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3478\u001b[0m             \u001b[0mitemmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First pass at normalization -- it doesn't give errors but it just endlessly processes.\n",
    "\n",
    "for i in train_data_all.columns:\n",
    "    col_mean = (train_data_all.loc[:]).mean()\n",
    "    col_sd = (train_data_all.loc[:]).std()\n",
    "    train_data_all[i] = train_data_all[i].apply(lambda x: (x - col_mean ) / (col_sd ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates                       659.900837\n",
      "Address                    6046.321721\n",
      "X                             0.025285\n",
      "Y                             0.024165\n",
      "holidays                      0.175814\n",
      "PRCP                          0.208314\n",
      "SNOW                          0.000000\n",
      "TMAX                          7.446029\n",
      "TMIN                          4.475019\n",
      "PdDistrict_BAYVIEW            0.302450\n",
      "PdDistrict_CENTRAL            0.296408\n",
      "PdDistrict_INGLESIDE          0.285892\n",
      "PdDistrict_MISSION            0.343394\n",
      "PdDistrict_NORTHERN           0.324863\n",
      "PdDistrict_PARK               0.230238\n",
      "PdDistrict_RICHMOND           0.220983\n",
      "PdDistrict_SOUTHERN           0.383367\n",
      "PdDistrict_TARAVAL            0.262919\n",
      "PdDistrict_TENDERLOIN         0.290659\n",
      "DayOfWeek_Friday              0.359319\n",
      "DayOfWeek_Monday              0.345391\n",
      "DayOfWeek_Saturday            0.351522\n",
      "DayOfWeek_Sunday              0.339488\n",
      "DayOfWeek_Thursday            0.349463\n",
      "DayOfWeek_Tuesday             0.349378\n",
      "DayOfWeek_Wednesday           0.354261\n",
      "month_1                       0.277018\n",
      "month_2                       0.272300\n",
      "month_3                       0.281723\n",
      "month_4                       0.284666\n",
      "                              ...     \n",
      "dayparts_early_evening        0.375632\n",
      "dayparts_early_morning        0.267440\n",
      "dayparts_late_afternoon       0.378096\n",
      "dayparts_late_evening         0.357250\n",
      "dayparts_late_morning         0.333255\n",
      "dayparts_late_night           0.350402\n",
      "hour_0                        0.220186\n",
      "hour_1                        0.170048\n",
      "hour_2                        0.157321\n",
      "hour_3                        0.125323\n",
      "hour_4                        0.105392\n",
      "hour_5                        0.098694\n",
      "hour_6                        0.121385\n",
      "hour_7                        0.156459\n",
      "hour_8                        0.189905\n",
      "hour_9                        0.197107\n",
      "hour_10                       0.202987\n",
      "hour_11                       0.204435\n",
      "hour_12                       0.235902\n",
      "hour_13                       0.216158\n",
      "hour_14                       0.219173\n",
      "hour_15                       0.227462\n",
      "hour_16                       0.232034\n",
      "hour_17                       0.239312\n",
      "hour_18                       0.242516\n",
      "hour_19                       0.230591\n",
      "hour_20                       0.219793\n",
      "hour_21                       0.217380\n",
      "hour_22                       0.222225\n",
      "hour_23                       0.214521\n",
      "Length: 82, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(col_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data and set aside 20% as development data\n",
    "train_data_all = train_data_all.values\n",
    "test_data_all = test_data_all.values\n",
    "n = train_data_all.shape[0]\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(train_data_all.shape[0]))\n",
    "\n",
    "train_data_all = train_data_all[shuffle]\n",
    "train_labels_all = train_labels_all[shuffle]\n",
    "\n",
    "n_train = int(0.8*n)\n",
    "\n",
    "train_data = train_data_all[:n_train,:]\n",
    "train_labels = train_labels_all[:n_train]\n",
    "dev_data = train_data_all[n_train:,:]\n",
    "dev_labels = train_labels_all[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (702385, 59)\n",
      "train_labels shape is (702385,)\n",
      "dev_data shape is (175597, 59)\n",
      "dev_labels shape is (175597,)\n",
      "train_data_all shape is (877982, 59)\n",
      "train_data_all shape is (877982, 59)\n",
      "train_labels_all shape is (877982,)\n",
      "test_data_all shape is (884186, 59)\n"
     ]
    }
   ],
   "source": [
    "# print shapes and some data to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)\n",
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save arrays as CSV files in a subdirectory\n",
    "\n",
    "# NOTE: mkdir will make a \"csv\" directory in your local repo if there is not already one there.\n",
    "# It will return an error if the directory already exists in your local repo\n",
    "# but that will not impact how this code runs\n",
    "\n",
    "! mkdir csv\n",
    "np.savetxt(\"csv/train_data.csv\", train_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/train_labels.csv\", train_labels, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_data.csv\", dev_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_labels.csv\", dev_labels, fmt=\"%s\", delimiter=\",\")\n",
    "#np.savetxt(\"csv/train_data_all.csv\", train_data_all, delimiter=\",\")\n",
    "#np.savetxt(\"csv/train_labels_all.csv\", train_labels_all, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/test_data_all.csv\", test_data_all, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zip up the CSV files\n",
    "\n",
    "# **IMPORTANT**  This code will rewrite the existing data.zip file in your local repo\n",
    "# You will need to push it to the group repo for everyone to have the updated zip file\n",
    "\n",
    "zip_files = zipfile.ZipFile(\"data.zip\", \"w\")\n",
    "zip_files.write(\"csv/train_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/train_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/dev_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/dev_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "#zip_files.write(\"csv/train_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "#zip_files.write(\"csv/train_labels_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.write(\"csv/test_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy =  0.223785144393\n"
     ]
    }
   ],
   "source": [
    "# Get baseline KNN accuracy with new features\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors = 1, n_jobs = -1)\n",
    "neigh.fit(train_data, train_labels)\n",
    "knn_pred = neigh.predict(dev_data)\n",
    "print(\"KNN Accuracy = \", np.mean(knn_pred == dev_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
