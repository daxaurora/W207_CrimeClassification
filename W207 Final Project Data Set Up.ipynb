{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W207 Fall 2017 Final ProjectÂ¶\n",
    "## Data Set Up - Data Cleaning and Feature Engineering\n",
    "Laura Williams, Kim Vignola, Cyprian Gascoigne  \n",
    "SF Crime Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads raw data (saved in a zip file) from Kaggle, processes and organizes the data for training a variety of machine learning models, and outputs the data as zipped csv files that other notebooks can unzip and use to train different models.\n",
    "\n",
    "The intention is that data cleaning and/or feature engineering will be added to this file as we progress through the project and look for additional way to process the data to improve our predictions.\n",
    "\n",
    "For ease of processing this data, exploratory data analysis will be done separately.\n",
    "\n",
    "Resulting zipped files will include: \n",
    "\n",
    "1) train_data.csv and train_labels.csv - includes 80% of the total training data, for training models that are not yet going to be submitted to Kaggle\n",
    "\n",
    "2) dev_data.csv and dev_labels.csv - includes 20% of the total training data, for testing models before they are submitted to Kaggle\n",
    "\n",
    "3) train_data_all.csv and train_labels_all.csv - includes all the training data. After testing models with the train and dev data split above, train the model from this full set of data for submission to Kaggle.\n",
    "\n",
    "4) test_data_all.csv - create predictions on this data for submission to Kaggle.\n",
    "\n",
    "Weather data for San Franscisco County was added to this analysis.\n",
    "Source: https://www.ncdc.noaa.gov/cdo-web/search\n",
    "Report: Daily Summaries, Date Range: 1/1/2003 - 12/31/2015, Search for: Counties/San Francisco, Station: SAN FRANCISCO DOWNTOWN, CA US, Metrics: Precipitation, Maximum Temperature, Minimum Temperature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: holidays package is not native with anaconda and may need to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta, date\n",
    "import holidays\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip raw data into a subdirectory \n",
    "unzip_files = zipfile.ZipFile(\"raw_data.zip\", \"r\")\n",
    "unzip_files.extractall(\"raw_data\")\n",
    "unzip_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read CSV files into pandas dataframes\n",
    "train = pd.read_csv(\"raw_data/train.csv\")\n",
    "test = pd.read_csv(\"raw_data/test.csv\")\n",
    "weather = pd.read_csv(\"raw_data/SF_county.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates' 'Category' 'Descript' 'DayOfWeek' 'PdDistrict' 'Resolution'\n",
      " 'Address' 'X' 'Y' 'month' 'year' 'hour' 'day' 'holidays' 'first_day'\n",
      " 'month_year' 'doy' 'spring' 'summer' 'fall' 'winter' 'dayparts']\n",
      "(878049, 22)\n",
      "['Id' 'Dates' 'DayOfWeek' 'PdDistrict' 'Address' 'X' 'Y' 'month' 'year'\n",
      " 'hour' 'day' 'holidays' 'first_day' 'month_year' 'doy' 'spring' 'summer'\n",
      " 'fall' 'winter' 'dayparts']\n",
      "(884262, 20)\n",
      "['DATE' 'PRCP' 'SNOW' 'TMAX' 'TMIN']\n",
      "(4748, 5)\n"
     ]
    }
   ],
   "source": [
    "# Raw data shape and features\n",
    "print(train.columns.values)\n",
    "print(train.shape)\n",
    "print(test.columns.values)\n",
    "print(test.shape)\n",
    "print(weather.columns.values)\n",
    "print(weather.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract month, year and hour from both datasets\n",
    "train[\"month\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "train[\"year\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "train[\"hour\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "train[\"day\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "\n",
    "test[\"month\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").month)\n",
    "test[\"year\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").year)\n",
    "test[\"hour\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "test[\"day\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").day)\n",
    "\n",
    "# map holidays\n",
    "US_Holidays = holidays.UnitedStates()\n",
    "train[\"holidays\"] = train[\"Dates\"].map(lambda x: x in US_Holidays)\n",
    "test[\"holidays\"] = test[\"Dates\"].map(lambda x: x in US_Holidays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull out first day of the month as it ranks first for crime volume and specific crimes may be associated with the day\n",
    "train[\"first_day\"] = [1 if x==1 else 0 for x in train[\"day\"]]\n",
    "test[\"first_day\"] = [1 if x==1 else 0 for x in test[\"day\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a bucket variable for month_year\n",
    "\n",
    "train[\"month_year\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "train[\"month_year\"] = train[\"month_year\"].map(lambda x: datetime.strftime(x,\"%Y-%m\"))\n",
    "\n",
    "test[\"month_year\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "test[\"month_year\"] = test[\"month_year\"].map(lambda x: datetime.strftime(x,\"%Y-%m\"))\n",
    "\n",
    "# would month_day have any value?\n",
    "#train[\"month_day\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "#train[\"month_day\"] = train[\"month_day\"].map(lambda x: datetime.strftime(x,\"%m-%d\"))\n",
    "#test[\"month_day\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "#test[\"month_day\"] = test[\"month_day\"].map(lambda x: datetime.strftime(x,\"%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse out day of year for bucketing seasons\n",
    "\n",
    "train[\"doy\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").timetuple().tm_yday)\n",
    "test[\"doy\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").timetuple().tm_yday)\n",
    "\n",
    "train[\"spring\"] = [1 if x in range(81,173) else 0 for x in train[\"doy\"]]\n",
    "train[\"summer\"] = [1 if x in range(173,265) else 0 for x in train[\"doy\"]]\n",
    "train[\"fall\"] = [1 if x in range(265,356) else 0 for x in train[\"doy\"]]\n",
    "train[\"winter\"] = [1 if x in range(1,81) or x in range(356,366) else 0 for x in train[\"doy\"]]\n",
    "\n",
    "test[\"spring\"] = [1 if x in range(81,173) else 0 for x in test[\"doy\"]]\n",
    "test[\"summer\"] = [1 if x in range(173,265) else 0 for x in test[\"doy\"]]\n",
    "test[\"fall\"] = [1 if x in range(265,356) else 0 for x in test[\"doy\"]]\n",
    "test[\"winter\"] = [1 if x in range(1,81) or x in range(356,366) else 0 for x in test[\"doy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary for bucketing hours\n",
    "time_periods = {6:\"early_morning\", 7:\"early_morning\", 8:\"early_morning\", \n",
    "               9:\"late_morning\", 10:\"late_morning\", 11:\"late_morning\",\n",
    "              12:\"early_afternoon\", 13:\"early_afternoon\", 14:\"early_afternoon\",\n",
    "              15:\"late_afternoon\", 16:\"late_afternoon\", 17:\"late_afternoon\",\n",
    "              18:\"early_evening\",  19:\"early_evening\",  20:\"early_evening\",\n",
    "              21:\"late_evening\", 22:\"late_evening\", 23:\"late_evening\",\n",
    "              0:\"late_night\", 1:\"late_night\", 2:\"late_night\",\n",
    "              3:\"late_night\", 4:\"late_night\", 5:\"late_night\"}\n",
    "\n",
    "# map time periods to dayparts\n",
    "train[\"dayparts\"] = train[\"hour\"].map(time_periods)\n",
    "test[\"dayparts\"] = test[\"hour\"].map(time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up weather data\n",
    "del weather['NAME']\n",
    "weather[\"SNOW\"] = weather[\"SNOW\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop time from train and test date fields to be able to map Dates against weather data; remove hyphens too.\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "train[\"Dates\"] = train[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "test[\"Dates\"] = test[\"Dates\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))\n",
    "\n",
    "# Convert Weather DATE to same format as train and test data\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strptime(x,\"%m/%d/%y\"))\n",
    "weather[\"DATE\"] = weather[\"DATE\"].map(lambda x: datetime.strftime(x,\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# convert date objects to numeric\n",
    "train[\"Dates\"] = pd.to_numeric(train[\"Dates\"])\n",
    "test[\"Dates\"] = pd.to_numeric(test[\"Dates\"])\n",
    "weather[\"DATE\"] = pd.to_numeric(weather[\"DATE\"])\n",
    "print(type(train[\"Dates\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# left merge weather data based on dates\n",
    "weather_train = pd.merge(train, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "del weather_train['DATE']\n",
    "del weather_train[\"SNOW\"]\n",
    "weather_test = pd.merge(test, weather, how='left', left_on=\"Dates\", right_on = \"DATE\")\n",
    "del weather_test['DATE']\n",
    "del weather_test[\"SNOW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates' 'Category' 'Descript' 'DayOfWeek' 'PdDistrict' 'Resolution'\n",
      " 'Address' 'X' 'Y' 'month' 'year' 'hour' 'day' 'holidays' 'first_day'\n",
      " 'month_year' 'doy' 'spring' 'summer' 'fall' 'winter' 'dayparts' 'PRCP'\n",
      " 'TMAX' 'TMIN']\n",
      "(878049, 25)\n",
      "['Id' 'Dates' 'DayOfWeek' 'PdDistrict' 'Address' 'X' 'Y' 'month' 'year'\n",
      " 'hour' 'day' 'holidays' 'first_day' 'month_year' 'doy' 'spring' 'summer'\n",
      " 'fall' 'winter' 'dayparts' 'PRCP' 'TMAX' 'TMIN']\n",
      "(884262, 23)\n"
     ]
    }
   ],
   "source": [
    "print(weather_train.columns.values)\n",
    "print(weather_train.shape)\n",
    "print(weather_test.columns.values)\n",
    "print(weather_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, fix outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data indicates outliers with latitude = 90. Test data has these same outliers.\n",
    "# Set latitiude to the median of the district where the crime occured.\n",
    "districts = set(weather_train[\"PdDistrict\"])\n",
    "medians = {el:0 for el in districts}\n",
    "for district in districts:\n",
    "    medians[district] = weather_train[\"Y\"][weather_train[\"PdDistrict\"] == district].median()\n",
    "weather_train.loc[weather_train.Y > 38, \"Y\"] = weather_train[weather_train.Y > 38][\"PdDistrict\"].map(lambda x: \n",
    "                                                                                                     medians[x])\n",
    "weather_test.loc[weather_test.Y > 38, \"Y\"] = weather_test[weather_test.Y > 38][\"PdDistrict\"].map(lambda x : \n",
    "                                                                                                 medians[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 25)\n",
      "(884262, 23)\n",
      "Cases removed from train data = 0\n",
      "Cases removed from test data = 0\n",
      "Cases fixed in the train data = 67\n",
      "Cases fixed in the test data = 76\n"
     ]
    }
   ],
   "source": [
    "#print new shape\n",
    "print(weather_train.shape)\n",
    "print(weather_test.shape)\n",
    "\n",
    "print(\"Cases removed from train data =\", np.sum(878049 - weather_train.shape[0]))\n",
    "print(\"Cases removed from test data =\", np.sum(884262 - weather_test.shape[0]))\n",
    "print(\"Cases fixed in the train data =\", len(train[train.Y>38]))\n",
    "print(\"Cases fixed in the test data =\", len(test[test.Y > 38]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add additional positional arguments. First one is distance to nearest police station. \n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "#We use Euclidean distance since curvature of the earth doesn't matter for small distances.\n",
    "def dist(p1, p2):\n",
    "    return sqrt((p2[1]-p1[0])**2 + (p2[0]-p1[1])**2)\n",
    "\n",
    "PStations = {'Central Station':  (37.798732, -122.409919), 'Southern Station': (37.772380,-122.389412), \n",
    "             'Bayview':  (37.729732, -122.397981), 'Mission': (37.762849, -122.422005), 'Northern': (37.780190, -122.432445), \n",
    "             'Park': (37.767797, -122.455287), 'Richmond': (37.779928, -122.464467), 'Ingleside': (37.724676, -122.446215), \n",
    "             'Taraval': (37.782988, -122.483874), 'Tenderloin': (37.783669, -122.412896)}\n",
    "\n",
    "def mindist(p1, l):\n",
    "    return min([dist(p1,x) for x in l])\n",
    "\n",
    "weather_train['d_police'] = weather_train.apply(lambda x : mindist((x.X, x.Y), PStations.values()), axis = 1)\n",
    "weather_test['d_police'] = weather_train.apply(lambda x : mindist((x.X, x.Y), PStations.values()), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rotational data for better spatial understanding\n",
    "weather_train[\"rot_45_X\"] = .707*weather_train[\"Y\"] + .707*weather_train[\"X\"]\n",
    "weather_train[\"rot_45_Y\"] = .707* weather_train[\"Y\"] - .707* weather_train[\"X\"]\n",
    "\n",
    "weather_train[\"rot_30_X\"] = (1.732/2)*weather_train[\"X\"] + (1./2)*weather_train[\"Y\"]\n",
    "weather_train[\"rot_30_Y\"] = (1.732/2)*weather_train[\"Y\"] - (1./2)*weather_train[\"X\"]\n",
    "\n",
    "weather_train[\"rot_60_X\"] = (1./2)*weather_train[\"X\"] + (1.732/2)*weather_train[\"Y\"]\n",
    "weather_train[\"rot_60_Y\"] = (1./2)*weather_train[\"Y\"] - (1.732/2)*weather_train[\"X\"]\n",
    "\n",
    "weather_train[\"radial_r\"] = np.sqrt(np.power(weather_train[\"Y\"],2) + np.power(weather_train[\"X\"],2))\n",
    "\n",
    "#Test data\n",
    "weather_test[\"rot_45_X\"] = .707*weather_test[\"Y\"] + .707*weather_test[\"X\"]\n",
    "weather_test[\"rot_45_Y\"] = .707* weather_test[\"Y\"] - .707* weather_test[\"X\"]\n",
    "\n",
    "weather_test[\"rot_30_X\"] = (1.732/2)*weather_test[\"X\"] + (1./2)*weather_test[\"Y\"]\n",
    "weather_test[\"rot_30_Y\"] = (1.732/2)*weather_test[\"Y\"] - (1./2)*weather_test[\"X\"]\n",
    "\n",
    "weather_test[\"rot_60_X\"] = (1./2)*weather_test[\"X\"] + (1.732/2)*weather_test[\"Y\"]\n",
    "weather_test[\"rot_60_Y\"] = (1./2)*weather_test[\"Y\"] - (1.732/2)*weather_test[\"X\"]\n",
    "\n",
    "weather_test[\"radial_r\"] = np.sqrt(np.power(weather_test[\"Y\"],2) + np.power(weather_test[\"X\"],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y', 'month', 'year', 'hour', 'day',\n",
      "       'holidays', 'first_day', 'month_year', 'doy', 'spring', 'summer',\n",
      "       'fall', 'winter', 'dayparts'],\n",
      "      dtype='object')\n",
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y', 'month', 'year', 'hour', 'day',\n",
      "       'holidays', 'first_day', 'month_year', 'doy', 'spring', 'summer',\n",
      "       'fall', 'winter', 'dayparts', 'PRCP', 'TMAX', 'TMIN', 'd_police',\n",
      "       'rot_45_X', 'rot_45_Y', 'rot_30_X', 'rot_30_Y', 'rot_60_X', 'rot_60_Y',\n",
      "       'radial_r'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'month',\n",
      "       'year', 'hour', 'day', 'holidays', 'first_day', 'month_year', 'doy',\n",
      "       'spring', 'summer', 'fall', 'winter', 'dayparts'],\n",
      "      dtype='object')\n",
      "Index(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'month',\n",
      "       'year', 'hour', 'day', 'holidays', 'first_day', 'month_year', 'doy',\n",
      "       'spring', 'summer', 'fall', 'winter', 'dayparts', 'PRCP', 'TMAX',\n",
      "       'TMIN', 'd_police', 'rot_45_X', 'rot_45_Y', 'rot_30_X', 'rot_30_Y',\n",
      "       'rot_60_X', 'rot_60_Y', 'radial_r'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(weather_train.columns)\n",
    "print(test.columns)\n",
    "print(weather_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "weather_train[\"holidays\"] = weather_train[\"holidays\"].astype(int)\n",
    "weather_test[\"holidays\"] = weather_test[\"holidays\"].astype(int)\n",
    "print(type(weather_train[\"holidays\"][0]))\n",
    "print(type(weather_test[\"holidays\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates' 'Category' 'Descript' 'DayOfWeek' 'PdDistrict' 'Resolution'\n",
      " 'Address' 'X' 'Y' 'month' 'year' 'hour' 'day' 'holidays' 'first_day'\n",
      " 'month_year' 'doy' 'spring' 'summer' 'fall' 'winter' 'dayparts' 'PRCP'\n",
      " 'TMAX' 'TMIN' 'd_police' 'rot_45_X' 'rot_45_Y' 'rot_30_X' 'rot_30_Y'\n",
      " 'rot_60_X' 'rot_60_Y' 'radial_r']\n",
      "(878049, 33)\n",
      "['Id' 'Dates' 'DayOfWeek' 'PdDistrict' 'Address' 'X' 'Y' 'month' 'year'\n",
      " 'hour' 'day' 'holidays' 'first_day' 'month_year' 'doy' 'spring' 'summer'\n",
      " 'fall' 'winter' 'dayparts' 'PRCP' 'TMAX' 'TMIN' 'd_police' 'rot_45_X'\n",
      " 'rot_45_Y' 'rot_30_X' 'rot_30_Y' 'rot_60_X' 'rot_60_Y' 'radial_r']\n",
      "(884262, 31)\n",
      "Dates           int64\n",
      "Category       object\n",
      "Descript       object\n",
      "DayOfWeek      object\n",
      "PdDistrict     object\n",
      "Resolution     object\n",
      "Address        object\n",
      "X             float64\n",
      "Y             float64\n",
      "month           int64\n",
      "year            int64\n",
      "hour            int64\n",
      "day             int64\n",
      "holidays        int64\n",
      "first_day       int64\n",
      "month_year     object\n",
      "doy             int64\n",
      "spring          int64\n",
      "summer          int64\n",
      "fall            int64\n",
      "winter          int64\n",
      "dayparts       object\n",
      "PRCP          float64\n",
      "TMAX            int64\n",
      "TMIN            int64\n",
      "d_police      float64\n",
      "rot_45_X      float64\n",
      "rot_45_Y      float64\n",
      "rot_30_X      float64\n",
      "rot_30_Y      float64\n",
      "rot_60_X      float64\n",
      "rot_60_Y      float64\n",
      "radial_r      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(weather_train.columns.values)\n",
    "print(weather_train.shape)\n",
    "print(weather_test.columns.values)\n",
    "print(weather_test.shape)\n",
    "print(weather_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# Encode string features into numeric features\n",
    "\n",
    "LE = preprocessing.LabelEncoder()\n",
    "LE.fit(train_data_all['month_year'])\n",
    "train_data_all['month_year'] = LE.transform(train_data_all['month_year'])\n",
    "test_data_all['month_year'] = LE.transform(test_data_all['month_year'])\n",
    "\n",
    "\n",
    "train_data_all = pd.get_dummies(weather_train, columns = ['DayOfWeek', 'PdDistrict',\n",
    "       'month', 'year', 'dayparts'])\n",
    "del train_data_all[\"Dates\"]\n",
    "del train_data_all[\"Descript\"]\n",
    "del train_data_all[\"Resolution\"]\n",
    "del train_data_all[\"day\"]\n",
    "del train_data_all[\"doy\"]\n",
    "del train_data_all[\"Address\"]\n",
    "\n",
    "train_labels_all = np.array(train_data_all['Category'])\n",
    "del train_data_all[\"Category\"]\n",
    "\n",
    "train_data_all.reindex()\n",
    "\n",
    "test_data_all = pd.get_dummies(weather_test, columns = ['DayOfWeek', 'PdDistrict',\n",
    "       'month', 'year','dayparts'])\n",
    "\n",
    "del test_data_all[\"Id\"]\n",
    "del test_data_all[\"day\"]\n",
    "del test_data_all[\"doy\"]\n",
    "del test_data_all[\"Dates\"]\n",
    "del test_data_all[\"Address\"]\n",
    "                                 \n",
    "print(test_data_all.columns == train_data_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X' 'Y' 'hour' 'holidays' 'first_day' 'month_year' 'spring' 'summer'\n",
      " 'fall' 'winter' 'PRCP' 'TMAX' 'TMIN' 'd_police' 'rot_45_X' 'rot_45_Y'\n",
      " 'rot_30_X' 'rot_30_Y' 'rot_60_X' 'rot_60_Y' 'radial_r' 'DayOfWeek_Friday'\n",
      " 'DayOfWeek_Monday' 'DayOfWeek_Saturday' 'DayOfWeek_Sunday'\n",
      " 'DayOfWeek_Thursday' 'DayOfWeek_Tuesday' 'DayOfWeek_Wednesday'\n",
      " 'PdDistrict_BAYVIEW' 'PdDistrict_CENTRAL' 'PdDistrict_INGLESIDE'\n",
      " 'PdDistrict_MISSION' 'PdDistrict_NORTHERN' 'PdDistrict_PARK'\n",
      " 'PdDistrict_RICHMOND' 'PdDistrict_SOUTHERN' 'PdDistrict_TARAVAL'\n",
      " 'PdDistrict_TENDERLOIN' 'month_1' 'month_2' 'month_3' 'month_4' 'month_5'\n",
      " 'month_6' 'month_7' 'month_8' 'month_9' 'month_10' 'month_11' 'month_12'\n",
      " 'year_2003' 'year_2004' 'year_2005' 'year_2006' 'year_2007' 'year_2008'\n",
      " 'year_2009' 'year_2010' 'year_2011' 'year_2012' 'year_2013' 'year_2014'\n",
      " 'year_2015' 'dayparts_early_afternoon' 'dayparts_early_evening'\n",
      " 'dayparts_early_morning' 'dayparts_late_afternoon' 'dayparts_late_evening'\n",
      " 'dayparts_late_morning' 'dayparts_late_night']\n",
      "(878049, 70)\n",
      "['X' 'Y' 'hour' 'holidays' 'first_day' 'month_year' 'spring' 'summer'\n",
      " 'fall' 'winter' 'PRCP' 'TMAX' 'TMIN' 'd_police' 'rot_45_X' 'rot_45_Y'\n",
      " 'rot_30_X' 'rot_30_Y' 'rot_60_X' 'rot_60_Y' 'radial_r' 'DayOfWeek_Friday'\n",
      " 'DayOfWeek_Monday' 'DayOfWeek_Saturday' 'DayOfWeek_Sunday'\n",
      " 'DayOfWeek_Thursday' 'DayOfWeek_Tuesday' 'DayOfWeek_Wednesday'\n",
      " 'PdDistrict_BAYVIEW' 'PdDistrict_CENTRAL' 'PdDistrict_INGLESIDE'\n",
      " 'PdDistrict_MISSION' 'PdDistrict_NORTHERN' 'PdDistrict_PARK'\n",
      " 'PdDistrict_RICHMOND' 'PdDistrict_SOUTHERN' 'PdDistrict_TARAVAL'\n",
      " 'PdDistrict_TENDERLOIN' 'month_1' 'month_2' 'month_3' 'month_4' 'month_5'\n",
      " 'month_6' 'month_7' 'month_8' 'month_9' 'month_10' 'month_11' 'month_12'\n",
      " 'year_2003' 'year_2004' 'year_2005' 'year_2006' 'year_2007' 'year_2008'\n",
      " 'year_2009' 'year_2010' 'year_2011' 'year_2012' 'year_2013' 'year_2014'\n",
      " 'year_2015' 'dayparts_early_afternoon' 'dayparts_early_evening'\n",
      " 'dayparts_early_morning' 'dayparts_late_afternoon' 'dayparts_late_evening'\n",
      " 'dayparts_late_morning' 'dayparts_late_night']\n",
      "(884262, 70)\n",
      "X                           float64\n",
      "Y                           float64\n",
      "hour                          int64\n",
      "holidays                      int64\n",
      "first_day                     int64\n",
      "month_year                   object\n",
      "spring                        int64\n",
      "summer                        int64\n",
      "fall                          int64\n",
      "winter                        int64\n",
      "PRCP                        float64\n",
      "TMAX                          int64\n",
      "TMIN                          int64\n",
      "d_police                    float64\n",
      "rot_45_X                    float64\n",
      "rot_45_Y                    float64\n",
      "rot_30_X                    float64\n",
      "rot_30_Y                    float64\n",
      "rot_60_X                    float64\n",
      "rot_60_Y                    float64\n",
      "radial_r                    float64\n",
      "DayOfWeek_Friday              uint8\n",
      "DayOfWeek_Monday              uint8\n",
      "DayOfWeek_Saturday            uint8\n",
      "DayOfWeek_Sunday              uint8\n",
      "DayOfWeek_Thursday            uint8\n",
      "DayOfWeek_Tuesday             uint8\n",
      "DayOfWeek_Wednesday           uint8\n",
      "PdDistrict_BAYVIEW            uint8\n",
      "PdDistrict_CENTRAL            uint8\n",
      "                             ...   \n",
      "month_3                       uint8\n",
      "month_4                       uint8\n",
      "month_5                       uint8\n",
      "month_6                       uint8\n",
      "month_7                       uint8\n",
      "month_8                       uint8\n",
      "month_9                       uint8\n",
      "month_10                      uint8\n",
      "month_11                      uint8\n",
      "month_12                      uint8\n",
      "year_2003                     uint8\n",
      "year_2004                     uint8\n",
      "year_2005                     uint8\n",
      "year_2006                     uint8\n",
      "year_2007                     uint8\n",
      "year_2008                     uint8\n",
      "year_2009                     uint8\n",
      "year_2010                     uint8\n",
      "year_2011                     uint8\n",
      "year_2012                     uint8\n",
      "year_2013                     uint8\n",
      "year_2014                     uint8\n",
      "year_2015                     uint8\n",
      "dayparts_early_afternoon      uint8\n",
      "dayparts_early_evening        uint8\n",
      "dayparts_early_morning        uint8\n",
      "dayparts_late_afternoon       uint8\n",
      "dayparts_late_evening         uint8\n",
      "dayparts_late_morning         uint8\n",
      "dayparts_late_night           uint8\n",
      "Length: 70, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_data_all.columns.values)\n",
    "print(train_data_all.shape)\n",
    "print(test_data_all.columns.values)\n",
    "print(test_data_all.shape)\n",
    "print(train_data_all.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WARRANTS' 'OTHER OFFENSES' 'OTHER OFFENSES' ..., 'LARCENY/THEFT'\n",
      " 'VANDALISM' 'FORGERY/COUNTERFEITING']\n",
      "Index(['X', 'Y', 'hour', 'holidays', 'first_day', 'month_year', 'spring',\n",
      "       'summer', 'fall', 'winter', 'PRCP', 'TMAX', 'TMIN', 'd_police',\n",
      "       'rot_45_X', 'rot_45_Y', 'rot_30_X', 'rot_30_Y', 'rot_60_X', 'rot_60_Y',\n",
      "       'radial_r', 'DayOfWeek_Friday', 'DayOfWeek_Monday',\n",
      "       'DayOfWeek_Saturday', 'DayOfWeek_Sunday', 'DayOfWeek_Thursday',\n",
      "       'DayOfWeek_Tuesday', 'DayOfWeek_Wednesday', 'PdDistrict_BAYVIEW',\n",
      "       'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', 'PdDistrict_MISSION',\n",
      "       'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND',\n",
      "       'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN',\n",
      "       'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
      "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12',\n",
      "       'year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007',\n",
      "       'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012',\n",
      "       'year_2013', 'year_2014', 'year_2015', 'dayparts_early_afternoon',\n",
      "       'dayparts_early_evening', 'dayparts_early_morning',\n",
      "       'dayparts_late_afternoon', 'dayparts_late_evening',\n",
      "       'dayparts_late_morning', 'dayparts_late_night'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_labels_all)\n",
    "print(train_data_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(878049,)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_labels_all))\n",
    "print(train_labels_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "train_data_all = (train_data_all - train_data_all.mean())/(train_data_all.std())\n",
    "test_data_all = (test_data_all - train_data_all.mean())/(train_data_all.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            X          Y  hour      holidays     first_day  month_year  \\\n",
      "0 -122.399588  37.735051  23.0  1.134103e-15  1.221126e-15       148.0   \n",
      "1 -122.391523  37.732432  23.0  1.134103e-15  1.221126e-15       148.0   \n",
      "2 -122.426002  37.792212  23.0  1.134103e-15  1.221126e-15       148.0   \n",
      "3 -122.437394  37.721412  23.0  1.134103e-15  1.221126e-15       148.0   \n",
      "4 -122.437394  37.721412  23.0  1.134103e-15  1.221126e-15       148.0   \n",
      "\n",
      "   spring        summer          fall        winter         ...           \\\n",
      "0     1.0  7.105523e-14  5.976261e-14 -2.581381e-14         ...            \n",
      "1     1.0  7.105523e-14  5.976261e-14 -2.581381e-14         ...            \n",
      "2     1.0  7.105523e-14  5.976261e-14 -2.581381e-14         ...            \n",
      "3     1.0  7.105523e-14  5.976261e-14 -2.581381e-14         ...            \n",
      "4     1.0  7.105523e-14  5.976261e-14 -2.581381e-14         ...            \n",
      "\n",
      "      year_2013     year_2014  year_2015  dayparts_early_afternoon  \\\n",
      "0  3.143028e-13 -4.016814e-12        1.0             -4.851726e-14   \n",
      "1  3.143028e-13 -4.016814e-12        1.0             -4.851726e-14   \n",
      "2  3.143028e-13 -4.016814e-12        1.0             -4.851726e-14   \n",
      "3  3.143028e-13 -4.016814e-12        1.0             -4.851726e-14   \n",
      "4  3.143028e-13 -4.016814e-12        1.0             -4.851726e-14   \n",
      "\n",
      "   dayparts_early_evening  dayparts_early_morning  dayparts_late_afternoon  \\\n",
      "0            4.313563e-14            4.774700e-14             2.782321e-14   \n",
      "1            4.313563e-14            4.774700e-14             2.782321e-14   \n",
      "2            4.313563e-14            4.774700e-14             2.782321e-14   \n",
      "3            4.313563e-14            4.774700e-14             2.782321e-14   \n",
      "4            4.313563e-14            4.774700e-14             2.782321e-14   \n",
      "\n",
      "   dayparts_late_evening  dayparts_late_morning  dayparts_late_night  \n",
      "0                    1.0           5.322651e-14         1.176042e-14  \n",
      "1                    1.0           5.322651e-14         1.176042e-14  \n",
      "2                    1.0           5.322651e-14         1.176042e-14  \n",
      "3                    1.0           5.322651e-14         1.176042e-14  \n",
      "4                    1.0           5.322651e-14         1.176042e-14  \n",
      "\n",
      "[5 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_data_all[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data and set aside 20% as development data\n",
    "train_data_all = train_data_all.values\n",
    "test_data_all = test_data_all.values\n",
    "n = train_data_all.shape[0]\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(train_data_all.shape[0]))\n",
    "\n",
    "train_data_all = train_data_all[shuffle]\n",
    "train_labels_all = train_labels_all[shuffle]\n",
    "\n",
    "n_train = int(0.8*n)\n",
    "\n",
    "train_data = train_data_all[:n_train,:]\n",
    "train_labels = train_labels_all[:n_train]\n",
    "dev_data = train_data_all[n_train:,:]\n",
    "dev_labels = train_labels_all[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can run two types of Random Forest classifiers: \n",
    "\n",
    "The first model has basic sub-optimal parameters to quickly confirm that no errors are returned by running the data through the model.  \n",
    "\n",
    "The second model is set with optimal parameters and a random seed to accurately compare the results from different changes to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages for the Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62062662827\n"
     ]
    }
   ],
   "source": [
    "# Basic model - to check for errors\n",
    "\n",
    "# Set up variables\n",
    "n = 10\n",
    "depth = 2\n",
    "features = 'sqrt'\n",
    "\n",
    "# Train the model and create predictions\n",
    "RF = RandomForestClassifier(n_estimators=n, max_depth=depth, max_features=features, n_jobs=1)\n",
    "RF.fit(train_data, train_labels)\n",
    "pp = RF.predict_proba(dev_data)\n",
    "logloss = metrics.log_loss(dev_labels, pp)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3358727151\n"
     ]
    }
   ],
   "source": [
    "# Optimal hyperparameters with random forest model\n",
    "# Note that on the 70 feature data set this took about 20-25 minutes\n",
    "\n",
    "# Also the optimal hyperparameters might change with different types and number of features\n",
    "\n",
    "# Set random seed so that there will not be random changes when the model is run with different data sets.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set up variables\n",
    "n = 150\n",
    "depth = 16\n",
    "features = 0.40\n",
    "\n",
    "# Train the model and create predictions\n",
    "RF = RandomForestClassifier(n_estimators=n, max_depth=depth, max_features=features, n_jobs=1)\n",
    "RF.fit(train_data, train_labels)\n",
    "pp = RF.predict_proba(dev_data)\n",
    "logloss = metrics.log_loss(dev_labels, pp)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's a place to keep track of logloss results with the optimal hyperparameters from different data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3358727151** with dataset as of 5pm Saturday 12/2, with 70 features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (702439, 70)\n",
      "train_labels shape is (702439,)\n",
      "dev_data shape is (175610, 70)\n",
      "dev_labels shape is (175610,)\n",
      "train_data_all shape is (878049, 70)\n",
      "train_labels_all shape is (878049,)\n",
      "test_data_all shape is (884262, 70)\n"
     ]
    }
   ],
   "source": [
    "# print shapes and some data to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)\n",
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: csv: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Save data as CSV files in a subdirectory\n",
    "\n",
    "# NOTE: mkdir will make a \"csv\" directory in your local repo if there is not already one there.\n",
    "# It will return an error if the directory already exists in your local repo\n",
    "# but that will not impact how this code runs\n",
    "\n",
    "! mkdir csv\n",
    "np.savetxt(\"csv/train_data.csv\", train_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/train_labels.csv\", train_labels, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_data.csv\", dev_data, delimiter=\",\")\n",
    "np.savetxt(\"csv/dev_labels.csv\", dev_labels, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/train_data_all.csv\", train_data_all, delimiter=\",\")\n",
    "np.savetxt(\"csv/train_labels_all.csv\", train_labels_all, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"csv/test_data_all.csv\", test_data_all, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zip up the CSV files\n",
    "\n",
    "# **IMPORTANT**  This code will rewrite existing zip files in your local repo\n",
    "# You will need to push it to the group repo for everyone to have the updated zip file\n",
    "\n",
    "# Full set of training data and labels --> data.zip\n",
    "zip_train_all = zipfile.ZipFile(\"data.zip\", \"w\")\n",
    "zip_train_all.write(\"csv/train_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_train_all.write(\"csv/train_labels_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_train_all.close()\n",
    "\n",
    "# Subset of training data and labels --> data_subset.zip\n",
    "zip_train_subset = zipfile.ZipFile(\"data_subset.zip\", \"w\")\n",
    "zip_train_subset.write(\"csv/train_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_train_subset.write(\"csv/train_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_train_subset.close()\n",
    "\n",
    "\n",
    "# Data used for testing models (test data from Kaggle and our 20% development data) --> testing.zip\n",
    "zip_testing = zipfile.ZipFile(\"testing.zip\", \"w\")\n",
    "zip_testing.write(\"csv/test_data_all.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_testing.write(\"csv/dev_data.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_testing.write(\"csv/dev_labels.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_testing.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
