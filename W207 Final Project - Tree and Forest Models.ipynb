{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a random forest model and also several different decision tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip data files into the \"csv\" subdirectory \n",
    "# (unless this has already done this since running the Data Set Up notebook)\n",
    "\n",
    "# **IMPORTANT**  This will overwrite existing files in the \"csv\" folder in your local repo\n",
    "# with the most recent data files from the data.zip file\n",
    "\n",
    "# Unzip 80% training data\n",
    "unzip_training_data = zipfile.ZipFile(\"data_subset.zip\", \"r\")\n",
    "unzip_training_data.extractall()\n",
    "unzip_training_data.close()\n",
    "\n",
    "# Unzip development and training data\n",
    "unzip_test_data = zipfile.ZipFile(\"testing.zip\", \"r\")\n",
    "unzip_test_data.extractall()\n",
    "unzip_test_data.close()\n",
    "\n",
    "# Unzip full set of training data for creating predictions to submit to Kaggle\n",
    "unzip_all_data = zipfile.ZipFile(\"data.zip\", \"r\")\n",
    "unzip_all_data.extractall()\n",
    "unzip_all_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for testing on development data\n",
    "train_data = np.loadtxt('csv/train_data.csv', delimiter=\",\")\n",
    "train_labels = np.loadtxt('csv/train_labels.csv', dtype=str, delimiter=\",\")\n",
    "dev_data = np.loadtxt('csv/dev_data.csv', delimiter=\",\")\n",
    "dev_labels = np.loadtxt('csv/dev_labels.csv', dtype=str, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for creating predictions to submit to Kaggle\n",
    "train_data_all = np.loadtxt('csv/train_data_all.csv', delimiter=\",\")\n",
    "train_labels_all = np.loadtxt('csv/train_labels_all.csv', dtype=str, delimiter=\",\")\n",
    "test_data_all = np.loadtxt('csv/test_data_all.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (702439, 58)\n",
      "train_labels shape is (702439,)\n",
      "dev_data shape is (175610, 58)\n",
      "dev_labels shape is (175610,)\n"
     ]
    }
   ],
   "source": [
    "# print shapes to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_all shape is (878049, 58)\n",
      "train_labels_all shape is (878049,)\n",
      "test_data_all shape is (884262, 58)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IF there are additional changes to make to the data for this model\n",
    "# that would be easier to do in pandas, uncomment and run this code. \n",
    "# This model works the same whether the data is in numpy or pandas, so presumably so do other models\n",
    "\n",
    "#train_data = pd.DataFrame(train_data)\n",
    "#train_labels = pd.DataFrame(train_labels)\n",
    "#dev_data = pd.DataFrame(dev_data)\n",
    "#dev_labels = pd.DataFrame(dev_labels)\n",
    "#train_data_all = pd.DataFrame(train_data_all)\n",
    "#train_labels_all = pd.DataFrame(train_labels_all)\n",
    "#test_data_all = pd.DataFrame(test_data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up functions for training random forest and finding optimal hyperparameters\n",
    "\n",
    "def TrainRF(data, labels, test_data, depth, n=10):\n",
    "    \"\"\"This function takes in training data and labels, testing data,\n",
    "    and can accept different values of n (the number of random decision trees to create),\n",
    "    and can also accept different values of max_depth (which can also dramatically influence performance).\n",
    "    \n",
    "    It trains a random forest model and returns the model and predicted probabilities.\n",
    "    \"\"\"\n",
    "    RF = RandomForestClassifier(n_estimators=n, max_depth=depth, n_jobs=1)\n",
    "    RF.fit(data, labels)\n",
    "    pp = RF.predict_proba(test_data)\n",
    "    return RF, pp\n",
    "\n",
    "def find_max_depth(data, labels, dev_data, dev_labels, depth_values):\n",
    "    \"\"\"Find optimal value of max_depth in a random forest model.  \n",
    "    \n",
    "    Note that this cannot be used on test data from Kaggle \n",
    "    because we do not have labels for that data.  This function is intended to only be used\n",
    "    in the development stage with the development data.    \n",
    "    \"\"\"\n",
    "    for d in depth_values:      \n",
    "        RF, pp = TrainRF(data, labels, dev_data, d)\n",
    "        logloss = metrics.log_loss(dev_labels, pp)\n",
    "        \n",
    "        # Print log loss for each value\n",
    "        print(\"For max_depth =\", d, \"the Log Loss score is\", round(logloss, 6))  \n",
    "\n",
    "def find_n(data, labels, dev_data, dev_labels, depth, n_values):\n",
    "    \"\"\"Find optimal value of n in a random forest model.  \n",
    "    \n",
    "    Note that this cannot be used on test data from Kaggle \n",
    "    because we do not have labels for that data.  This function is intended to only be used\n",
    "    in the development stage with the development data.\n",
    "    \"\"\"\n",
    "    for n in n_values:      \n",
    "        RF, pp = TrainRF(data, labels, dev_data, depth, n)\n",
    "        logloss = metrics.log_loss(dev_labels, pp)\n",
    "        \n",
    "        # Print log loss for each value\n",
    "        print(\"For n =\", n, \"the Log Loss score is\", round(logloss, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_depth = 1 the Log Loss score is 2.651556\n",
      "For max_depth = 3 the Log Loss score is 2.61752\n",
      "For max_depth = 5 the Log Loss score is 2.578207\n",
      "For max_depth = 7 the Log Loss score is 2.546061\n",
      "For max_depth = 9 the Log Loss score is 2.511507\n",
      "For max_depth = 15 the Log Loss score is 2.437328\n",
      "For max_depth = 17 the Log Loss score is 2.447785\n",
      "For max_depth = 19 the Log Loss score is 2.465811\n",
      "For max_depth = 21 the Log Loss score is 2.55271\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try out some different values of max_depth\n",
    "values = [1, 3, 5, 7, 9, 15, 17, 19, 21]\n",
    "find_max_depth(train_data, train_labels, dev_data, dev_labels, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_depth = 14 the Log Loss score is 2.450646\n",
      "For max_depth = 15 the Log Loss score is 2.44434\n",
      "For max_depth = 16 the Log Loss score is 2.439742\n",
      "For max_depth = 17 the Log Loss score is 2.446666\n",
      "For max_depth = 18 the Log Loss score is 2.449507\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try out some more precise values of max_depth\n",
    "values = [14, 15, 16, 17, 18]\n",
    "find_max_depth(train_data, train_labels, dev_data, dev_labels, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result re: max depth:\n",
    "\n",
    "Log loss varies a bit with each time the model is trained.  \n",
    "The best log loss is with max_depth set to 15, 16, or 17, so I chose 15 to keep the max_depth a bit smaller to improve speed of training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depth = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 10 the Log Loss score is 2.438711\n",
      "For n = 50 the Log Loss score is 2.412623\n",
      "For n = 100 the Log Loss score is 2.410666\n",
      "For n = 200 the Log Loss score is 2.406928\n",
      "For n = 300 the Log Loss score is 2.406008\n",
      "For n = 500 the Log Loss score is 2.405178\n",
      "For n = 750 the Log Loss score is 2.405141\n",
      "For n = 1000 the Log Loss score is 2.404154\n",
      "For n = 1500 the Log Loss score is 2.403657\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the optimal value of n using the optimal max depth and \n",
    "# the 80% training data and the development data\n",
    "\n",
    "# NOTE: With the default max_depth of None and n_jobs = -1 (using all cores), higher values of n crashed my laptop\n",
    "# Apparently this is common with random forests\n",
    "# The fix I used is to limit the max_depth and limit n_jobs to 1 core (n_jobs=1 is the default)  --Laura\n",
    "\n",
    "n_values = [10, 50, 100, 200, 300, 500, 750, 1000, 1500]\n",
    "find_n(train_data, train_labels, dev_data, dev_labels, depth, n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 500 the Log Loss score is 2.403892\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repeat higher values of n\n",
    "n_values = [500]\n",
    "find_n(train_data, train_labels, dev_data, dev_labels, depth, n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 1000 the Log Loss score is 2.404119\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_values = [1000]\n",
    "find_n(train_data, train_labels, dev_data, dev_labels, depth, n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 2000 the Log Loss score is 2.403935\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_values = [2000]\n",
    "find_n(train_data, train_labels, dev_data, dev_labels, depth, n_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result re: n values:\n",
    "\n",
    "Use n=50 or n=100 for testing hyperparameters for quick processing.  \n",
    "Use at least n=500 for best log loss score.  \n",
    "Results vary each time the model is run (because of randomness), so no single n value will always return the best score.  \n",
    "Higher values are usually better, but anything over about n=500 or n=1000 will be close to the best it can get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest results\n",
    "Using max_depth of 15 and the highest n value practical (i.e., that can be run in a reasonable amount of time returns a better log loss score thus far than either KNN or logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model with single n value and single max_depth value on full set of training data\n",
    "depth = 15\n",
    "n = 1000\n",
    "RF, pp = TrainRF(train_data_all, train_labels_all, test_data_all, depth, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up functions for boosted trees and finding optimal hyperparameters\n",
    "\n",
    "def BoostedTree(data, labels, test_data, learn=1.0, n=10, depth=1):\n",
    "    \"\"\"This function takes in training data and labels, testing data,\n",
    "    and can accept different values of n (number of estimators), \n",
    "    max_depth of the decision tree\n",
    "    and can also accept different values of learning rate for the booster\n",
    "    \n",
    "    It trains a Boosted Decision Tree and returns the model and predicted probabilities.\n",
    "    \"\"\"\n",
    "    Boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth), \n",
    "                               n_estimators=n, \n",
    "                               learning_rate=learn)\n",
    "    Boost.fit(data, labels)\n",
    "    pp = Boost.predict_proba(test_data)\n",
    "    return Boost, pp\n",
    "\n",
    "def find_learning_rate(data, labels, dev_data, dev_labels, learn_values):\n",
    "    \"\"\"Find optimal learning rate in an AdaBoost model on a decision tree. \n",
    "    \n",
    "    Note that this cannot be used on test data from Kaggle \n",
    "    because we do not have labels for that data.  This function is intended to only be used\n",
    "    in the development stage with the development data.    \n",
    "    \"\"\"\n",
    "    for learn in learn_values:      \n",
    "        Boost, pp = BoostedTree(data, labels, dev_data, learn)\n",
    "        logloss = metrics.log_loss(dev_labels, pp)\n",
    "        \n",
    "        # Print log loss for each value of k\n",
    "        print(\"For learning rate =\", learn, \"the Log Loss score is\", round(logloss, 6))  \n",
    "        \n",
    "def find_n_learn(data, labels, dev_data, dev_labels, learn_values, n_values):\n",
    "    \"\"\"There is a tradeoff in boosting between learning rate and n_estimators.\n",
    "    This function looks for the optimal combination of learning rate and n_estimators.\n",
    "    \"\"\"\n",
    "    for learn in learn_values:\n",
    "        for n in n_values:\n",
    "            Boost, pp = BoostedTree(data, labels, dev_data, learn, n)\n",
    "            logloss = metrics.log_loss(dev_labels, pp)        \n",
    "            # Print log loss for each combined value of learning rate and n_estimators:\n",
    "            print(\"For learning rate =\", learn, \"and n value\", n, \"the Log Loss score is\", round(logloss, 6))  \n",
    "        \n",
    "def find_max_depth(data, labels, dev_data, dev_labels, learn, n, depth_values):\n",
    "    \"\"\"Find optimal value of max_depth in the boosted model \n",
    "    given optimal combination of learning rate and n_estimators    \n",
    "    \"\"\"\n",
    "    for d in depth_values:      \n",
    "        BT, pp = BoostedTree(data, labels, dev_data, learn, n, d)\n",
    "        logloss = metrics.log_loss(dev_labels, pp)        \n",
    "        # Print log loss for each value\n",
    "        print(\"For max_depth =\", d, \"the Log Loss score is\", round(logloss, 6))         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.39454132531\n"
     ]
    }
   ],
   "source": [
    "# First train a boosted decision tree model with default values\n",
    "Boost, pp = BoostedTree(train_data, train_labels, dev_data)\n",
    "logloss = metrics.log_loss(dev_labels, pp)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 0.0001 the Log Loss score is 2.647279\n",
      "For learning rate = 0.001 the Log Loss score is 2.647272\n",
      "For learning rate = 0.01 the Log Loss score is 2.644091\n",
      "For learning rate = 0.05 the Log Loss score is 2.658273\n",
      "For learning rate = 0.1 the Log Loss score is 2.712374\n",
      "For learning rate = 0.5 the Log Loss score is 3.183055\n",
      "For learning rate = 1 the Log Loss score is 3.394541\n"
     ]
    }
   ],
   "source": [
    "# Find the optimal learning rate for default value of n=10\n",
    "learn_values = [.0001, .001, .01, .05, .1, .5, 1.0]\n",
    "find_learning_rate(train_data, train_labels, dev_data, dev_labels, learn_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 0.0001 and n value 50 the Log Loss score is 2.647274\n",
      "For learning rate = 0.0001 and n value 100 the Log Loss score is 2.647273\n",
      "For learning rate = 0.0001 and n value 250 the Log Loss score is 2.647309\n",
      "For learning rate = 0.001 and n value 50 the Log Loss score is 2.647485\n",
      "For learning rate = 0.001 and n value 100 the Log Loss score is 2.643299\n",
      "For learning rate = 0.001 and n value 250 the Log Loss score is 2.644949\n",
      "For learning rate = 0.01 and n value 50 the Log Loss score is 2.662126\n",
      "For learning rate = 0.01 and n value 100 the Log Loss score is 2.721616\n",
      "For learning rate = 0.01 and n value 250 the Log Loss score is 2.941801\n",
      "For learning rate = 0.05 and n value 50 the Log Loss score is 2.939409\n",
      "For learning rate = 0.05 and n value 100 the Log Loss score is 3.189014\n",
      "For learning rate = 0.05 and n value 250 the Log Loss score is 3.434238\n",
      "For learning rate = 0.1 and n value 50 the Log Loss score is 3.187824\n",
      "For learning rate = 0.1 and n value 100 the Log Loss score is 3.387812\n",
      "For learning rate = 0.1 and n value 250 the Log Loss score is 3.530941\n"
     ]
    }
   ],
   "source": [
    "# There is a trade off between learning rate and values of n, while max_depth=1\n",
    "# so iterate through learning rate with values of n higher than the default of n\n",
    "n_values = [50, 100, 250]\n",
    "learn_values = [.0001, .001, .01, .05, .1]\n",
    "find_n_learn(train_data, train_labels, dev_data, dev_labels, learn_values, n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_depth = 3 the Log Loss score is 2.589809\n",
      "For max_depth = 5 the Log Loss score is 2.548867\n",
      "For max_depth = 6 the Log Loss score is 2.527923\n",
      "For max_depth = 7 the Log Loss score is 2.509453\n",
      "For max_depth = 8 the Log Loss score is 2.504026\n",
      "For max_depth = 9 the Log Loss score is 2.521608\n"
     ]
    }
   ],
   "source": [
    "# See if changes to max_depth improves the model with basic specific learning rate and n\n",
    "learn = .01\n",
    "n = 10\n",
    "depth_values = [3, 5, 6, 7, 8, 9]\n",
    "find_max_depth(train_data, train_labels, dev_data, dev_labels, learn, n, depth_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_depth = 5 the Log Loss score is 2.548683\n",
      "For max_depth = 6 the Log Loss score is 2.527825\n",
      "For max_depth = 7 the Log Loss score is 2.50858\n",
      "For max_depth = 8 the Log Loss score is 2.502424\n",
      "For max_depth = 9 the Log Loss score is 2.518338\n",
      "For max_depth = 10 the Log Loss score is 2.567408\n"
     ]
    }
   ],
   "source": [
    "# See if changes to max_depth improves the model with a optimal specific learning rate and n\n",
    "learn = 0.001\n",
    "n = 100\n",
    "depth_values = [5, 6, 7, 8, 9, 10]\n",
    "find_max_depth(train_data, train_labels, dev_data, dev_labels, learn, n, depth_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boosted Decision Tree results:\n",
    "\n",
    "Optimal combination, using data set as of 11/19 (weather added, binarized, normalized), resulting in log loss of 2.502 on dev data is:  \n",
    "learning rate = 0.001  \n",
    "n estimators = 100  \n",
    "max depth =  8  \n",
    "Higher values of n do not necessarily improve, and sometimes decrease, performance in boosted trees.\n",
    "\n",
    "Lower values of n=10 and learning rate of 0.01 with max depth of 5 or 6 return similar log loss values with shorter processing time.\n",
    "\n",
    "So on this dataset, a boosted decision tree does a bit better than logistic regression (2.54) but does not out-perform the random forest (2.40).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up functions for training bagged trees and finding optimal hyperparameters\n",
    "\n",
    "# TBD, requires writing some code outside of sk learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create zipped csv file of probabilities to submit to Kaggle\n",
    "This code uses the predicted probabilities from the most recent model trained in this notebook, saved as the variable pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id         ARSON   ASSAULT  BAD CHECKS   BRIBERY  BURGLARY  \\\n",
      "0       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "1       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "2       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "3       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "4       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "5       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "6       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "7       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "8       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "9       0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "10      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "11      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "12      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "13      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "14      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "15      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "16      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "17      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "18      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "19      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "20      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "21      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "22      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "23      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "24      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "25      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "26      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "27      0.001870  0.061964    0.000035  0.000113  0.015451   \n",
      "28      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "29      0.000873  0.041185    0.000029  0.000063  0.013059   \n",
      "...          ...       ...         ...       ...       ...   \n",
      "884232  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884233  0.000667  0.056823    0.000206  0.000051  0.013583   \n",
      "884234  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884235  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884236  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884237  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884238  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884239  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884240  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884241  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884242  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884243  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884244  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884245  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884246  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884247  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884248  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884249  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884250  0.000667  0.056823    0.000206  0.000051  0.013583   \n",
      "884251  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884252  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884253  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884254  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884255  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884256  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884257  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884258  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884259  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884260  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "884261  0.000757  0.047696    0.000158  0.000056  0.012341   \n",
      "\n",
      "Id      DISORDERLY CONDUCT  DRIVING UNDER THE INFLUENCE  DRUG/NARCOTIC  \\\n",
      "0                 0.001722                     0.008363       0.030176   \n",
      "1                 0.001722                     0.008363       0.030176   \n",
      "2                 0.001722                     0.008363       0.030176   \n",
      "3                 0.001722                     0.008363       0.030176   \n",
      "4                 0.001722                     0.008363       0.030176   \n",
      "5                 0.001722                     0.008363       0.030176   \n",
      "6                 0.001722                     0.008363       0.030176   \n",
      "7                 0.001722                     0.008363       0.030176   \n",
      "8                 0.001722                     0.008363       0.030176   \n",
      "9                 0.001722                     0.008363       0.030176   \n",
      "10                0.001722                     0.008363       0.030176   \n",
      "11                0.001722                     0.008363       0.030176   \n",
      "12                0.001722                     0.008363       0.030176   \n",
      "13                0.001722                     0.008363       0.030176   \n",
      "14                0.001722                     0.008363       0.030176   \n",
      "15                0.001722                     0.008363       0.030176   \n",
      "16                0.001722                     0.008363       0.030176   \n",
      "17                0.001722                     0.008363       0.030176   \n",
      "18                0.001722                     0.008363       0.030176   \n",
      "19                0.001722                     0.008363       0.030176   \n",
      "20                0.001722                     0.008363       0.030176   \n",
      "21                0.001722                     0.008363       0.030176   \n",
      "22                0.001722                     0.008363       0.030176   \n",
      "23                0.001722                     0.008363       0.030176   \n",
      "24                0.001722                     0.008363       0.030176   \n",
      "25                0.001722                     0.008363       0.030176   \n",
      "26                0.001722                     0.008363       0.030176   \n",
      "27                0.002683                     0.005950       0.038311   \n",
      "28                0.001722                     0.008363       0.030176   \n",
      "29                0.001722                     0.008363       0.030176   \n",
      "...                    ...                          ...            ...   \n",
      "884232            0.003924                     0.003617       0.036178   \n",
      "884233            0.003266                     0.002590       0.054933   \n",
      "884234            0.003924                     0.003617       0.036178   \n",
      "884235            0.003924                     0.003617       0.036178   \n",
      "884236            0.003924                     0.003617       0.036178   \n",
      "884237            0.003924                     0.003617       0.036178   \n",
      "884238            0.003924                     0.003617       0.036178   \n",
      "884239            0.003924                     0.003617       0.036178   \n",
      "884240            0.003924                     0.003617       0.036178   \n",
      "884241            0.003924                     0.003617       0.036178   \n",
      "884242            0.003924                     0.003617       0.036178   \n",
      "884243            0.003924                     0.003617       0.036178   \n",
      "884244            0.003924                     0.003617       0.036178   \n",
      "884245            0.003924                     0.003617       0.036178   \n",
      "884246            0.003924                     0.003617       0.036178   \n",
      "884247            0.003924                     0.003617       0.036178   \n",
      "884248            0.003924                     0.003617       0.036178   \n",
      "884249            0.003924                     0.003617       0.036178   \n",
      "884250            0.003266                     0.002590       0.054933   \n",
      "884251            0.003924                     0.003617       0.036178   \n",
      "884252            0.003924                     0.003617       0.036178   \n",
      "884253            0.003924                     0.003617       0.036178   \n",
      "884254            0.003924                     0.003617       0.036178   \n",
      "884255            0.003924                     0.003617       0.036178   \n",
      "884256            0.003924                     0.003617       0.036178   \n",
      "884257            0.003924                     0.003617       0.036178   \n",
      "884258            0.003924                     0.003617       0.036178   \n",
      "884259            0.003924                     0.003617       0.036178   \n",
      "884260            0.003924                     0.003617       0.036178   \n",
      "884261            0.003924                     0.003617       0.036178   \n",
      "\n",
      "Id      DRUNKENNESS  EMBEZZLEMENT     ...       SEX OFFENSES NON FORCIBLE  \\\n",
      "0          0.004213      0.000145     ...                        0.000040   \n",
      "1          0.004213      0.000145     ...                        0.000040   \n",
      "2          0.004213      0.000145     ...                        0.000040   \n",
      "3          0.004213      0.000145     ...                        0.000040   \n",
      "4          0.004213      0.000145     ...                        0.000040   \n",
      "5          0.004213      0.000145     ...                        0.000040   \n",
      "6          0.004213      0.000145     ...                        0.000040   \n",
      "7          0.004213      0.000145     ...                        0.000040   \n",
      "8          0.004213      0.000145     ...                        0.000040   \n",
      "9          0.004213      0.000145     ...                        0.000040   \n",
      "10         0.004213      0.000145     ...                        0.000040   \n",
      "11         0.004213      0.000145     ...                        0.000040   \n",
      "12         0.004213      0.000145     ...                        0.000040   \n",
      "13         0.004213      0.000145     ...                        0.000040   \n",
      "14         0.004213      0.000145     ...                        0.000040   \n",
      "15         0.004213      0.000145     ...                        0.000040   \n",
      "16         0.004213      0.000145     ...                        0.000040   \n",
      "17         0.004213      0.000145     ...                        0.000040   \n",
      "18         0.004213      0.000145     ...                        0.000040   \n",
      "19         0.004213      0.000145     ...                        0.000040   \n",
      "20         0.004213      0.000145     ...                        0.000040   \n",
      "21         0.004213      0.000145     ...                        0.000040   \n",
      "22         0.004213      0.000145     ...                        0.000040   \n",
      "23         0.004213      0.000145     ...                        0.000040   \n",
      "24         0.004213      0.000145     ...                        0.000040   \n",
      "25         0.004213      0.000145     ...                        0.000040   \n",
      "26         0.004213      0.000145     ...                        0.000040   \n",
      "27         0.006741      0.000119     ...                        0.000022   \n",
      "28         0.004213      0.000145     ...                        0.000040   \n",
      "29         0.004213      0.000145     ...                        0.000040   \n",
      "...             ...           ...     ...                             ...   \n",
      "884232     0.003547      0.000281     ...                        0.000059   \n",
      "884233     0.003919      0.000305     ...                        0.000030   \n",
      "884234     0.003547      0.000281     ...                        0.000059   \n",
      "884235     0.003547      0.000281     ...                        0.000059   \n",
      "884236     0.003547      0.000281     ...                        0.000059   \n",
      "884237     0.003547      0.000281     ...                        0.000059   \n",
      "884238     0.003547      0.000281     ...                        0.000059   \n",
      "884239     0.003547      0.000281     ...                        0.000059   \n",
      "884240     0.003547      0.000281     ...                        0.000059   \n",
      "884241     0.003547      0.000281     ...                        0.000059   \n",
      "884242     0.003547      0.000281     ...                        0.000059   \n",
      "884243     0.003547      0.000281     ...                        0.000059   \n",
      "884244     0.003547      0.000281     ...                        0.000059   \n",
      "884245     0.003547      0.000281     ...                        0.000059   \n",
      "884246     0.003547      0.000281     ...                        0.000059   \n",
      "884247     0.003547      0.000281     ...                        0.000059   \n",
      "884248     0.003547      0.000281     ...                        0.000059   \n",
      "884249     0.003547      0.000281     ...                        0.000059   \n",
      "884250     0.003919      0.000305     ...                        0.000030   \n",
      "884251     0.003547      0.000281     ...                        0.000059   \n",
      "884252     0.003547      0.000281     ...                        0.000059   \n",
      "884253     0.003547      0.000281     ...                        0.000059   \n",
      "884254     0.003547      0.000281     ...                        0.000059   \n",
      "884255     0.003547      0.000281     ...                        0.000059   \n",
      "884256     0.003547      0.000281     ...                        0.000059   \n",
      "884257     0.003547      0.000281     ...                        0.000059   \n",
      "884258     0.003547      0.000281     ...                        0.000059   \n",
      "884259     0.003547      0.000281     ...                        0.000059   \n",
      "884260     0.003547      0.000281     ...                        0.000059   \n",
      "884261     0.003547      0.000281     ...                        0.000059   \n",
      "\n",
      "Id      STOLEN PROPERTY   SUICIDE  SUSPICIOUS OCC          TREA  TRESPASS  \\\n",
      "0              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "1              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "2              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "3              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "4              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "5              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "6              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "7              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "8              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "9              0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "10             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "11             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "12             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "13             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "14             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "15             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "16             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "17             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "18             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "19             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "20             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "21             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "22             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "23             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "24             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "25             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "26             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "27             0.005414  0.000102        0.018465  0.000000e+00  0.001357   \n",
      "28             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "29             0.005121  0.000652        0.019425  3.305785e-07  0.001320   \n",
      "...                 ...       ...             ...           ...       ...   \n",
      "884232         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884233         0.004805  0.001284        0.024611  2.196397e-06  0.003024   \n",
      "884234         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884235         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884236         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884237         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884238         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884239         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884240         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884241         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884242         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884243         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884244         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884245         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884246         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884247         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884248         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884249         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884250         0.004805  0.001284        0.024611  2.196397e-06  0.003024   \n",
      "884251         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884252         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884253         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884254         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884255         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884256         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884257         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884258         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884259         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884260         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "884261         0.005194  0.002050        0.025719  2.196397e-06  0.002462   \n",
      "\n",
      "Id      VANDALISM  VEHICLE THEFT  WARRANTS  WEAPON LAWS  \n",
      "0        0.060972       0.087831  0.035111     0.006583  \n",
      "1        0.060972       0.087831  0.035111     0.006583  \n",
      "2        0.060972       0.087831  0.035111     0.006583  \n",
      "3        0.060972       0.087831  0.035111     0.006583  \n",
      "4        0.060972       0.087831  0.035111     0.006583  \n",
      "5        0.060972       0.087831  0.035111     0.006583  \n",
      "6        0.060972       0.087831  0.035111     0.006583  \n",
      "7        0.060972       0.087831  0.035111     0.006583  \n",
      "8        0.060972       0.087831  0.035111     0.006583  \n",
      "9        0.060972       0.087831  0.035111     0.006583  \n",
      "10       0.060972       0.087831  0.035111     0.006583  \n",
      "11       0.060972       0.087831  0.035111     0.006583  \n",
      "12       0.060972       0.087831  0.035111     0.006583  \n",
      "13       0.060972       0.087831  0.035111     0.006583  \n",
      "14       0.060972       0.087831  0.035111     0.006583  \n",
      "15       0.060972       0.087831  0.035111     0.006583  \n",
      "16       0.060972       0.087831  0.035111     0.006583  \n",
      "17       0.060972       0.087831  0.035111     0.006583  \n",
      "18       0.060972       0.087831  0.035111     0.006583  \n",
      "19       0.060972       0.087831  0.035111     0.006583  \n",
      "20       0.060972       0.087831  0.035111     0.006583  \n",
      "21       0.060972       0.087831  0.035111     0.006583  \n",
      "22       0.060972       0.087831  0.035111     0.006583  \n",
      "23       0.060972       0.087831  0.035111     0.006583  \n",
      "24       0.060972       0.087831  0.035111     0.006583  \n",
      "25       0.060972       0.087831  0.035111     0.006583  \n",
      "26       0.060972       0.087831  0.035111     0.006583  \n",
      "27       0.056009       0.065642  0.038154     0.009204  \n",
      "28       0.060972       0.087831  0.035111     0.006583  \n",
      "29       0.060972       0.087831  0.035111     0.006583  \n",
      "...           ...            ...       ...          ...  \n",
      "884232   0.049501       0.076352  0.035295     0.005706  \n",
      "884233   0.041353       0.058518  0.041203     0.006213  \n",
      "884234   0.049501       0.076352  0.035295     0.005706  \n",
      "884235   0.049501       0.076352  0.035295     0.005706  \n",
      "884236   0.049501       0.076352  0.035295     0.005706  \n",
      "884237   0.049501       0.076352  0.035295     0.005706  \n",
      "884238   0.049501       0.076352  0.035295     0.005706  \n",
      "884239   0.049501       0.076352  0.035295     0.005706  \n",
      "884240   0.049501       0.076352  0.035295     0.005706  \n",
      "884241   0.049501       0.076352  0.035295     0.005706  \n",
      "884242   0.049501       0.076352  0.035295     0.005706  \n",
      "884243   0.049501       0.076352  0.035295     0.005706  \n",
      "884244   0.049501       0.076352  0.035295     0.005706  \n",
      "884245   0.049501       0.076352  0.035295     0.005706  \n",
      "884246   0.049501       0.076352  0.035295     0.005706  \n",
      "884247   0.049501       0.076352  0.035295     0.005706  \n",
      "884248   0.049501       0.076352  0.035295     0.005706  \n",
      "884249   0.049501       0.076352  0.035295     0.005706  \n",
      "884250   0.041353       0.058518  0.041203     0.006213  \n",
      "884251   0.049501       0.076352  0.035295     0.005706  \n",
      "884252   0.049501       0.076352  0.035295     0.005706  \n",
      "884253   0.049501       0.076352  0.035295     0.005706  \n",
      "884254   0.049501       0.076352  0.035295     0.005706  \n",
      "884255   0.049501       0.076352  0.035295     0.005706  \n",
      "884256   0.049501       0.076352  0.035295     0.005706  \n",
      "884257   0.049501       0.076352  0.035295     0.005706  \n",
      "884258   0.049501       0.076352  0.035295     0.005706  \n",
      "884259   0.049501       0.076352  0.035295     0.005706  \n",
      "884260   0.049501       0.076352  0.035295     0.005706  \n",
      "884261   0.049501       0.076352  0.035295     0.005706  \n",
      "\n",
      "[884262 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# Set up predictions for submission to Kaggle \n",
    "\n",
    "\n",
    "headers = [\"ARSON\",\"ASSAULT\",\"BAD CHECKS\",\"BRIBERY\",\"BURGLARY\",\"DISORDERLY CONDUCT\",\"DRIVING UNDER THE INFLUENCE\",\n",
    "           \"DRUG/NARCOTIC\",\"DRUNKENNESS\",\"EMBEZZLEMENT\",\"EXTORTION\",\"FAMILY OFFENSES\",\"FORGERY/COUNTERFEITING\",\n",
    "           \"FRAUD\",\"GAMBLING\",\"KIDNAPPING\",\"LARCENY/THEFT\",\"LIQUOR LAWS\",\"LOITERING\",\"MISSING PERSON\",\"NON-CRIMINAL\",\n",
    "           \"OTHER OFFENSES\",\"PORNOGRAPHY/OBSCENE MAT\",\"PROSTITUTION\",\"RECOVERED VEHICLE\",\"ROBBERY\",\"RUNAWAY\",\n",
    "           \"SECONDARY CODES\",\"SEX OFFENSES FORCIBLE\",\"SEX OFFENSES NON FORCIBLE\",\"STOLEN PROPERTY\",\"SUICIDE\",\n",
    "           \"SUSPICIOUS OCC\",\"TREA\",\"TRESPASS\",\"VANDALISM\",\"VEHICLE THEFT\",\"WARRANTS\",\"WEAPON LAWS\"]\n",
    "data = pd.DataFrame(data=pp, \n",
    "                    index=[x for x in range(len(test_data_all))], \n",
    "                    columns=headers)\n",
    "data.columns.name =\"Id\"\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create zipped csv file for Kaggle\n",
    "#### Update the filename first in all lines of the following code\n",
    "Add something unique after our names to avoid overwriting other submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('Williams_Gascoigne_Vignola_RandomForest1.csv', index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_probs = zipfile.ZipFile(\"Williams_Gascoigne_Vignola_RandomForest1.zip\", \"w\")\n",
    "zip_probs.write(\"Williams_Gascoigne_Vignola_RandomForest1.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_probs.close()### Results from previous datasets and/or model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from previous datasets and/or model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest model on dataset as of 11/18 (weather added, binarized, normalized) with max_depth set to 15 and n_estimators set to 1000 returned log loss on development data of 2.404."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
