{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains random forest model and also several different decision tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip data files into the \"csv\" subdirectory \n",
    "# (unless this has already done this since running the Data Set Up notebook)\n",
    "\n",
    "# **IMPORTANT**  This will overwrite existing files in the \"csv\" folder in your local repo\n",
    "# with the most recent data files from the data.zip file\n",
    "\n",
    "# Unzip 80% training data\n",
    "unzip_training_data = zipfile.ZipFile(\"data_subset.zip\", \"r\")\n",
    "unzip_training_data.extractall()\n",
    "unzip_training_data.close()\n",
    "\n",
    "# Unzip development and training data\n",
    "unzip_test_data = zipfile.ZipFile(\"testing.zip\", \"r\")\n",
    "unzip_test_data.extractall()\n",
    "unzip_test_data.close()\n",
    "\n",
    "# Unzip full set of training data for creating predictions to submit to Kaggle\n",
    "unzip_all_data = zipfile.ZipFile(\"data.zip\", \"r\")\n",
    "unzip_all_data.extractall()\n",
    "unzip_all_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for testing on development data\n",
    "train_data = np.loadtxt('csv/train_data.csv', delimiter=\",\")\n",
    "train_labels = np.loadtxt('csv/train_labels.csv', dtype=str, delimiter=\",\")\n",
    "dev_data = np.loadtxt('csv/dev_data.csv', delimiter=\",\")\n",
    "dev_labels = np.loadtxt('csv/dev_labels.csv', dtype=str, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for creating predictions to submit to Kaggle\n",
    "train_data_all = np.loadtxt('csv/train_data_all.csv', delimiter=\",\")\n",
    "train_labels_all = np.loadtxt('csv/train_labels_all.csv', dtype=str, delimiter=\",\")\n",
    "test_data_all = np.loadtxt('csv/test_data_all.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (702439, 58)\n",
      "train_labels shape is (702439,)\n",
      "dev_data shape is (175610, 58)\n",
      "dev_labels shape is (175610,)\n"
     ]
    }
   ],
   "source": [
    "# print shapes to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IF there are additional changes to make to the data for this model\n",
    "# that would be easier to do in pandas, uncomment and run this code. \n",
    "# This model works the same whether the data is in numpy or pandas, so presumably so do other models\n",
    "\n",
    "#train_data = pd.DataFrame(train_data)\n",
    "#train_labels = pd.DataFrame(train_labels)\n",
    "#dev_data = pd.DataFrame(dev_data)\n",
    "#dev_labels = pd.DataFrame(dev_labels)\n",
    "#train_data_all = pd.DataFrame(train_data_all)\n",
    "#train_labels_all = pd.DataFrame(train_labels_all)\n",
    "#test_data_all = pd.DataFrame(test_data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up functions for training random forest and finding optimal number of estimators\n",
    "\n",
    "def TrainRF(data, labels, test_data, n=10):\n",
    "    \"\"\"This function takes in training data and labels, testing data,\n",
    "    and can accept different values of n (the number of random decision trees to create)\n",
    "    It trains a random forest model and returns the model and predicted probabilities.\n",
    "    \"\"\"\n",
    "    RF = RandomForestClassifier(n_estimators=n)\n",
    "    RF.fit(data, labels)\n",
    "    pp = RF.predict_proba(test_data)\n",
    "    return RF, pp\n",
    "\n",
    "def find_n(data, labels, dev_data, dev_labels, n_values):\n",
    "    \"\"\"Find optimal value of n in a random forest model.  \n",
    "    \n",
    "    Note that this cannot be used on test data from Kaggle \n",
    "    because we do not have labels for that data.  This function is intended to only be used\n",
    "    in the development stage with the development data.\n",
    "    \"\"\"\n",
    "    for n in n_values:      \n",
    "        RF, pp = TrainRF(data, labels, dev_data, n)\n",
    "        predictions = RF.predict(dev_data)\n",
    "        f1 = metrics.f1_score(dev_labels, predictions, average = \"weighted\")\n",
    "        logloss = metrics.log_loss(dev_labels, pp)\n",
    "        \n",
    "        # Print F1 score and log loss for each value of k\n",
    "        print(\"For n =\", n, \"the F1 score is\", round(f1, 6), \"and the Log Loss score is\", round(logloss, 6))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.7449184017\n"
     ]
    }
   ],
   "source": [
    "# Train model with a default value of n=10\n",
    "RF, pp = TrainRF(train_data, train_labels, dev_data)\n",
    "logloss = metrics.log_loss(dev_labels, pp)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laura/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 10 the F1 score is 0.260758 and the Log Loss score is 13.860644\n",
      "For n = 100 the F1 score is 0.278999 and the Log Loss score is 5.154798\n"
     ]
    }
   ],
   "source": [
    "# Find the optimal value of n using the 80% training data and the development data\n",
    "n_values = [10, 100]\n",
    "find_n(train_data, train_labels, dev_data, dev_labels, n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the optimal value of n using the 80% training data and the development data\n",
    "# Doing higher numbers separately\n",
    "n_values = [500, 1000]\n",
    "find_n(train_data, train_labels, dev_data, dev_labels, n_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up functions for training bagged trees and finding optimal number of estimators\n",
    "\n",
    "def TrainRF(data, labels, test_data, n=10):\n",
    "    \"\"\"This function takes in training data and labels, testing data,\n",
    "    and can accept different values of n (the number of random decision trees to create)\n",
    "    It trains a random forest model and returns the model and predicted probabilities.\n",
    "    \"\"\"\n",
    "    RF = RandomForestClassifier(n_estimators=n)\n",
    "    RF.fit(data, labels)\n",
    "    pp = RF.predict_proba(test_data)\n",
    "    return RF, pp\n",
    "\n",
    "def find_n(data, labels, dev_data, dev_labels, n_values):\n",
    "    \"\"\"Find optimal value of n in a random forest model.  \n",
    "    \n",
    "    Note that this cannot be used on test data from Kaggle \n",
    "    because we do not have labels for that data.  This function is intended to only be used\n",
    "    in the development stage with the development data.\n",
    "    \"\"\"\n",
    "    for n in n_values:      \n",
    "        RF, pp = TrainRF(data, labels, dev_data, n)\n",
    "        predictions = RF.predict(dev_data)\n",
    "        f1 = metrics.f1_score(dev_labels, predictions, average = \"weighted\")\n",
    "        logloss = metrics.log_loss(dev_labels, pp)\n",
    "        \n",
    "        # Print F1 score and log loss for each value of k\n",
    "        print(\"For n =\", n, \"the F1 score is\", round(f1, 6), \"and the Log Loss score is\", round(logloss, 6))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
