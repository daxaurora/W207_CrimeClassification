{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a logistic regression model, finds the optimal value of C, and reports F1 and log loss scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip data files into the \"csv\" subdirectory \n",
    "# (unless you have already done this since running the Data Set Up notebook)\n",
    "\n",
    "# **IMPORTANT**  This will overwrite existing files in the \"csv\" folder in your local repo\n",
    "# with the most recent data files from the data.zip file\n",
    "\n",
    "# Unzip 80% training data\n",
    "unzip_training_data = zipfile.ZipFile(\"data_subset.zip\", \"r\")\n",
    "unzip_training_data.extractall()\n",
    "unzip_training_data.close()\n",
    "\n",
    "# Unzip development and training data\n",
    "unzip_test_data = zipfile.ZipFile(\"testing.zip\", \"r\")\n",
    "unzip_test_data.extractall()\n",
    "unzip_test_data.close()\n",
    "\n",
    "# Unzip full set of training data for creating predictions to submit to Kaggle\n",
    "unzip_all_data = zipfile.ZipFile(\"data.zip\", \"r\")\n",
    "unzip_all_data.extractall()\n",
    "unzip_all_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for testing on development data\n",
    "train_data = np.loadtxt('csv/train_data.csv', delimiter=\",\")\n",
    "train_labels = np.loadtxt('csv/train_labels.csv', dtype=str, delimiter=\",\")\n",
    "dev_data = np.loadtxt('csv/dev_data.csv', delimiter=\",\")\n",
    "dev_labels = np.loadtxt('csv/dev_labels.csv', dtype=str, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for creating predictions to submit to Kaggle\n",
    "train_data_all = np.loadtxt('csv/train_data_all.csv', delimiter=\",\")\n",
    "train_labels_all = np.loadtxt('csv/train_labels_all.csv', dtype=str, delimiter=\",\")\n",
    "test_data_all = np.loadtxt('csv/test_data_all.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (702439, 70)\n",
      "train_labels shape is (702439,)\n",
      "dev_data shape is (175610, 70)\n",
      "dev_labels shape is (175610,)\n"
     ]
    }
   ],
   "source": [
    "# print shapes to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_all shape is (878049, 70)\n",
      "train_labels_all shape is (878049,)\n",
      "test_data_all shape is (884262, 70)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up functions for training logistic regression model and finding optimal value of C\n",
    "\n",
    "def TrainLR(data, labels, test_data, C_value=1.0):\n",
    "    \"\"\"This function takes in training data and labels, testing data,\n",
    "    and can accept different values of C (the learning rate).\n",
    "    It trains a logistic regression model and returns the model and predicted probabilities.\n",
    "    \"\"\"\n",
    "    LR = LogisticRegression(C=C_value, n_jobs = -1)\n",
    "    LR.fit(data, labels)\n",
    "    pp = LR.predict_proba(test_data)\n",
    "    return LR, pp\n",
    "\n",
    "def find_C(data, labels, dev_data, dev_labels, C_values):\n",
    "    \"\"\"Find optimal value of C in a logistic regression model.  \n",
    "    \n",
    "    Note that this cannot be used on test data from Kaggle \n",
    "    because we do not have labels for that data.  This function is intended to only be used\n",
    "    in the development stage with the development data.\n",
    "    \"\"\"\n",
    "    for C in C_values:      \n",
    "        LR, pp = TrainLR(data, labels, dev_data, C, n_jobs = -1)\n",
    "        predictions = LR.predict(dev_data)\n",
    "        f1 = metrics.f1_score(dev_labels, predictions, average = \"weighted\")\n",
    "        logloss = metrics.log_loss(dev_labels, pp)\n",
    "        \n",
    "        # Print F1 score and log loss for each value of k\n",
    "        print(\"For C =\", C, \"the F1 score is\", round(f1, 6), \"and the Log Loss score is\", round(logloss, 6))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.73167566453e-11\n"
     ]
    }
   ],
   "source": [
    "# IF there are additional changes to make to the data for this model\n",
    "# that would be easier to do in pandas, uncomment and run this code. \n",
    "# This model works the same whether the data is in numpy or pandas, so presumably so do other models\n",
    "\n",
    "#train_data = pd.DataFrame(train_data)\n",
    "#train_labels = pd.DataFrame(train_labels)\n",
    "#dev_data = pd.DataFrame(dev_data)\n",
    "#dev_labels = pd.DataFrame(dev_labels)\n",
    "#train_data_all = pd.DataFrame(train_data_all)\n",
    "#train_labels_all = pd.DataFrame(train_labels_all)\n",
    "#test_data_all = pd.DataFrame(test_data_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C = 0.0001 the F1 score is 0.148899 and the Log Loss score is 3.011221\n",
      "For C = 0.001 the F1 score is 0.152004 and the Log Loss score is 2.632073\n",
      "For C = 0.01 the F1 score is 0.152955 and the Log Loss score is 2.544749\n",
      "For C = 0.1 the F1 score is 0.153044 and the Log Loss score is 2.536512\n"
     ]
    }
   ],
   "source": [
    "# Find the optimal value of C using the 80% training data and the development data\n",
    "C_values = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 100.00, 1000.0]\n",
    "find_C(train_data, train_labels, dev_data, dev_labels, C_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.55164421465\n"
     ]
    }
   ],
   "source": [
    "# Train model with a single value of C with 80% training data and development data\n",
    "C_value = 2.0\n",
    "LR, pp = TrainLR(train_data, train_labels, dev_data, C_value)\n",
    "logloss = metrics.log_loss(dev_labels, pp)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.22402131e+02   3.77993635e+01   2.30000000e+01   1.13410312e-15\n",
      "    1.22112641e-15   1.48000000e+02   1.00000000e+00   7.10552343e-14\n",
      "    5.97626091e-14  -2.58138065e-14   7.97651384e-14   5.70000000e+01\n",
      "    4.90000000e+01   4.18974337e-03  -5.98141563e+01   1.13262456e+02\n",
      "   -8.71005634e+01   9.39353141e+01  -2.84668165e+01   1.24899927e+02\n",
      "    1.28105712e+02  -7.27129946e-15  -5.84756682e-15  -8.36377413e-14\n",
      "    1.00000000e+00  -5.32049518e-14  -6.10362984e-14  -2.53076411e-14\n",
      "   -3.28003920e-14   1.00000000e+00   9.51304769e-14   6.01701285e-14\n",
      "    7.89733135e-14  -6.26487169e-14  -6.51588321e-15  -2.45310261e-13\n",
      "    1.12259130e-13  -1.60885502e-14  -1.32793199e-13  -1.56762497e-13\n",
      "    1.14828026e-13  -1.77027955e-14   1.00000000e+00  -1.17785832e-13\n",
      "    1.87579739e-13   1.48970436e-13   1.13883935e-13   1.03036516e-13\n",
      "   -1.78693525e-13  -1.49548207e-13  -5.10486444e-12   3.06833227e-12\n",
      "   -1.05432682e-12   1.09916826e-12  -1.01266124e-15   2.79962986e-12\n",
      "    2.19059370e-12   1.56141243e-12   7.38312708e-13   1.73889404e-12\n",
      "    3.14302841e-13  -4.01681366e-12   1.00000000e+00  -4.85172616e-14\n",
      "    4.31356250e-14   4.77470004e-14   2.78232081e-14   1.00000000e+00\n",
      "    5.32265142e-14   1.17604192e-14]]\n"
     ]
    }
   ],
   "source": [
    "print(test_data_all[[13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-de252309b486>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# using the optimal value for the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mC_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-0fdeba09fa44>\u001b[0m in \u001b[0;36mTrainLR\u001b[1;34m(data, labels, test_data, C_value)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mLR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mC_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mpp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0mcalculate_ovr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"ovr\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcalculate_ovr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1286\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1287\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \"\"\"\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[0mprob\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    310\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 58\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Before submitting to Kaggle, run the model on the full set of training data and test data\n",
    "# using the optimal value for the model\n",
    "C_value = 2.0\n",
    "LR, pp = TrainLR(train_data_all, train_labels_all, test_data_all, C_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_names = ['X', 'Y', 'hour', 'holidays', 'first_day', 'month_year','PRCP', 'TMAX', 'TMIN', 'DayOfWeek_Friday', 'DayOfWeek_Monday',\n",
    "       'DayOfWeek_Saturday', 'DayOfWeek_Sunday', 'DayOfWeek_Thursday',\n",
    "       'DayOfWeek_Tuesday', 'DayOfWeek_Wednesday', 'PdDistrict_BAYVIEW',\n",
    "       'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', 'PdDistrict_MISSION',\n",
    "       'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND',\n",
    "       'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN',\n",
    "       'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12',\n",
    "       'year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007',\n",
    "       'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012',\n",
    "       'year_2013', 'year_2014', 'year_2015', 'dayparts_early_afternoon',\n",
    "       'dayparts_early_evening', 'dayparts_early_morning',\n",
    "       'dayparts_late_afternoon', 'dayparts_late_evening',\n",
    "       'dayparts_late_morning', 'dayparts_late_night']\n",
    "\n",
    "headers = [\"ARSON\",\"ASSAULT\",\"BAD CHECKS\",\"BRIBERY\",\"BURGLARY\",\"DISORDERLY CONDUCT\",\"DRIVING UNDER THE INFLUENCE\",\n",
    "           \"DRUG/NARCOTIC\",\"DRUNKENNESS\",\"EMBEZZLEMENT\",\"EXTORTION\",\"FAMILY OFFENSES\",\"FORGERY/COUNTERFEITING\",\n",
    "           \"FRAUD\",\"GAMBLING\",\"KIDNAPPING\",\"LARCENY/THEFT\",\"LIQUOR LAWS\",\"LOITERING\",\"MISSING PERSON\",\"NON-CRIMINAL\",\n",
    "           \"OTHER OFFENSES\",\"PORNOGRAPHY/OBSCENE MAT\",\"PROSTITUTION\",\"RECOVERED VEHICLE\",\"ROBBERY\",\"RUNAWAY\",\n",
    "           \"SECONDARY CODES\",\"SEX OFFENSES FORCIBLE\",\"SEX OFFENSES NON FORCIBLE\",\"STOLEN PROPERTY\",\"SUICIDE\",\n",
    "           \"SUSPICIOUS OCC\",\"TREA\",\"TRESPASS\",\"VANDALISM\",\"VEHICLE THEFT\",\"WARRANTS\",\"WEAPON LAWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-414166c87fda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Finds the top features (as a tuple)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtopfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtopfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "#Finds the top features (as a tuple)\n",
    "topfeatures = [0]*len(headers)\n",
    "for i in range(len(headers)):\n",
    "    topfeatures[i] = sorted(enumerate(LR.coef_[i]), key=lambda tup: tup[1], reverse = True)[0:5]\n",
    "\n",
    "#Extracts the indices of the top features\n",
    "feature_index = []\n",
    "for lst in topfeatures:\n",
    "    for j in lst:\n",
    "        feature_index.append(j[0])\n",
    "feature_index.sort()\n",
    "\n",
    "#Sets up the data for the table\n",
    "feature_names = []\n",
    "table_text = []\n",
    "for i in feature_index:\n",
    "    feature_names.append(get_feature_names[i])\n",
    "    table_text.append([LR.coef_[j][i] for j in range(len(headers))])\n",
    "    \n",
    "#Table\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "ax.table(cellText=table_text, rowLabels=feature_names, colLabels=headers, loc='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262, 39)\n",
      "Id         ARSON   ASSAULT  BAD CHECKS   BRIBERY  BURGLARY  \\\n",
      "0       0.011413  0.132513    0.000007  0.001572  0.033707   \n",
      "1       0.010538  0.126436    0.000006  0.001534  0.024041   \n",
      "2       0.003258  0.087975    0.000011  0.000386  0.054249   \n",
      "3       0.005019  0.116111    0.000005  0.001531  0.020660   \n",
      "4       0.005019  0.116111    0.000005  0.001531  0.020660   \n",
      "5       0.004678  0.102299    0.000005  0.000805  0.014835   \n",
      "6       0.005589  0.120152    0.000010  0.001376  0.056189   \n",
      "7       0.005032  0.113020    0.000009  0.001291  0.041262   \n",
      "8       0.002683  0.102761    0.000007  0.001217  0.024580   \n",
      "9       0.001612  0.057956    0.000006  0.000204  0.011200   \n",
      "10      0.005135  0.112459    0.000011  0.001215  0.058701   \n",
      "11      0.002489  0.098141    0.000006  0.001206  0.016913   \n",
      "12      0.002489  0.098141    0.000006  0.001206  0.016913   \n",
      "13      0.003073  0.090870    0.000016  0.000321  0.051168   \n",
      "14      0.001703  0.059428    0.000008  0.000194  0.018935   \n",
      "15      0.001199  0.096515    0.000003  0.000386  0.005397   \n",
      "16      0.004864  0.116086    0.000015  0.000806  0.039758   \n",
      "17      0.005974  0.147590    0.000012  0.001993  0.035862   \n",
      "18      0.002906  0.093536    0.000016  0.000407  0.052273   \n",
      "19      0.001639  0.061645    0.000008  0.000247  0.019849   \n",
      "20      0.001639  0.061645    0.000008  0.000247  0.019849   \n",
      "21      0.002481  0.110570    0.000013  0.001238  0.032697   \n",
      "22      0.001929  0.084544    0.000035  0.000260  0.049774   \n",
      "23      0.004836  0.147881    0.000014  0.002036  0.023349   \n",
      "24      0.002430  0.098156    0.000015  0.000466  0.024845   \n",
      "25      0.003772  0.078270    0.000030  0.000362  0.051107   \n",
      "26      0.002124  0.088486    0.000014  0.000400  0.023024   \n",
      "27      0.001741  0.082172    0.000013  0.000486  0.012559   \n",
      "28      0.002980  0.103701    0.000013  0.001100  0.024802   \n",
      "29      0.002467  0.079259    0.000022  0.000403  0.043317   \n",
      "...          ...       ...         ...       ...       ...   \n",
      "884232  0.002938  0.126063    0.001736  0.000352  0.022771   \n",
      "884233  0.002680  0.099451    0.001999  0.000131  0.020161   \n",
      "884234  0.005165  0.090983    0.002383  0.000107  0.031698   \n",
      "884235  0.003588  0.104583    0.001551  0.000238  0.027162   \n",
      "884236  0.002224  0.103011    0.001381  0.000266  0.017072   \n",
      "884237  0.003919  0.128494    0.004919  0.000104  0.066020   \n",
      "884238  0.005069  0.122933    0.002312  0.000184  0.036226   \n",
      "884239  0.002985  0.127658    0.001642  0.000367  0.020876   \n",
      "884240  0.005267  0.090098    0.003799  0.000092  0.059981   \n",
      "884241  0.012014  0.155569    0.001898  0.000413  0.039427   \n",
      "884242  0.002816  0.122135    0.001759  0.000332  0.023533   \n",
      "884243  0.002850  0.122536    0.001876  0.000329  0.026800   \n",
      "884244  0.009732  0.136406    0.001119  0.000379  0.018097   \n",
      "884245  0.001688  0.118276    0.000935  0.000105  0.011703   \n",
      "884246  0.013654  0.168168    0.002308  0.000454  0.050529   \n",
      "884247  0.002791  0.121533    0.001701  0.000332  0.022206   \n",
      "884248  0.003002  0.108574    0.003599  0.000083  0.041379   \n",
      "884249  0.002758  0.102914    0.003022  0.000079  0.032126   \n",
      "884250  0.002661  0.099482    0.001680  0.000138  0.015717   \n",
      "884251  0.002700  0.118336    0.001847  0.000309  0.025220   \n",
      "884252  0.004408  0.121929    0.001497  0.000316  0.024903   \n",
      "884253  0.003402  0.120187    0.003320  0.000101  0.036097   \n",
      "884254  0.003850  0.127295    0.004739  0.000103  0.062734   \n",
      "884255  0.004614  0.124262    0.002321  0.000286  0.045945   \n",
      "884256  0.002749  0.118996    0.002128  0.000300  0.032098   \n",
      "884257  0.002985  0.127658    0.001642  0.000367  0.020876   \n",
      "884258  0.003601  0.112541    0.003238  0.000106  0.065958   \n",
      "884259  0.005115  0.136016    0.001174  0.000418  0.016985   \n",
      "884260  0.011433  0.150954    0.002060  0.000375  0.043205   \n",
      "884261  0.004863  0.117855    0.002670  0.000165  0.046310   \n",
      "\n",
      "Id      DISORDERLY CONDUCT  DRIVING UNDER THE INFLUENCE  DRUG/NARCOTIC  \\\n",
      "0                 0.001040                     0.003383       0.016911   \n",
      "1                 0.001016                     0.004528       0.017439   \n",
      "2                 0.001888                     0.002629       0.011706   \n",
      "3                 0.000863                     0.005559       0.010972   \n",
      "4                 0.000863                     0.005559       0.010972   \n",
      "5                 0.000946                     0.010094       0.008949   \n",
      "6                 0.000990                     0.002552       0.006935   \n",
      "7                 0.001001                     0.003617       0.006884   \n",
      "8                 0.004157                     0.005230       0.024134   \n",
      "9                 0.002122                     0.008011       0.008219   \n",
      "10                0.001040                     0.002690       0.005733   \n",
      "11                0.003995                     0.006942       0.025080   \n",
      "12                0.003995                     0.006942       0.025080   \n",
      "13                0.002574                     0.001991       0.005498   \n",
      "14                0.002257                     0.005517       0.007344   \n",
      "15                0.004294                     0.004904       0.103208   \n",
      "16                0.001060                     0.003885       0.006106   \n",
      "17                0.000965                     0.002747       0.007103   \n",
      "18                0.001878                     0.002363       0.012558   \n",
      "19                0.001735                     0.006454       0.012919   \n",
      "20                0.001735                     0.006454       0.012919   \n",
      "21                0.004381                     0.003697       0.021144   \n",
      "22                0.002596                     0.001714       0.005012   \n",
      "23                0.000915                     0.003413       0.008497   \n",
      "24                0.001786                     0.003897       0.016359   \n",
      "25                0.001014                     0.008785       0.005500   \n",
      "26                0.001776                     0.004404       0.015812   \n",
      "27                0.001372                     0.004442       0.020092   \n",
      "28                0.000914                     0.004618       0.010006   \n",
      "29                0.001105                     0.004558       0.005305   \n",
      "...                    ...                          ...            ...   \n",
      "884232            0.016361                     0.005264       0.062571   \n",
      "884233            0.006435                     0.005059       0.051167   \n",
      "884234            0.004073                     0.021110       0.026618   \n",
      "884235            0.003964                     0.007342       0.028750   \n",
      "884236            0.015531                     0.007357       0.069133   \n",
      "884237            0.011463                     0.001934       0.018817   \n",
      "884238            0.004431                     0.006373       0.022556   \n",
      "884239            0.016017                     0.005404       0.064142   \n",
      "884240            0.005007                     0.015208       0.014403   \n",
      "884241            0.004157                     0.002966       0.046824   \n",
      "884242            0.016406                     0.005323       0.063655   \n",
      "884243            0.015918                     0.004654       0.071483   \n",
      "884244            0.003726                     0.005450       0.055374   \n",
      "884245            0.017188                     0.002518       0.191898   \n",
      "884246            0.004423                     0.002298       0.035970   \n",
      "884247            0.016452                     0.005591       0.061990   \n",
      "884248            0.011840                     0.003696       0.019803   \n",
      "884249            0.011480                     0.004666       0.021883   \n",
      "884250            0.005977                     0.005645       0.059594   \n",
      "884251            0.017049                     0.005424       0.058595   \n",
      "884252            0.003796                     0.006455       0.030062   \n",
      "884253            0.011335                     0.003685       0.022926   \n",
      "884254            0.011387                     0.002048       0.019696   \n",
      "884255            0.004550                     0.004727       0.019571   \n",
      "884256            0.017026                     0.004485       0.062335   \n",
      "884257            0.016017                     0.005404       0.064142   \n",
      "884258            0.008868                     0.003129       0.035024   \n",
      "884259            0.003357                     0.006571       0.034694   \n",
      "884260            0.004565                     0.003212       0.038598   \n",
      "884261            0.004479                     0.005403       0.022104   \n",
      "\n",
      "Id      DRUNKENNESS  EMBEZZLEMENT     ...       SEX OFFENSES NON FORCIBLE  \\\n",
      "0          0.004402      0.000053     ...                        0.000117   \n",
      "1          0.004486      0.000041     ...                        0.000125   \n",
      "2          0.004979      0.000069     ...                        0.000033   \n",
      "3          0.003733      0.000032     ...                        0.000137   \n",
      "4          0.003733      0.000032     ...                        0.000137   \n",
      "5          0.006389      0.000033     ...                        0.000203   \n",
      "6          0.003559      0.000067     ...                        0.000080   \n",
      "7          0.003724      0.000052     ...                        0.000079   \n",
      "8          0.010315      0.000048     ...                        0.000118   \n",
      "9          0.008832      0.000032     ...                        0.000028   \n",
      "10         0.003615      0.000067     ...                        0.000063   \n",
      "11         0.010393      0.000036     ...                        0.000131   \n",
      "12         0.010393      0.000036     ...                        0.000131   \n",
      "13         0.008355      0.000112     ...                        0.000034   \n",
      "14         0.008723      0.000046     ...                        0.000021   \n",
      "15         0.010192      0.000044     ...                        0.000042   \n",
      "16         0.006051      0.000125     ...                        0.000192   \n",
      "17         0.003631      0.000093     ...                        0.000222   \n",
      "18         0.005042      0.000120     ...                        0.000048   \n",
      "19         0.005289      0.000050     ...                        0.000029   \n",
      "20         0.005289      0.000050     ...                        0.000029   \n",
      "21         0.010331      0.000107     ...                        0.000145   \n",
      "22         0.008437      0.000331     ...                        0.000042   \n",
      "23         0.003732      0.000121     ...                        0.000354   \n",
      "24         0.005436      0.000126     ...                        0.000102   \n",
      "25         0.004891      0.000143     ...                        0.000136   \n",
      "26         0.005448      0.000114     ...                        0.000081   \n",
      "27         0.009940      0.000156     ...                        0.000088   \n",
      "28         0.003883      0.000107     ...                        0.000130   \n",
      "29         0.006253      0.000197     ...                        0.000079   \n",
      "...             ...           ...     ...                             ...   \n",
      "884232     0.011012      0.004546     ...                        0.001377   \n",
      "884233     0.011666      0.006760     ...                        0.000416   \n",
      "884234     0.006046      0.003115     ...                        0.001072   \n",
      "884235     0.004504      0.003307     ...                        0.000501   \n",
      "884236     0.011081      0.003400     ...                        0.000975   \n",
      "884237     0.010040      0.013907     ...                        0.000420   \n",
      "884238     0.007213      0.006067     ...                        0.001157   \n",
      "884239     0.010919      0.004309     ...                        0.001520   \n",
      "884240     0.006248      0.004850     ...                        0.000629   \n",
      "884241     0.004672      0.005808     ...                        0.001093   \n",
      "884242     0.011059      0.004584     ...                        0.001238   \n",
      "884243     0.010818      0.005048     ...                        0.001201   \n",
      "884244     0.004712      0.003163     ...                        0.001250   \n",
      "884245     0.009700      0.004291     ...                        0.000260   \n",
      "884246     0.004601      0.007196     ...                        0.001141   \n",
      "884247     0.011104      0.004389     ...                        0.001256   \n",
      "884248     0.011072      0.009116     ...                        0.000328   \n",
      "884249     0.011240      0.007442     ...                        0.000334   \n",
      "884250     0.011428      0.005709     ...                        0.000505   \n",
      "884251     0.011258      0.004729     ...                        0.001059   \n",
      "884252     0.004353      0.003348     ...                        0.000824   \n",
      "884253     0.010968      0.008674     ...                        0.000478   \n",
      "884254     0.010111      0.013336     ...                        0.000423   \n",
      "884255     0.004533      0.005158     ...                        0.000536   \n",
      "884256     0.011093      0.005630     ...                        0.000950   \n",
      "884257     0.010919      0.004309     ...                        0.001520   \n",
      "884258     0.006235      0.007746     ...                        0.000308   \n",
      "884259     0.004085      0.002722     ...                        0.001525   \n",
      "884260     0.004913      0.006052     ...                        0.000872   \n",
      "884261     0.007100      0.007106     ...                        0.000903   \n",
      "\n",
      "Id      STOLEN PROPERTY   SUICIDE  SUSPICIOUS OCC      TREA  TRESPASS  \\\n",
      "0              0.005118  0.000204        0.032920  0.000067  0.005154   \n",
      "1              0.004737  0.000127        0.029977  0.000060  0.003959   \n",
      "2              0.007459  0.000303        0.023272  0.000033  0.006436   \n",
      "3              0.004561  0.000272        0.026251  0.000009  0.002653   \n",
      "4              0.004561  0.000272        0.026251  0.000009  0.002653   \n",
      "5              0.003088  0.000120        0.027088  0.000007  0.002241   \n",
      "6              0.005190  0.000446        0.031489  0.000016  0.005672   \n",
      "7              0.004849  0.000244        0.028701  0.000016  0.004374   \n",
      "8              0.006010  0.000238        0.023221  0.000031  0.006172   \n",
      "9              0.004970  0.000089        0.015312  0.000023  0.002395   \n",
      "10             0.005029  0.000290        0.030276  0.000019  0.005702   \n",
      "11             0.005487  0.000147        0.020929  0.000026  0.004656   \n",
      "12             0.005487  0.000147        0.020929  0.000026  0.004656   \n",
      "13             0.006313  0.000298        0.023976  0.000034  0.009302   \n",
      "14             0.005564  0.000155        0.017310  0.000029  0.003489   \n",
      "15             0.004203  0.000072        0.019989  0.000011  0.005021   \n",
      "16             0.003441  0.000290        0.036185  0.000009  0.005158   \n",
      "17             0.004341  0.000244        0.032294  0.000010  0.004975   \n",
      "18             0.007371  0.000420        0.025723  0.000029  0.006712   \n",
      "19             0.006004  0.000111        0.017867  0.000027  0.002652   \n",
      "20             0.006004  0.000111        0.017867  0.000027  0.002652   \n",
      "21             0.006081  0.000326        0.026964  0.000032  0.008262   \n",
      "22             0.005916  0.000394        0.026729  0.000033  0.009494   \n",
      "23             0.004006  0.000242        0.032360  0.000008  0.003786   \n",
      "24             0.006448  0.000267        0.024656  0.000018  0.004174   \n",
      "25             0.004415  0.000481        0.038759  0.000012  0.004565   \n",
      "26             0.006333  0.000231        0.023304  0.000020  0.003723   \n",
      "27             0.005851  0.000112        0.022740  0.000009  0.004469   \n",
      "28             0.004662  0.000482        0.030787  0.000011  0.003008   \n",
      "29             0.003567  0.000362        0.035116  0.000014  0.004462   \n",
      "...                 ...       ...             ...       ...       ...   \n",
      "884232         0.005479  0.000504        0.033875  0.000015  0.010993   \n",
      "884233         0.006862  0.000242        0.033410  0.000008  0.010288   \n",
      "884234         0.004917  0.001087        0.049571  0.000006  0.004990   \n",
      "884235         0.005159  0.000893        0.039392  0.000009  0.004741   \n",
      "884236         0.005386  0.000483        0.030223  0.000015  0.007842   \n",
      "884237         0.007217  0.001405        0.042938  0.000020  0.021417   \n",
      "884238         0.003852  0.000912        0.050713  0.000006  0.007440   \n",
      "884239         0.005348  0.000477        0.033326  0.000014  0.010459   \n",
      "884240         0.004962  0.000539        0.052724  0.000010  0.008165   \n",
      "884241         0.005131  0.000766        0.050701  0.000037  0.010405   \n",
      "884242         0.005610  0.000565        0.034001  0.000015  0.010961   \n",
      "884243         0.006021  0.001025        0.035854  0.000015  0.011832   \n",
      "884244         0.004387  0.000379        0.041121  0.000027  0.005548   \n",
      "884245         0.004392  0.000302        0.031308  0.000007  0.015665   \n",
      "884246         0.004955  0.000543        0.052562  0.000041  0.013468   \n",
      "884247         0.005474  0.000474        0.033293  0.000015  0.010546   \n",
      "884248         0.006831  0.000654        0.037262  0.000021  0.013648   \n",
      "884249         0.006636  0.000568        0.035074  0.000019  0.010977   \n",
      "884250         0.006641  0.000264        0.032072  0.000006  0.008608   \n",
      "884251         0.005643  0.000487        0.033901  0.000017  0.011312   \n",
      "884252         0.004891  0.000859        0.040262  0.000007  0.004968   \n",
      "884253         0.006739  0.000743        0.037949  0.000017  0.013115   \n",
      "884254         0.007248  0.001443        0.042661  0.000019  0.020415   \n",
      "884255         0.005202  0.000638        0.044390  0.000011  0.007911   \n",
      "884256         0.006168  0.000886        0.036512  0.000019  0.013234   \n",
      "884257         0.005348  0.000477        0.033326  0.000014  0.010459   \n",
      "884258         0.008322  0.000899        0.039323  0.000022  0.013441   \n",
      "884259         0.004345  0.000753        0.038063  0.000004  0.004143   \n",
      "884260         0.005087  0.000466        0.050228  0.000045  0.010971   \n",
      "884261         0.004129  0.001384        0.053031  0.000007  0.008573   \n",
      "\n",
      "Id      VANDALISM  VEHICLE THEFT  WARRANTS  WEAPON LAWS  \n",
      "0        0.086258       0.104453  0.033869     0.024774  \n",
      "1        0.080787       0.104450  0.036385     0.025873  \n",
      "2        0.071592       0.070022  0.027192     0.008528  \n",
      "3        0.085471       0.144872  0.023298     0.019053  \n",
      "4        0.085471       0.144872  0.023298     0.019053  \n",
      "5        0.088298       0.109534  0.022245     0.016132  \n",
      "6        0.100761       0.134239  0.018392     0.014758  \n",
      "7        0.095908       0.136794  0.020025     0.015083  \n",
      "8        0.061941       0.080373  0.039636     0.013698  \n",
      "9        0.049860       0.067480  0.024306     0.005997  \n",
      "10       0.100537       0.131632  0.018077     0.013453  \n",
      "11       0.057211       0.079314  0.042523     0.014498  \n",
      "12       0.057211       0.079314  0.042523     0.014498  \n",
      "13       0.071817       0.058013  0.020322     0.006626  \n",
      "14       0.055067       0.068269  0.021633     0.005266  \n",
      "15       0.024531       0.019270  0.077691     0.013857  \n",
      "16       0.103240       0.097664  0.017258     0.013744  \n",
      "17       0.096111       0.118128  0.021810     0.021799  \n",
      "18       0.071041       0.069165  0.026856     0.009067  \n",
      "19       0.054651       0.074372  0.029106     0.007305  \n",
      "20       0.054651       0.074372  0.029106     0.007305  \n",
      "21       0.065027       0.076060  0.036621     0.013465  \n",
      "22       0.067294       0.055666  0.017920     0.005790  \n",
      "23       0.088229       0.119119  0.023134     0.024168  \n",
      "24       0.063493       0.070463  0.032539     0.011834  \n",
      "25       0.091159       0.103306  0.012773     0.008200  \n",
      "26       0.061056       0.071778  0.031703     0.010632  \n",
      "27       0.049601       0.038194  0.041445     0.009576  \n",
      "28       0.083934       0.149011  0.019618     0.014805  \n",
      "29       0.095319       0.109191  0.013826     0.008018  \n",
      "...           ...            ...       ...          ...  \n",
      "884232   0.044003       0.061962  0.081735     0.011837  \n",
      "884233   0.043032       0.035657  0.090149     0.007239  \n",
      "884234   0.068482       0.113168  0.035369     0.007790  \n",
      "884235   0.064977       0.147102  0.043144     0.009948  \n",
      "884236   0.039763       0.067379  0.079388     0.009908  \n",
      "884237   0.060775       0.053771  0.045804     0.006164  \n",
      "884238   0.078212       0.100768  0.038756     0.010058  \n",
      "884239   0.043210       0.061031  0.083104     0.012350  \n",
      "884240   0.077960       0.102817  0.032326     0.006452  \n",
      "884241   0.063964       0.086079  0.064864     0.018961  \n",
      "884242   0.044065       0.063698  0.080140     0.011230  \n",
      "884243   0.044858       0.066462  0.075906     0.010761  \n",
      "884244   0.053498       0.087374  0.072497     0.020129  \n",
      "884245   0.019601       0.013895  0.125272     0.009447  \n",
      "884246   0.067619       0.077314  0.063888     0.019704  \n",
      "884247   0.043571       0.062887  0.081434     0.011377  \n",
      "884248   0.055968       0.059705  0.051247     0.005772  \n",
      "884249   0.053059       0.061840  0.053403     0.005821  \n",
      "884250   0.040638       0.035882  0.092712     0.007766  \n",
      "884251   0.044673       0.064088  0.079367     0.010588  \n",
      "884252   0.065144       0.135806  0.045860     0.012443  \n",
      "884253   0.055696       0.058887  0.054517     0.006836  \n",
      "884254   0.060288       0.054830  0.046306     0.006181  \n",
      "884255   0.074799       0.130930  0.042588     0.010713  \n",
      "884256   0.046652       0.066872  0.074073     0.009849  \n",
      "884257   0.043210       0.061031  0.083104     0.012350  \n",
      "884258   0.059499       0.066492  0.061408     0.007136  \n",
      "884259   0.060215       0.123361  0.049660     0.016083  \n",
      "884260   0.066110       0.085988  0.066050     0.017830  \n",
      "884261   0.080510       0.103649  0.035382     0.008796  \n",
      "\n",
      "[884262 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# Set up predictions for submission to Kaggle\n",
    "\n",
    "data = pd.DataFrame(data=pp, \n",
    "                    index=[x for x in range(len(test_data_all))], \n",
    "                    columns=headers)\n",
    "data.columns.name =\"Id\"\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create zipped csv file for Kaggle\n",
    "#### Update the filename first in all lines of the following code\n",
    "Add something unique after our names to avoid overwriting other submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('Williams_Gascoigne_Vignola_Regression4.csv', index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_probs = zipfile.ZipFile(\"Williams_Gascoigne_Vignola_Regression4.zip\", \"w\")\n",
    "zip_probs.write(\"Williams_Gascoigne_Vignola_Regression4.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_probs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from previous datasets and/or model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Submission**   \n",
    "Results on development data from dataset as of Saturday 11/18, with weather added, latitude outliers removed, binarized and normalized features:\n",
    "\n",
    "For C = 0.0001 the F1 score is 0.147075 and the Log Loss score is 3.014795  \n",
    "For C = 0.001 the F1 score is 0.150284 and the Log Loss score is 2.638404  \n",
    "For C = 0.01 the F1 score is 0.151366 and the Log Loss score is 2.551881  \n",
    "For C = 0.1 the F1 score is 0.151589 and the Log Loss score is 2.543797  \n",
    "For C = 0.5 the F1 score is 0.151615 and the Log Loss score is 2.543427  \n",
    "For C = 1.0 the F1 score is 0.151579 and the Log Loss score is 2.543396  \n",
    "**For C = 2.0 the F1 score is 0.151605 and the Log Loss score is 2.543383**  \n",
    "For C = 10.0 the F1 score is 0.151657 and the Log Loss score is 2.543385  \n",
    "For C = 100.0 the F1 score is 0.151619 and the Log Loss score is 2.543447  \n",
    "For C = 1000.0 the F1 score is 0.151616 and the Log Loss score is 2.543544  \n",
    "\n",
    "Predictions on test data from training on full data set are in zip file ending with Regression1\n",
    "\n",
    "Kaggle score from that zip file that we thought should have correlated with the above scores on dev data was 18.20988 (!?)  \n",
    "\n",
    "\n",
    "**Second Submission**  \n",
    "We changed our workflow along the way, so our first step was to re-run this notebook to confirm that we are unzipping and using the latest version of the data, in particular the full set of training and test data. \n",
    "\n",
    "Log loss on dev data after this step, with C=2.0 is 2.54338  \n",
    "Predictions from this step are in zip file ending with Regression2  \n",
    "Kaggle score is the same: 18.20989   \n",
    "So the problem ended up being our test data was not normalized.\n",
    "\n",
    "\n",
    "**Third Submission**  \n",
    "On Cyprian's suggestion, we added the multi_class = 'multinomial' argument to the logistic regression model, because we don't have a binary output variable.  \n",
    "\n",
    "Log loss on dev data after this step, with C=2.0 is 2.54267  \n",
    "Predictions from this step are in zip file ending with Regression3  \n",
    "Kaggle score on this set of predictions is:  33.37633   \n",
    "This is even worse! Removed the multi_class argument from the model code.\n",
    "\n",
    "**Fourth Submission**\n",
    "Fixed a critical bug in submissions 2 and 3 that basically made them nonsense. Our test data was not normalized and now it is.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
