{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Crime Logistic Regression\n",
    "\n",
    "This notebook is of 3rd importance since this model is not as strong as our Random Forests. However we used the information here to try and get a good model and to use feature analysis to help give insight as to what features will lead to a stronger Random Forest model. The steps we took here were as follows. We optimized for C using L2 penalty on our initial 58 feature set. Using this C value we then use a Logistic Regression with L1 penalty to analyze which features are strong, and use this information to get insight into what features represent the data as whole. (This actuall informed our decisions on what features to test in random forests). Finally, we train some Logistic Regression models with the L2 penalty with different combinations of features to see if we find any interesting results.\n",
    "\n",
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional field below for our zipfile workflow. If you have the csvs from the data setup file then this cell can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip data files into the \"csv\" subdirectory \n",
    "# (unless you have already done this since running the Data Set Up notebook)\n",
    "\n",
    "# **IMPORTANT**  This will overwrite existing files in the \"csv\" folder in your local repo\n",
    "# with the most recent data files from the data.zip file\n",
    "\n",
    "# Unzip 80% training data\n",
    "unzip_training_data = zipfile.ZipFile(\"data_subset.zip\", \"r\")\n",
    "unzip_training_data.extractall()\n",
    "unzip_training_data.close()\n",
    "\n",
    "# Unzip development and training data\n",
    "unzip_test_data = zipfile.ZipFile(\"testing.zip\", \"r\")\n",
    "unzip_test_data.extractall()\n",
    "unzip_test_data.close()\n",
    "\n",
    "# Unzip full set of training data for creating predictions to submit to Kaggle\n",
    "unzip_all_data = zipfile.ZipFile(\"data.zip\", \"r\")\n",
    "unzip_all_data.extractall()\n",
    "unzip_all_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for testing on development data\n",
    "train_data = np.loadtxt('csv/train_data.csv', delimiter=\",\")\n",
    "train_labels = np.loadtxt('csv/train_labels.csv', dtype=str, delimiter=\",\")\n",
    "dev_data = np.loadtxt('csv/dev_data.csv', delimiter=\",\")\n",
    "dev_labels = np.loadtxt('csv/dev_labels.csv', dtype=str, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for creating predictions to submit to Kaggle\n",
    "train_data_all = np.loadtxt('csv/train_data_all.csv', delimiter=\",\")\n",
    "train_labels_all = np.loadtxt('csv/train_labels_all.csv', dtype=str, delimiter=\",\")\n",
    "test_data_all = np.loadtxt('csv/test_data_all.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print shapes to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#70 feature set. Our 58 feature set lacked seasons, first_day, month_year, d_police and rotational data.\n",
    "get_feature_names = ['X', 'Y', 'hour', 'holidays', 'first_day', 'month_year', 'spring',\n",
    "       'summer', 'fall', 'winter', 'PRCP', 'TMAX', 'TMIN', 'd_police',\n",
    "       'rot_45_X', 'rot_45_Y', 'rot_30_X', 'rot_30_Y', 'rot_60_X', 'rot_60_Y',\n",
    "       'radial_r', 'DayOfWeek_Friday', 'DayOfWeek_Monday',\n",
    "       'DayOfWeek_Saturday', 'DayOfWeek_Sunday', 'DayOfWeek_Thursday',\n",
    "       'DayOfWeek_Tuesday', 'DayOfWeek_Wednesday', 'PdDistrict_BAYVIEW',\n",
    "       'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', 'PdDistrict_MISSION',\n",
    "       'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND',\n",
    "       'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN',\n",
    "       'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12',\n",
    "       'year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007',\n",
    "       'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012',\n",
    "       'year_2013', 'year_2014', 'year_2015', 'dayparts_early_afternoon',\n",
    "       'dayparts_early_evening', 'dayparts_early_morning',\n",
    "       'dayparts_late_afternoon', 'dayparts_late_evening',\n",
    "       'dayparts_late_morning', 'dayparts_late_night']\n",
    "\n",
    "#Classes\n",
    "headers = [\"ARSON\",\"ASSAULT\",\"BAD CHECKS\",\"BRIBERY\",\"BURGLARY\",\"DISORDERLY CONDUCT\",\"DRIVING UNDER THE INFLUENCE\",\n",
    "           \"DRUG/NARCOTIC\",\"DRUNKENNESS\",\"EMBEZZLEMENT\",\"EXTORTION\",\"FAMILY OFFENSES\",\"FORGERY/COUNTERFEITING\",\n",
    "           \"FRAUD\",\"GAMBLING\",\"KIDNAPPING\",\"LARCENY/THEFT\",\"LIQUOR LAWS\",\"LOITERING\",\"MISSING PERSON\",\"NON-CRIMINAL\",\n",
    "           \"OTHER OFFENSES\",\"PORNOGRAPHY/OBSCENE MAT\",\"PROSTITUTION\",\"RECOVERED VEHICLE\",\"ROBBERY\",\"RUNAWAY\",\n",
    "           \"SECONDARY CODES\",\"SEX OFFENSES FORCIBLE\",\"SEX OFFENSES NON FORCIBLE\",\"STOLEN PROPERTY\",\"SUICIDE\",\n",
    "           \"SUSPICIOUS OCC\",\"TREA\",\"TRESPASS\",\"VANDALISM\",\"VEHICLE THEFT\",\"WARRANTS\",\"WEAPON LAWS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training L1 Models to Analyze Feature Importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature extraction with L1 Logistic regression model.\n",
    "C_value = 2.0\n",
    "#LR = LogisticRegression(penalty = 'l1', C = C_value, n_jobs = -1)\n",
    "#LR.fit(train_data_all, train_labels_all)\n",
    "LR = joblib.load('LR.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finds the top 5 features for each class (as a tuple)\n",
    "topfeatures = [0]*len(headers)\n",
    "for i in range(len(headers)):\n",
    "    topfeatures[i] = sorted(enumerate(LR.coef_[i]), key=lambda tup: tup[1], reverse = True)[0:5]\n",
    "\n",
    "#Extracts the indices of the top features\n",
    "feature_index = [] \n",
    "for lst in topfeatures:\n",
    "    for j in lst:\n",
    "        feature_index.append(j[0])\n",
    "feature_index.sort()\n",
    "\n",
    "#Sets up the data for the table\n",
    "feature_names = []\n",
    "table_text = []\n",
    "for i in feature_index:\n",
    "    feature_names.append(get_feature_names[i])\n",
    "    table_text.append([LR.coef_[j][i] for j in range(len(headers))])\n",
    "print(Counter(feature_names)) #Now we count how many times features showed up in the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sort features by the mean of the absolute value of coefficients\n",
    "coefficients = pd.DataFrame(LR.coef_)\n",
    "d = sorted(dict(coefficients.abs().mean().rank()).items(), key=lambda x:x[1])\n",
    "print(\"Rank\")\n",
    "for i in range(len(d)):\n",
    "    print(71-d[i][1], get_feature_names[d[i][0]]) #They casted as tuples so I had to strongarm them with a full reassignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X                           76\n",
      "Y                           76\n",
      "hour                         0\n",
      "holidays                     0\n",
      "first_day                    0\n",
      "month_year                   0\n",
      "spring                       0\n",
      "summer                       0\n",
      "fall                         0\n",
      "winter                       0\n",
      "PRCP                         0\n",
      "TMAX                         0\n",
      "TMIN                         0\n",
      "d_police                    76\n",
      "rot_45_X                    76\n",
      "rot_45_Y                    76\n",
      "rot_30_X                    76\n",
      "rot_30_Y                    76\n",
      "rot_60_X                    76\n",
      "rot_60_Y                    76\n",
      "radial_r                    76\n",
      "DayOfWeek_Friday             0\n",
      "DayOfWeek_Monday             0\n",
      "DayOfWeek_Saturday           0\n",
      "DayOfWeek_Sunday             0\n",
      "DayOfWeek_Thursday           0\n",
      "DayOfWeek_Tuesday            0\n",
      "DayOfWeek_Wednesday          0\n",
      "PdDistrict_BAYVIEW           0\n",
      "PdDistrict_CENTRAL           0\n",
      "                            ..\n",
      "month_3                      0\n",
      "month_4                      0\n",
      "month_5                      0\n",
      "month_6                      0\n",
      "month_7                      0\n",
      "month_8                      0\n",
      "month_9                      0\n",
      "month_10                     0\n",
      "month_11                     0\n",
      "month_12                     0\n",
      "year_2003                    0\n",
      "year_2004                    0\n",
      "year_2005                    0\n",
      "year_2006                    0\n",
      "year_2007                    0\n",
      "year_2008                    0\n",
      "year_2009                    0\n",
      "year_2010                    0\n",
      "year_2011                    0\n",
      "year_2012                    0\n",
      "year_2013                    0\n",
      "year_2014                    0\n",
      "year_2015                    0\n",
      "dayparts_early_afternoon     0\n",
      "dayparts_early_evening       0\n",
      "dayparts_early_morning       0\n",
      "dayparts_late_afternoon      0\n",
      "dayparts_late_evening        0\n",
      "dayparts_late_morning        0\n",
      "dayparts_late_night          0\n",
      "Length: 70, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_data_all.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['X', 'Y', 'hour', 'holidays', 'first_day', 'month_year', 'spring',\n",
      "       'summer', 'fall', 'winter', 'PRCP', 'TMAX', 'TMIN', 'd_police',\n",
      "       'rot_45_X', 'rot_45_Y', 'rot_30_X', 'rot_30_Y', 'rot_60_X', 'rot_60_Y',\n",
      "       'radial_r', 'DayOfWeek_Friday', 'DayOfWeek_Monday',\n",
      "       'DayOfWeek_Saturday', 'DayOfWeek_Sunday', 'DayOfWeek_Thursday',\n",
      "       'DayOfWeek_Tuesday', 'DayOfWeek_Wednesday', 'PdDistrict_BAYVIEW',\n",
      "       'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', 'PdDistrict_MISSION',\n",
      "       'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND',\n",
      "       'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN',\n",
      "       'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
      "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12',\n",
      "       'year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007',\n",
      "       'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012',\n",
      "       'year_2013', 'year_2014', 'year_2015', 'dayparts_early_afternoon',\n",
      "       'dayparts_early_evening', 'dayparts_early_morning',\n",
      "       'dayparts_late_afternoon', 'dayparts_late_evening',\n",
      "       'dayparts_late_morning', 'dayparts_late_night'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#In this cell we prepare the data for analysis with both dropping the seasons from our complete feature set and additionally\n",
    "#dropping months.\n",
    "\n",
    "\n",
    "train_data_all = pd.DataFrame(train_data_all, columns = get_feature_names)\n",
    "train_labels_all = pd.DataFrame(train_labels_all)\n",
    "test_data_all = pd.DataFrame(test_data_all, columns = get_feature_names)\n",
    "\n",
    "train_data_no_seasons = train_data_all.drop(['winter', 'spring', 'summer', 'fall'], axis = 1) \n",
    "test_data_no_seasons = test_data_all.drop(['winter', 'spring', 'summer', 'fall'], axis = 1)\n",
    "\n",
    "train_data_no_months = train_data_no_seasons.drop(['month_1', 'month_2','month_3','month_4','month_5'\n",
    "                                                 ,'month_6','month_7','month_8','month_9','month_10','month_11','month_12']\n",
    "                                                , axis = 1)\n",
    "\n",
    "test_data_no_months = test_data_no_seasons.drop(['month_1', 'month_2','month_3','month_4','month_5'\n",
    "                                                 ,'month_6','month_7','month_8','month_9','month_10','month_11','month_12']\n",
    "                                                , axis = 1)\n",
    "\n",
    "train_data_no_weekdays = train_data_no_months.drop(['DayOfWeek_Monday', 'DayOfWeek_Friday','DayOfWeek_Wednesday','DayOfWeek_Tuesday'\n",
    "                                                 ,'DayOfWeek_Thursday','DayOfWeek_Saturday','DayOfWeek_Sunday']\n",
    "                                                , axis = 1)\n",
    "\n",
    "test_data_no_weekdays = test_data_no_months.drop(['DayOfWeek_Monday', 'DayOfWeek_Friday','DayOfWeek_Wednesday','DayOfWeek_Tuesday'\n",
    "                                                 ,'DayOfWeek_Thursday','DayOfWeek_Saturday','DayOfWeek_Sunday']\n",
    "                                                , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up functions for training logistic regression model and finding 'optimal' value of C.\n",
    "\n",
    "def TrainLR(data, labels, test_data, C_value=1.0):\n",
    "    \"\"\"This function takes in training data and labels, testing data,\n",
    "    and can accept different values of C (the learning rate).\n",
    "    It trains a logistic regression model and returns the model and predicted probabilities.\n",
    "    \"\"\"\n",
    "    LR = LogisticRegression(C=C_value, n_jobs = -1)\n",
    "    LR.fit(data, labels)\n",
    "    pp = LR.predict_proba(test_data)\n",
    "    return LR, pp\n",
    "\n",
    "def find_C(data, labels, dev_data, dev_labels, C_values):\n",
    "    \"\"\"Find optimal value of C in a logistic regression model.  \n",
    "    \n",
    "    Note that this cannot be used on test data from Kaggle \n",
    "    because we do not have labels for that data.  This function is intended to only be used\n",
    "    in the development stage with the development data.\n",
    "    \"\"\"\n",
    "    for C in C_values:      \n",
    "        LR, pp = TrainLR(data, labels, dev_data, C, n_jobs = -1)\n",
    "        predictions = LR.predict(dev_data)\n",
    "        f1 = metrics.f1_score(dev_labels, predictions, average = \"weighted\")\n",
    "        logloss = metrics.log_loss(dev_labels, pp)\n",
    "        \n",
    "        # Print F1 score and log loss for each value of k\n",
    "        print(\"For C =\", C, \"the F1 score is\", round(f1, 6), \"and the Log Loss score is\", round(logloss, 6))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the optimal value of C using the 80% training data and the development data\n",
    "C_values = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 100.00, 1000.0]\n",
    "find_C(train_data, train_labels, dev_data, dev_labels, C_values)\n",
    "#We found the optimal C to be = 2.0 on our 58 feature set, so we used that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model with a single value of C with 80% training data and development data\n",
    "C_value = 2.0\n",
    "LR, pp = TrainLR(train_data, train_labels, dev_data, C_value)\n",
    "logloss = metrics.log_loss(dev_labels, pp)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this dictionary I'd like to test whether dropping seaons and/or months. Since they score at the very bottom. We confirmed this with our random forest optimization so that we knew we were getting something coherent.\n",
    "\n",
    "#### Preparing final Kaggle models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C_value = 2.0\n",
    "#LR = LogisticRegression(penalty = 'l2', C = C_value, n_jobs = -1)\n",
    "#LR.fit(train_data_no_seasons, train_labels_all)\n",
    "joblib.load(LR, 'LRno_seasons.pkl') \n",
    "pp = LR.predict_proba(test_data_no_seasons)\n",
    "#pp is used to export a file to kaggle for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No seasons kaggle score: 2.55255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_value = 2.0\n",
    "#LR = LogisticRegression(penalty = 'l2', C = C_value, n_jobs = -1)\n",
    "#LR.fit(train_data_all, train_labels_all)\n",
    "joblib.load(LR, 'LRwith_L2.pkl')\n",
    "pp = LR.predict_proba(test_data_all)\n",
    "#pp is used to export a file to kaggle for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle score of 2.55263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_value = 2.0\n",
    "#LR = LogisticRegression(penalty = 'l2', C = C_value, n_jobs = -1)\n",
    "#LR.fit(train_data_no_months, train_labels_all)\n",
    "joblib.load(LR, 'LRno_months.pkl')\n",
    "pp = LR.predict_proba(test_data_no_months)\n",
    "#pp is used to export a file to kaggle for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle score 2.55247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1c1faf1ffa7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#joblib.load(LR, 'LRno_months.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LRno_weekdays.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_no_weekdays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0mcalculate_ovr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"ovr\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcalculate_ovr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1286\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1287\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \"\"\"\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[0mprob\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    310\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kippy\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 58\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "C_value = 2.0\n",
    "LR = LogisticRegression(penalty = 'l2', C = C_value, n_jobs = -1)\n",
    "LR.fit(train_data_no_weekdays, train_labels_all)\n",
    "#joblib.load(LR, 'LRno_months.pkl')\n",
    "joblib.dump(LR, \"LRno_weekdays.pkl\")\n",
    "pp = LR.predict_proba(test_data_no_weekdays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up predictions for submission to Kaggle\n",
    "\n",
    "data = pd.DataFrame(data=pp, \n",
    "                    index=[x for x in range(len(test_data_all))], \n",
    "                    columns=headers)\n",
    "data.columns.name =\"Id\"\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create zipped csv file for Kaggle\n",
    "#### Update the filename first in all lines of the following code\n",
    "Add something unique after our names to avoid overwriting other submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('Williams_Gascoigne_Vignola_Regression8.csv', index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_probs = zipfile.ZipFile(\"Williams_Gascoigne_Vignola_Regression8.zip\", \"w\")\n",
    "zip_probs.write(\"Williams_Gascoigne_Vignola_Regression8.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_probs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from previous datasets and/or model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Submission**   \n",
    "Results on development data from dataset as of Saturday 11/18, with weather added, latitude outliers removed, binarized and normalized features:\n",
    "\n",
    "For C = 0.0001 the F1 score is 0.147075 and the Log Loss score is 3.014795  \n",
    "For C = 0.001 the F1 score is 0.150284 and the Log Loss score is 2.638404  \n",
    "For C = 0.01 the F1 score is 0.151366 and the Log Loss score is 2.551881  \n",
    "For C = 0.1 the F1 score is 0.151589 and the Log Loss score is 2.543797  \n",
    "For C = 0.5 the F1 score is 0.151615 and the Log Loss score is 2.543427  \n",
    "For C = 1.0 the F1 score is 0.151579 and the Log Loss score is 2.543396  \n",
    "**For C = 2.0 the F1 score is 0.151605 and the Log Loss score is 2.543383**  \n",
    "For C = 10.0 the F1 score is 0.151657 and the Log Loss score is 2.543385  \n",
    "For C = 100.0 the F1 score is 0.151619 and the Log Loss score is 2.543447  \n",
    "For C = 1000.0 the F1 score is 0.151616 and the Log Loss score is 2.543544  \n",
    "\n",
    "Predictions on test data from training on full data set are in zip file ending with Regression1\n",
    "\n",
    "Kaggle score from that zip file that we thought should have correlated with the above scores on dev data was 18.20988 (!?)  \n",
    "\n",
    "\n",
    "**Second Submission**  \n",
    "We changed our workflow along the way, so our first step was to re-run this notebook to confirm that we are unzipping and using the latest version of the data, in particular the full set of training and test data. \n",
    "\n",
    "Log loss on dev data after this step, with C=2.0 is 2.54338  \n",
    "Predictions from this step are in zip file ending with Regression2  \n",
    "Kaggle score is the same: 18.20989   \n",
    "So the problem ended up being our test data was not normalized.\n",
    "\n",
    "\n",
    "**Third Submission**  \n",
    "On Cyprian's suggestion, we added the multi_class = 'multinomial' argument to the logistic regression model, because we don't have a binary output variable.  \n",
    "\n",
    "Log loss on dev data after this step, with C=2.0 is 2.54267  \n",
    "Predictions from this step are in zip file ending with Regression3  \n",
    "Kaggle score on this set of predictions is:  33.37633   \n",
    "This is even worse! Removed the multi_class argument from the model code.\n",
    "\n",
    "**Fourth Submission**\n",
    "Fixed a critical bug in submissions 2 and 3 that basically made them nonsense. Our test data was not normalized and now it is. Scored a *2.54* on Kaggle, not bad. This is our best score with Logistic regression.\n",
    "\n",
    "**Fifth-Seventh Submission**\n",
    "More bug fixing.\n",
    "\n",
    "**Eight Submission**\n",
    "\n",
    "L2 Log Regression with no seasons. Kaggle score of 2.55255. Worse than with our 58 feature set (no rotational data, d_police or seasons). We tried dropping season first because we knew they were bad from our L1 regression analysis.\n",
    "\n",
    "**Ninth Submission**\n",
    "\n",
    "L2 Log Regression with full 70 features. Kaggle score of 2.55263. This is worse than with seasons.\n",
    "\n",
    "**Tenth Submission**\n",
    "\n",
    "L2 Log Regression without seasons and months. Kaggle score of 2.55247. This is the best with our full feature set, but still not better than our reduced 58 feature set.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
